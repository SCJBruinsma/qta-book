<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10.1 Support Vector Machines | Introduction to Quantitative Text Analysis</title>
  <meta name="description" content="10.1 Support Vector Machines | Introduction to Quantitative Text Analysis" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="10.1 Support Vector Machines | Introduction to Quantitative Text Analysis" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.1 Support Vector Machines | Introduction to Quantitative Text Analysis" />
  
  
  

<meta name="author" content="Kostas Gemenis and Bastiaan Bruinsma" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="supervised-methods.html"/>
<link rel="next" href="naive-bayes.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="assets/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="assets/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="assets/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="assets/datatables-binding-0.33/datatables.js"></script>
<link href="assets/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="assets/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="assets/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="assets/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="assets/crosstalk-1.2.1/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 2em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="1" data-path="getting-started-with-qta.html"><a href="getting-started-with-qta.html"><i class="fa fa-check"></i><b>1</b> Getting Started with Quantitative Text Analysis</a>
<ul>
<li class="chapter" data-level="1.1" data-path="qta-in-steps.html"><a href="qta-in-steps.html"><i class="fa fa-check"></i><b>1.1</b> QTA in steps</a></li>
<li class="chapter" data-level="1.2" data-path="how-this-book-works.html"><a href="how-this-book-works.html"><i class="fa fa-check"></i><b>1.2</b> How this book works</a></li>
<li class="chapter" data-level="1.3" data-path="further-literature.html"><a href="further-literature.html"><i class="fa fa-check"></i><b>1.3</b> Further Literature</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2</b> Getting Started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-r.html"><a href="what-is-r.html"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="r-on-windows.html"><a href="r-on-windows.html"><i class="fa fa-check"></i><b>2.2</b> R on Windows</a></li>
<li class="chapter" data-level="2.3" data-path="r-on-linux.html"><a href="r-on-linux.html"><i class="fa fa-check"></i><b>2.3</b> R on Linux</a></li>
<li class="chapter" data-level="2.4" data-path="r-on-macos.html"><a href="r-on-macos.html"><i class="fa fa-check"></i><b>2.4</b> R on MacOS</a></li>
<li class="chapter" data-level="2.5" data-path="r-in-the-cloud.html"><a href="r-in-the-cloud.html"><i class="fa fa-check"></i><b>2.5</b> R in the Cloud</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="installing-packages.html"><a href="installing-packages.html"><i class="fa fa-check"></i><b>3</b> Installing Packages</a>
<ul>
<li class="chapter" data-level="3.1" data-path="installing-from-cran.html"><a href="installing-from-cran.html"><i class="fa fa-check"></i><b>3.1</b> Installing from CRAN</a></li>
<li class="chapter" data-level="3.2" data-path="installing-from-github.html"><a href="installing-from-github.html"><i class="fa fa-check"></i><b>3.2</b> Installing from GitHub</a></li>
<li class="chapter" data-level="3.3" data-path="packages-for-quantitative-text-analysis-in-r.html"><a href="packages-for-quantitative-text-analysis-in-r.html"><i class="fa fa-check"></i><b>3.3</b> Packages for Quantitative Text Analysis in R</a></li>
<li class="chapter" data-level="3.4" data-path="problems-bugs-and-errors.html"><a href="problems-bugs-and-errors.html"><i class="fa fa-check"></i><b>3.4</b> Problems, Bugs and Errors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>4</b> Importing Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="text-in-r.html"><a href="text-in-r.html"><i class="fa fa-check"></i><b>4.1</b> Text in R</a></li>
<li class="chapter" data-level="4.2" data-path="import-.txt-files.html"><a href="import-.txt-files.html"><i class="fa fa-check"></i><b>4.2</b> Import .txt Files</a></li>
<li class="chapter" data-level="4.3" data-path="import-.pdf-files.html"><a href="import-.pdf-files.html"><i class="fa fa-check"></i><b>4.3</b> Import .pdf Files</a></li>
<li class="chapter" data-level="4.4" data-path="import-.csv-files.html"><a href="import-.csv-files.html"><i class="fa fa-check"></i><b>4.4</b> Import .csv Files</a></li>
<li class="chapter" data-level="4.5" data-path="import-from-an-api.html"><a href="import-from-an-api.html"><i class="fa fa-check"></i><b>4.5</b> Import from an API</a></li>
<li class="chapter" data-level="4.6" data-path="import-using-web-scraping.html"><a href="import-using-web-scraping.html"><i class="fa fa-check"></i><b>4.6</b> Import using Web Scraping</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="reliability-validity.html"><a href="reliability-validity.html"><i class="fa fa-check"></i><b>5</b> Reliability and Validity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="inter-coder-agreement.html"><a href="inter-coder-agreement.html"><i class="fa fa-check"></i><b>5.1</b> Inter-Coder Agreement</a></li>
<li class="chapter" data-level="5.2" data-path="visualizing-quality.html"><a href="visualizing-quality.html"><i class="fa fa-check"></i><b>5.2</b> Visualizing Quality</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>6</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="6.1" data-path="the-corpus.html"><a href="the-corpus.html"><i class="fa fa-check"></i><b>6.1</b> The Corpus</a></li>
<li class="chapter" data-level="6.2" data-path="keywords-in-context.html"><a href="keywords-in-context.html"><i class="fa fa-check"></i><b>6.2</b> Keywords in Context</a></li>
<li class="chapter" data-level="6.3" data-path="visualisations-and-descriptives.html"><a href="visualisations-and-descriptives.html"><i class="fa fa-check"></i><b>6.3</b> Visualisations and Descriptives</a></li>
<li class="chapter" data-level="6.4" data-path="text-statistics.html"><a href="text-statistics.html"><i class="fa fa-check"></i><b>6.4</b> Text Statistics</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html"><i class="fa fa-check"></i><b>7</b> Dictionary Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="classical-dictionary-analysis.html"><a href="classical-dictionary-analysis.html"><i class="fa fa-check"></i><b>7.1</b> Classical Dictionary Analysis</a></li>
<li class="chapter" data-level="7.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html"><i class="fa fa-check"></i><b>7.2</b> Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#movie-reviews"><i class="fa fa-check"></i><b>7.2.1</b> Movie Reviews</a></li>
<li class="chapter" data-level="7.2.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#twitter"><i class="fa fa-check"></i><b>7.2.2</b> Twitter</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="sentiment-analysis-using-vader.html"><a href="sentiment-analysis-using-vader.html"><i class="fa fa-check"></i><b>7.3</b> Sentiment Analysis using VADER</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="exercices.html"><a href="exercices.html"><i class="fa fa-check"></i><b>8</b> Exercices</a></li>
<li class="chapter" data-level="9" data-path="scaling.html"><a href="scaling.html"><i class="fa fa-check"></i><b>9</b> Scaling Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="wordscores.html"><a href="wordscores.html"><i class="fa fa-check"></i><b>9.1</b> Wordscores</a></li>
<li class="chapter" data-level="9.2" data-path="wordfish.html"><a href="wordfish.html"><i class="fa fa-check"></i><b>9.2</b> Wordfish</a></li>
<li class="chapter" data-level="9.3" data-path="correspondence-analysis.html"><a href="correspondence-analysis.html"><i class="fa fa-check"></i><b>9.3</b> Correspondence Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised-methods.html"><a href="supervised-methods.html"><i class="fa fa-check"></i><b>10</b> Supervised Methods</a>
<ul>
<li class="chapter" data-level="10.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>10.1</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-with-rtexttools"><i class="fa fa-check"></i><b>10.1.1</b> SVM with RTextTools</a></li>
<li class="chapter" data-level="10.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-with-quanteda"><i class="fa fa-check"></i><b>10.1.2</b> SVM with Quanteda</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>10.2</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Methods</a>
<ul>
<li class="chapter" data-level="11.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html"><i class="fa fa-check"></i><b>11.1</b> Latent Dirichlet Allocation</a></li>
<li class="chapter" data-level="11.2" data-path="seeded-latent-dirichlet-allocation.html"><a href="seeded-latent-dirichlet-allocation.html"><i class="fa fa-check"></i><b>11.2</b> Seeded Latent Dirichlet Allocation</a></li>
<li class="chapter" data-level="11.3" data-path="structural-topic-model.html"><a href="structural-topic-model.html"><i class="fa fa-check"></i><b>11.3</b> Structural Topic Model</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="texttricks.html"><a href="texttricks.html"><i class="fa fa-check"></i><b>12</b> Texttricks</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Quantitative Text Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Support Vector Machines<a href="support-vector-machines.html#support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To show how SVM works, we will look at an example of SVM in <code>quanteda</code> and one in <code>RTextTools</code>, and an example of NB in <code>quanteda</code>.</p>
<div id="svm-with-rtexttools" class="section level3 hasAnchor" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> SVM with RTextTools<a href="support-vector-machines.html#svm-with-rtexttools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the SVM, we will start with an example using our Twitter data and the <code>RTextTools</code> package. First, we load the Twitter data:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="support-vector-machines.html#cb175-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;RTextTools&quot;</span>)</span>
<span id="cb175-2"><a href="support-vector-machines.html#cb175-2" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;car&quot;</span>)</span>
<span id="cb175-3"><a href="support-vector-machines.html#cb175-3" tabindex="-1"></a></span>
<span id="cb175-4"><a href="support-vector-machines.html#cb175-4" tabindex="-1"></a>urlfile <span class="ot">&lt;-</span> <span class="st">&quot;https://raw.githubusercontent.com/SCJBruinsma/qta-files/master/Tweets.csv&quot;</span></span>
<span id="cb175-5"><a href="support-vector-machines.html#cb175-5" tabindex="-1"></a>tweets <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">url</span>(urlfile))</span>
<span id="cb175-6"><a href="support-vector-machines.html#cb175-6" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;http.*&quot;</span>,<span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text)</span>
<span id="cb175-7"><a href="support-vector-machines.html#cb175-7" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;https.*&quot;</span>,<span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text)</span>
<span id="cb175-8"><a href="support-vector-machines.html#cb175-8" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;</span><span class="sc">\\</span><span class="st">$&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-9"><a href="support-vector-machines.html#cb175-9" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;@</span><span class="sc">\\</span><span class="st">w+&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-10"><a href="support-vector-machines.html#cb175-10" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-11"><a href="support-vector-machines.html#cb175-11" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;[ |</span><span class="sc">\t</span><span class="st">]{2,}&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-12"><a href="support-vector-machines.html#cb175-12" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;^ &quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-13"><a href="support-vector-machines.html#cb175-13" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot; $&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-14"><a href="support-vector-machines.html#cb175-14" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;RT&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text) </span>
<span id="cb175-15"><a href="support-vector-machines.html#cb175-15" tabindex="-1"></a>tweets<span class="sc">$</span>text <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;href&quot;</span>, <span class="st">&quot;&quot;</span>, tweets<span class="sc">$</span>text)</span>
<span id="cb175-16"><a href="support-vector-machines.html#cb175-16" tabindex="-1"></a></span>
<span id="cb175-17"><a href="support-vector-machines.html#cb175-17" tabindex="-1"></a>labels <span class="ot">&lt;-</span> tweets<span class="sc">$</span>airline_sentiment</span>
<span id="cb175-18"><a href="support-vector-machines.html#cb175-18" tabindex="-1"></a>labels <span class="ot">&lt;-</span> car<span class="sc">::</span><span class="fu">recode</span>(labels, <span class="st">&quot;&#39;positive&#39;=1;&#39;negative&#39;=-1;&#39;neutral&#39;=0&quot;</span>)</span></code></pre></div>
<p>The goal of the supervised learning task is to use part of this dataset to train a certain algorithm and then use the trained algorithm to assign categories to the remaining sentences. Since we know the coded categories for the remaining sentences, we will be able to evaluate how well this training was in guessing/estimating what the codes for these sentences were. We start by creating a document term matrix;</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="support-vector-machines.html#cb176-1" tabindex="-1"></a>doc_matrix <span class="ot">&lt;-</span> <span class="fu">create_matrix</span>(tweets<span class="sc">$</span>text,</span>
<span id="cb176-2"><a href="support-vector-machines.html#cb176-2" tabindex="-1"></a>                            <span class="at">language =</span> <span class="st">&quot;english&quot;</span>, </span>
<span id="cb176-3"><a href="support-vector-machines.html#cb176-3" tabindex="-1"></a>                            <span class="at">removeNumbers =</span> <span class="cn">TRUE</span>, </span>
<span id="cb176-4"><a href="support-vector-machines.html#cb176-4" tabindex="-1"></a>                            <span class="at">stemWords =</span> <span class="cn">TRUE</span>, </span>
<span id="cb176-5"><a href="support-vector-machines.html#cb176-5" tabindex="-1"></a>                            <span class="at">removeSparseTerms =</span> <span class="fl">0.998</span>)</span>
<span id="cb176-6"><a href="support-vector-machines.html#cb176-6" tabindex="-1"></a>doc_matrix</span></code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 14640, terms: 693)&gt;&gt;
## Non-/sparse entries: 84521/10060999
## Sparsity           : 99%
## Maximal term length: 18
## Weighting          : term frequency (tf)</code></pre>
<p>Note that <code>RTextTools</code> gives you plenty of options in preprocessing. Apart from the options used above, we can also strip whitespace, remove punctuation, and remove stopwords. Stemming and stopword removal is language-specific, so when we select the language in the option above <code>(language=''english'')</code>, <code>RTextTools</code> will carry this out according to our language of choice. As of now, the package supports Danish, Dutch, English, Finnish, French, German, Italian, Norwegian, Portuguese, Russian, Spanish, and Swedish.</p>
<p>We then create a container parsing the document matrix into a training set, and a test set. We will use the training set will to train the algorithm and the test set to test how well this algorithm was trained. The following command instructs R to use the first 4000 sentences for the training set the remaining 449 sentences for the test set. Moreover, we specify to append to the document matrix the variable that contains the assigned coders:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="support-vector-machines.html#cb178-1" tabindex="-1"></a>container <span class="ot">&lt;-</span> <span class="fu">create_container</span>(doc_matrix,</span>
<span id="cb178-2"><a href="support-vector-machines.html#cb178-2" tabindex="-1"></a>                              labels,</span>
<span id="cb178-3"><a href="support-vector-machines.html#cb178-3" tabindex="-1"></a>                              <span class="at">trainSize =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span>, </span>
<span id="cb178-4"><a href="support-vector-machines.html#cb178-4" tabindex="-1"></a>                              <span class="at">testSize =</span> <span class="dv">10001</span><span class="sc">:</span><span class="dv">14640</span>, </span>
<span id="cb178-5"><a href="support-vector-machines.html#cb178-5" tabindex="-1"></a>                              <span class="at">virgin =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>We can then train a model using one of the available algorithms. For instance, we can use the Support Vector Machines algorithm (SVM) as follows:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="support-vector-machines.html#cb179-1" tabindex="-1"></a>SVM <span class="ot">&lt;-</span> <span class="fu">train_model</span>(container, <span class="st">&quot;SVM&quot;</span>)</span></code></pre></div>
<p>Other algorithms are available if you change the SVM option. Options exist for Lasso and Elastic-Net Regularized Generalized Linear Models (<code>GLMNET</code>), maximum entropy (<code>MAXENT</code>), scaled linear discriminant analysis (<code>SLDA</code>), bagging (<code>BAGGING</code>), boosting (<code>BOOSTING</code>), random forests (<code>RF</code>), neural networks (<code>NNET</code>), or classification trees (<code>TREE</code>).</p>
<p>We then use the model we trained to classify the texts in the test set. The following command instructs R to classify the documents in the test set of the container using the SVM model that we trained earlier.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="support-vector-machines.html#cb180-1" tabindex="-1"></a>SVM_CLASSIFY <span class="ot">&lt;-</span> <span class="fu">classify_model</span>(container, SVM)</span></code></pre></div>
<p>We can also view the classification that the SVM model performed as follows. The first column corresponds to the label that coders assigned to each of the tweets in the training set. The second column then gives the probability that the SVM algorithm assigned to that particular category. As you can see, while the probability for some sentences is quite high, for others it is quite low. This even while the classification always chooses the category with the highest probability.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="support-vector-machines.html#cb181-1" tabindex="-1"></a><span class="fu">head</span>(SVM_CLASSIFY)</span></code></pre></div>
<p>The next step is to check the classification performance of our model. To do this, we first request a function that returns a container with different summaries. For instance, we can request summaries based on the labels attached to the sentences, the documents (or in this case, the sentences) by label, or based on the algorithm.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="support-vector-machines.html#cb182-1" tabindex="-1"></a>analytics <span class="ot">&lt;-</span> <span class="fu">create_analytics</span>(container, SVM_CLASSIFY)</span>
<span id="cb182-2"><a href="support-vector-machines.html#cb182-2" tabindex="-1"></a><span class="fu">summary</span>(analytics)</span></code></pre></div>
<pre><code>## ENSEMBLE SUMMARY
## 
##        n-ENSEMBLE COVERAGE n-ENSEMBLE RECALL
## n &gt;= 1                   1               0.8
## 
## 
## ALGORITHM PERFORMANCE
## 
## SVM_PRECISION    SVM_RECALL    SVM_FSCORE 
##     0.6833333     0.6766667     0.6800000</code></pre>
<p>Here, precision gives the proportion of bills that SVM classified as belonging to a category that does belong to that category (true positives) to all the bills that are classified in that category (irrespective of where they belong). Recall, then, is the proportion of bills that SVM classifies as belonging to a category and belong to this category (true positives) to all the bills that belong to this category (true positives plus false negatives). The F score is a weighted average between precision and recall ranging from 0 to 1.</p>
<p>Finally, we can compare the scores between the labels given by the coders and those based on our SVM:</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="support-vector-machines.html#cb184-1" tabindex="-1"></a>compare <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(labels[<span class="dv">10001</span><span class="sc">:</span><span class="dv">14640</span>], SVM_CLASSIFY<span class="sc">$</span>SVM_LABEL))</span>
<span id="cb184-2"><a href="support-vector-machines.html#cb184-2" tabindex="-1"></a><span class="fu">table</span>(compare)</span></code></pre></div>
<pre><code>##     V2
## V1     -1    0    1
##   -1 3018  292  109
##   0   288  347   56
##   1   130   58  342</code></pre>
</div>
<div id="svm-with-quanteda" class="section level3 hasAnchor" number="10.1.2">
<h3><span class="header-section-number">10.1.2</span> SVM with Quanteda<a href="support-vector-machines.html#svm-with-quanteda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Instead of using a separate package, we can also use <code>quanteda</code> to carry out an SVM. For this, we load some movie reviews, select 1000 of them at random, and place them into our corpus:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="support-vector-machines.html#cb186-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb186-2"><a href="support-vector-machines.html#cb186-2" tabindex="-1"></a></span>
<span id="cb186-3"><a href="support-vector-machines.html#cb186-3" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb186-4"><a href="support-vector-machines.html#cb186-4" tabindex="-1"></a><span class="fu">library</span>(quanteda.classifiers)</span>
<span id="cb186-5"><a href="support-vector-machines.html#cb186-5" tabindex="-1"></a>corpus_reviews <span class="ot">&lt;-</span> <span class="fu">corpus_sample</span>(data_corpus_LMRD, <span class="dv">1000</span>)</span></code></pre></div>
<p>Our aim here will be to see how well the SVM algorithm can predict the rating of the reviews. To do this, we first have to create a new variable <code>prediction</code>. This variable contains the same scores as the original rating. Then, we remove 30% of the scores and replace them with NA. We do so by creating a <code>missing</code> variable that contains 30% 0s and 70% 1s. We then place the 0s with NAs. These NA scores are then the ones we want the algorithm to predict. Finally, we add the new variable to the corpus:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="support-vector-machines.html#cb187-1" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> corpus_reviews<span class="sc">$</span>rating</span>
<span id="cb187-2"><a href="support-vector-machines.html#cb187-2" tabindex="-1"></a></span>
<span id="cb187-3"><a href="support-vector-machines.html#cb187-3" tabindex="-1"></a>missing <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="fl">0.7</span>)</span>
<span id="cb187-4"><a href="support-vector-machines.html#cb187-4" tabindex="-1"></a>prediction[missing <span class="sc">==</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb187-5"><a href="support-vector-machines.html#cb187-5" tabindex="-1"></a></span>
<span id="cb187-6"><a href="support-vector-machines.html#cb187-6" tabindex="-1"></a><span class="fu">docvars</span>(corpus_reviews, <span class="st">&quot;prediction&quot;</span>) <span class="ot">&lt;-</span> prediction</span></code></pre></div>
<p>We then transform the corpus into a data frame, and also remove stopwords, numbers and punctuation:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="support-vector-machines.html#cb188-1" tabindex="-1"></a>data_reviews_tokens <span class="ot">&lt;-</span> <span class="fu">tokens</span>(</span>
<span id="cb188-2"><a href="support-vector-machines.html#cb188-2" tabindex="-1"></a> corpus_reviews,</span>
<span id="cb188-3"><a href="support-vector-machines.html#cb188-3" tabindex="-1"></a> <span class="at">what =</span> <span class="st">&quot;word&quot;</span>,</span>
<span id="cb188-4"><a href="support-vector-machines.html#cb188-4" tabindex="-1"></a> <span class="at">remove_punct =</span> <span class="cn">TRUE</span>,</span>
<span id="cb188-5"><a href="support-vector-machines.html#cb188-5" tabindex="-1"></a> <span class="at">remove_symbols =</span> <span class="cn">TRUE</span>,</span>
<span id="cb188-6"><a href="support-vector-machines.html#cb188-6" tabindex="-1"></a> <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>,</span>
<span id="cb188-7"><a href="support-vector-machines.html#cb188-7" tabindex="-1"></a> <span class="at">remove_url =</span> <span class="cn">TRUE</span>,</span>
<span id="cb188-8"><a href="support-vector-machines.html#cb188-8" tabindex="-1"></a> <span class="at">remove_separators =</span> <span class="cn">TRUE</span>,</span>
<span id="cb188-9"><a href="support-vector-machines.html#cb188-9" tabindex="-1"></a> <span class="at">split_hyphens =</span> <span class="cn">FALSE</span>,</span>
<span id="cb188-10"><a href="support-vector-machines.html#cb188-10" tabindex="-1"></a> <span class="at">include_docvars =</span> <span class="cn">TRUE</span>,</span>
<span id="cb188-11"><a href="support-vector-machines.html#cb188-11" tabindex="-1"></a> <span class="at">padding =</span> <span class="cn">FALSE</span>,</span>
<span id="cb188-12"><a href="support-vector-machines.html#cb188-12" tabindex="-1"></a> <span class="at">verbose =</span> <span class="cn">TRUE</span></span>
<span id="cb188-13"><a href="support-vector-machines.html#cb188-13" tabindex="-1"></a>)</span>
<span id="cb188-14"><a href="support-vector-machines.html#cb188-14" tabindex="-1"></a>data_reviews_tokens <span class="ot">&lt;-</span> <span class="fu">tokens_tolower</span>(data_reviews_tokens, <span class="at">keep_acronyms =</span> <span class="cn">FALSE</span>)</span>
<span id="cb188-15"><a href="support-vector-machines.html#cb188-15" tabindex="-1"></a>data_reviews_tokens <span class="ot">&lt;-</span> <span class="fu">tokens_select</span>(data_reviews_tokens, <span class="fu">stopwords</span>(<span class="st">&quot;english&quot;</span>), <span class="at">selection =</span> <span class="st">&quot;remove&quot;</span>)</span>
<span id="cb188-16"><a href="support-vector-machines.html#cb188-16" tabindex="-1"></a>dfm_reviews <span class="ot">&lt;-</span> <span class="fu">dfm</span>(data_reviews_tokens)</span></code></pre></div>
<p>Now we can run the SVM algorithm. To do so, we tell the model on which dfm we want to run our model, and which variable contains the scores to train the algorithm. Here, this is our <code>prediction</code> variable with the missing data:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="support-vector-machines.html#cb189-1" tabindex="-1"></a><span class="fu">library</span>(quanteda.textmodels)</span>
<span id="cb189-2"><a href="support-vector-machines.html#cb189-2" tabindex="-1"></a>svm_reviews <span class="ot">&lt;-</span> <span class="fu">textmodel_svm</span>(dfm_reviews, <span class="at">y =</span> <span class="fu">docvars</span>(dfm_reviews, <span class="st">&quot;prediction&quot;</span>))</span>
<span id="cb189-3"><a href="support-vector-machines.html#cb189-3" tabindex="-1"></a>svm_reviews</span></code></pre></div>
<pre><code>## 
## Call:
## textmodel_svm.dfm(x = dfm_reviews, y = docvars(dfm_reviews, &quot;prediction&quot;))
## 
## 672 training documents; 121,240 fitted features.
## Method: L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)</code></pre>
<p>Here we see that the algorithm used 672 texts to train the model (the one with a score) and fitted 133,728 features. The latter refers to the total number of words in the training texts and not only the unique ones. Now we can use this model to predict the ratings we removed earlier:</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="support-vector-machines.html#cb191-1" tabindex="-1"></a>svm_predict <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_reviews)</span></code></pre></div>
<p>While we can of course look at the resulting numbers, we can also place them in a two-way table with the actual rating, to see how well the algorithm did:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="support-vector-machines.html#cb192-1" tabindex="-1"></a>rating <span class="ot">&lt;-</span> corpus_reviews<span class="sc">$</span>rating</span>
<span id="cb192-2"><a href="support-vector-machines.html#cb192-2" tabindex="-1"></a>table_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(svm_predict, rating))</span>
<span id="cb192-3"><a href="support-vector-machines.html#cb192-3" tabindex="-1"></a><span class="fu">table</span>(table_data<span class="sc">$</span>svm_predict,table_data<span class="sc">$</span>rating)</span></code></pre></div>
<pre><code>##     
##        1   2   3   4   7   8   9  10
##   1  172  15   9  16   5   3   3   3
##   2    7  69   6   5   2   2   1   3
##   3    7   0  82   3   1   3   0   1
##   4    5   2   5  86   7   6   0   4
##   7    3   1   1   1  55   3   3   2
##   8    0   2   2   2   8  90   7   7
##   9    4   0   0   3   6   9  76  12
##   10   5   2   4   2   7   6   6 138</code></pre>
<p>Here, the table shows the prediction of the algorithm from top to bottom and the original rating from left to right. What we want is that all cases are on the diagonal: in that case, the prediction is the same as the original rating. Here, this happens in the majority of cases. Also, only in a few cases is the algorithm far off.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="supervised-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="naive-bayes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
