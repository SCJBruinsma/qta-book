<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="5.4 Text Statistics | Quantitative Text Analysis" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Theory and Methods for Quantitative Text Analysis" />


<meta name="author" content="Kostas Gemenis &amp; Bastiaan Bruinsma" />

<meta name="date" content="2021-07-28" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Theory and Methods for Quantitative Text Analysis">

<title>5.4 Text Statistics | Quantitative Text Analysis</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 2em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#welcome">Welcome!</a></li>
<li><a href="1-getting-started.html#getting-started"><span class="toc-section-number">1</span> Getting Started</a>
<ul>
<li><a href="1-1-r-on-windows.html#r-on-windows"><span class="toc-section-number">1.1</span> R on Windows</a></li>
<li><a href="1-2-r-on-linux.html#r-on-linux"><span class="toc-section-number">1.2</span> R on Linux</a></li>
<li><a href="1-3-r-on-macos.html#r-on-macos"><span class="toc-section-number">1.3</span> R on macOS</a></li>
<li><a href="1-4-r-in-the-cloud.html#r-in-the-cloud"><span class="toc-section-number">1.4</span> R in the Cloud</a></li>
</ul></li>
<li><a href="2-installing-packages.html#installing-packages"><span class="toc-section-number">2</span> Installing Packages</a>
<ul>
<li><a href="2-1-installing-from-cran.html#installing-from-cran"><span class="toc-section-number">2.1</span> Installing from CRAN</a></li>
<li><a href="2-2-installing-from-github.html#installing-from-github"><span class="toc-section-number">2.2</span> Installing from GitHub</a></li>
<li><a href="2-3-packages-for-quantitative-text-analysis-in-r.html#packages-for-quantitative-text-analysis-in-r"><span class="toc-section-number">2.3</span> Packages for Quantitative Text Analysis in R</a></li>
<li><a href="2-4-issues-bugs-and-errors.html#issues-bugs-and-errors"><span class="toc-section-number">2.4</span> Issues, Bugs and Errors</a></li>
</ul></li>
<li><a href="3-importing-data.html#importing-data"><span class="toc-section-number">3</span> Importing Data</a>
<ul>
<li><a href="3-1-text-in-r.html#text-in-r"><span class="toc-section-number">3.1</span> Text in R</a></li>
<li><a href="3-2-import-txt-files.html#import-.txt-files"><span class="toc-section-number">3.2</span> Import .txt Files</a></li>
<li><a href="3-3-import-pdf-files.html#import-.pdf-files"><span class="toc-section-number">3.3</span> Import .pdf Files</a></li>
<li><a href="3-4-import-csv-files.html#import-.csv-files"><span class="toc-section-number">3.4</span> Import .csv Files</a></li>
<li><a href="3-5-import-from-an-api.html#import-from-an-api"><span class="toc-section-number">3.5</span> Import from an API</a></li>
<li><a href="3-6-import-using-web-scraping.html#import-using-web-scraping"><span class="toc-section-number">3.6</span> Import using Web Scraping</a></li>
</ul></li>
<li><a href="4-reliability-and-validity.html#reliability-and-validity"><span class="toc-section-number">4</span> Reliability and Validity</a>
<ul>
<li><a href="4-1-inter-coder-agreement.html#inter-coder-agreement"><span class="toc-section-number">4.1</span> Inter-Coder Agreement</a></li>
<li><a href="4-2-visualizing-quality.html#visualizing-quality"><span class="toc-section-number">4.2</span> Visualizing Quality</a></li>
</ul></li>
<li><a href="5-preliminaries.html#preliminaries"><span class="toc-section-number">5</span> Preliminaries</a>
<ul>
<li><a href="5-1-the-corpus.html#the-corpus"><span class="toc-section-number">5.1</span> The Corpus</a></li>
<li><a href="5-2-keywords-in-context.html#keywords-in-context"><span class="toc-section-number">5.2</span> Keywords in Context</a></li>
<li><a href="5-3-visualisations-and-descriptives.html#visualisations-and-descriptives"><span class="toc-section-number">5.3</span> Visualisations and Descriptives</a></li>
<li><a href="5-4-text-statistics.html#text-statistics"><span class="toc-section-number">5.4</span> Text Statistics</a></li>
</ul></li>
<li><a href="6-dictionary-analysis.html#dictionary-analysis"><span class="toc-section-number">6</span> Dictionary Analysis</a>
<ul>
<li><a href="6-1-classical-dictionary-analysis.html#classical-dictionary-analysis"><span class="toc-section-number">6.1</span> Classical Dictionary Analysis</a></li>
<li><a href="6-2-sentiment-analysis.html#sentiment-analysis"><span class="toc-section-number">6.2</span> Sentiment Analysis</a>
<ul>
<li><a href="6-2-sentiment-analysis.html#movie-reviews"><span class="toc-section-number">6.2.1</span> Movie Reviews</a></li>
<li><a href="6-2-sentiment-analysis.html#twitter"><span class="toc-section-number">6.2.2</span> Twitter</a></li>
</ul></li>
</ul></li>
<li><a href="7-scaling.html#scaling"><span class="toc-section-number">7</span> Scaling</a>
<ul>
<li><a href="7-1-wordscores.html#wordscores"><span class="toc-section-number">7.1</span> Wordscores</a></li>
<li><a href="7-2-wordfish.html#wordfish"><span class="toc-section-number">7.2</span> Wordfish</a></li>
<li><a href="7-3-correspondence-analysis.html#correspondence-analysis"><span class="toc-section-number">7.3</span> Correspondence Analysis</a></li>
</ul></li>
<li><a href="8-supervised-methods.html#supervised-methods"><span class="toc-section-number">8</span> Supervised Methods</a>
<ul>
<li><a href="8-1-support-vector-machines.html#support-vector-machines"><span class="toc-section-number">8.1</span> Support Vector Machines</a>
<ul>
<li><a href="8-1-support-vector-machines.html#svm-with-rtexttools"><span class="toc-section-number">8.1.1</span> SVM with RTextTools</a></li>
<li><a href="8-1-support-vector-machines.html#svm-with-quanteda"><span class="toc-section-number">8.1.2</span> SVM with Quanteda</a></li>
</ul></li>
<li><a href="8-2-naive-bayes.html#naive-bayes"><span class="toc-section-number">8.2</span> Naive Bayes</a></li>
</ul></li>
<li><a href="9-unsupervised-methods.html#unsupervised-methods"><span class="toc-section-number">9</span> Unsupervised Methods</a>
<ul>
<li><a href="9-1-latent-dirichlet-allocation.html#latent-dirichlet-allocation"><span class="toc-section-number">9.1</span> Latent Dirichlet Allocation</a></li>
<li><a href="9-2-seeded-latent-dirichlet-allocation.html#seeded-latent-dirichlet-allocation"><span class="toc-section-number">9.2</span> Seeded Latent Dirichlet Allocation</a></li>
<li><a href="9-3-structural-topic-model.html#structural-topic-model"><span class="toc-section-number">9.3</span> Structural Topic Model</a></li>
</ul></li>
<li><a href="10-texttricks.html#texttricks"><span class="toc-section-number">10</span> Texttricks</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="text-statistics" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Text Statistics</h2>
<p>Finally, <em>quanteda</em> also allows us to calculate quite some textual statistics. These are all collected in the <em>quanteda.textstats</em> helper package. Here, we will look at several of them, starting with a simple overview of our corpus in the terms of a summary. This tells us the number of characters, sentences, tokens, etc. for each of the texts:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="5-4-text-statistics.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textstats)</span>
<span id="cb86-2"><a href="5-4-text-statistics.html#cb86-2" aria-hidden="true" tabindex="-1"></a>corpus_summary <span class="ot">&lt;-</span> <span class="fu">textstat_summary</span>(corpus_inaugural)</span></code></pre></div>
<p>If we want, we can then use this data to make some simple graphs telling us various things about the texts in our corpus. As an example, let’s look at the number of sentences the various presidents put in their speeches:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="5-4-text-statistics.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>corpus_summary, <span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>sents, <span class="at">group=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb87-2"><a href="5-4-text-statistics.html#cb87-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb87-3"><a href="5-4-text-statistics.html#cb87-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb87-4"><a href="5-4-text-statistics.html#cb87-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">ylab</span>(<span class="st">&quot;Number of Characters&quot;</span>)<span class="sc">+</span></span>
<span id="cb87-5"><a href="5-4-text-statistics.html#cb87-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">xlab</span>(<span class="st">&quot;President/Year&quot;</span>)<span class="sc">+</span></span>
<span id="cb87-6"><a href="5-4-text-statistics.html#cb87-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">theme_classic</span>()<span class="sc">+</span></span>
<span id="cb87-7"><a href="5-4-text-statistics.html#cb87-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>))</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-58-1.png" width="672"  /></p>
<p>Other things we can look at are the readability and lexical diversity of the texts. The former one of these refers to how readable a text is (i.e. how easy or difficult it is to read), while the latter tells us how many different types of words there are in the texts and thus how “diverse” the text is in word choice and use. Given that there are many ways to calculate both metrics, please have a look at the help file to see which one works best for you. Here, we will use the most popular:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="5-4-text-statistics.html#cb88-1" aria-hidden="true" tabindex="-1"></a>corpus_readability <span class="ot">&lt;-</span> <span class="fu">textstat_readability</span>(corpus_inaugural, <span class="at">measure =</span> <span class="fu">c</span>(<span class="st">&quot;Flesch.Kincaid&quot;</span>, <span class="st">&quot;Dale.Chall.old&quot;</span>))</span>
<span id="cb88-2"><a href="5-4-text-statistics.html#cb88-2" aria-hidden="true" tabindex="-1"></a>corpus_lexdiv <span class="ot">&lt;-</span> <span class="fu">textstat_lexdiv</span>(data_inaugural_tokens, <span class="fu">c</span>(<span class="st">&quot;CTTR&quot;</span>, <span class="st">&quot;TTR&quot;</span>, <span class="st">&quot;MATTR&quot;</span>), <span class="at">MATTR_window =</span> <span class="dv">100</span>)</span></code></pre></div>
<p>As before, we can plot this data in a graph to see how lexical diversity developed over time:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="5-4-text-statistics.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>corpus_lexdiv, <span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>CTTR, <span class="at">group=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb89-2"><a href="5-4-text-statistics.html#cb89-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb89-3"><a href="5-4-text-statistics.html#cb89-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb89-4"><a href="5-4-text-statistics.html#cb89-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">ylab</span>(<span class="st">&quot;Lexical Diversity (CTTR)&quot;</span>)<span class="sc">+</span></span>
<span id="cb89-5"><a href="5-4-text-statistics.html#cb89-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">xlab</span>(<span class="st">&quot;President/Year&quot;</span>)<span class="sc">+</span></span>
<span id="cb89-6"><a href="5-4-text-statistics.html#cb89-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">theme_classic</span>()<span class="sc">+</span></span>
<span id="cb89-7"><a href="5-4-text-statistics.html#cb89-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>))</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-60-1.png" width="672"  /></p>
<p>Another thing we can do is look at the similarities and distances between documents. With this, we can answer questions such as: how “different” are these documents from each other? And if different (or similar), how different (or similar)? The idea is that the larger the similarity is, the smaller the distance is as well. A good way to understand the idea of similarity is to consider how many operations you need to perform to change one text into the other. The more “replace” options you have to carry out, the more different the text. As for the distances, it is best to consider the texts as having positions on a Cartesian plane (with positions based on their word counts). The distance between these two points (either Euclidean, Manhattan or other) is then the distance between the texts.</p>
<p>Let’s start with a look at these similarities (note again that there are many different methods to calculate this):</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="5-4-text-statistics.html#cb90-1" aria-hidden="true" tabindex="-1"></a>corpus_similarties <span class="ot">&lt;-</span> <span class="fu">textstat_simil</span>(data_inaugural_dfm, <span class="at">method =</span> <span class="st">&quot;correlation&quot;</span>, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>)</span>
<span id="cb90-2"><a href="5-4-text-statistics.html#cb90-2" aria-hidden="true" tabindex="-1"></a>corpus_similarties <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_similarties)</span></code></pre></div>
<p>A brief look at these results tells us that the 1981 and 1985 Reagan speeches show the highest degree of similarity, while the
1945 Roosevelt and 2017 Trump speeches are the most different. Note that while we look here at the documents, we could also look at individual words (set <code>margin="features</code>). For now, let us look at the distances between the documents, choosing the Euclidean distance between the documents as our metric:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="5-4-text-statistics.html#cb91-1" aria-hidden="true" tabindex="-1"></a>corpus_distances <span class="ot">&lt;-</span> <span class="fu">textstat_dist</span>(data_inaugural_dfm, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb91-2"><a href="5-4-text-statistics.html#cb91-2" aria-hidden="true" tabindex="-1"></a>corpus_distances_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_distances)</span></code></pre></div>
<p>Here, we find the 1905 and 1945 Roosevelt speeches (the two different Roosevelts) to be the closest, and the 1909 Taft and 1997 Clinton speeches to be furthest apart. If we want to, we can even convert this data into a dendrogram. We do this by taking the information on the distances out of the <code>corpus_distances</code> object, make them into a triangular matrix, and plot them:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="5-4-text-statistics.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">hclust</span>(<span class="fu">as.dist</span>(corpus_distances)), <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-63-1.png" width="672"  /></p>
<p>Here, we can see that - amongst others - the 1909 Taft speech is the “farthest” away from all the others. Also, while the 1981 and 1985 Reagan speeches were very close, the 1997 Clinton speech was closer to Nixon’s speeches than his 1993 speech (which was close to the 2009 and 2013 Obama speeches).</p>
<p>Finally, let us look at the entropy of our texts. The entropy of a document measures the “amount” of information each letter of the text produces. To get an idea of what this means, consider the “e” is an often occurring letter in an English text, while “z” is not. Thus, a word with a “z” in it, it more unique and thus likely to carry unique and interesting information. The “higher” the entropy of a text, the less “information” is in it:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="5-4-text-statistics.html#cb93-1" aria-hidden="true" tabindex="-1"></a>corpus_entropy_docs <span class="ot">&lt;-</span> <span class="fu">textstat_entropy</span>(data_inaugural_dfm, <span class="st">&quot;documents&quot;</span>)</span>
<span id="cb93-2"><a href="5-4-text-statistics.html#cb93-2" aria-hidden="true" tabindex="-1"></a>corpus_entropy_docs <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_entropy_docs)</span></code></pre></div>
<p>As we can see, the Roosevelt speeches had the lowest entropies, while the 1909 Taft and 1925 Coolidge speeches were the highest (in relative terms). While not as common as the other distance metrics, entropy is sometimes used to measure the similarity between texts. Thus, it can be useful if we want to know the importance of certain words. This is because if a certain word is not “important,” we could consider it to be a stop word:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="5-4-text-statistics.html#cb94-1" aria-hidden="true" tabindex="-1"></a>corpus_entropy_feats <span class="ot">&lt;-</span> <span class="fu">textstat_entropy</span>(data_inaugural_dfm, <span class="st">&quot;features&quot;</span>)</span>
<span id="cb94-2"><a href="5-4-text-statistics.html#cb94-2" aria-hidden="true" tabindex="-1"></a>corpus_entropy_feats <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_entropy_feats)</span>
<span id="cb94-3"><a href="5-4-text-statistics.html#cb94-3" aria-hidden="true" tabindex="-1"></a>corpus_entropy_feats <span class="ot">&lt;-</span> corpus_entropy_feats[<span class="fu">order</span>(<span class="sc">-</span>corpus_entropy_feats<span class="sc">$</span>entropy),]</span>
<span id="cb94-4"><a href="5-4-text-statistics.html#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(corpus_entropy_feats, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##     feature  entropy
## 164  people 4.766391
## 488    life 4.747627
## 385  nation 4.737440
## 5     great 4.654392
## 114     can 4.651396
## 317  future 4.639222
## 197   world 4.616910
## 212    time 4.616614
## 402    must 4.610073
## 231     god 4.601430</code></pre>
<p>Looking at the data, we find that “people,” “life” and “nation” have pretty high entropies. This indicates that the words added little to the information of the documents, and would-be candidates for removal from our corpus.</p>

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="5-3-visualisations-and-descriptives.html"><button class="btn btn-default">Previous</button></a>
<a href="6-dictionary-analysis.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
