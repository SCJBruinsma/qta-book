% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Quantitative Text Analysis},
  pdfauthor={Kostas Gemenis \& Bastiaan Bruinsma},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{fit,positioning,arrows.meta}

\tikzset{
connect/.style={arrows=-{Triangle}, black, thick},
main/.style={circle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm},
plate/.style={draw, shape=rectangle,  thick, minimum width=3.1cm, text width=3.1cm, align=right,inner sep=10pt, inner ysep=10pt, append after command={node[above left= 3pt of \tikzlastnode.south east] {#1}}}
}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Quantitative Text Analysis}
\author{Kostas Gemenis \& Bastiaan Bruinsma}
\date{2021-07-21}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{foreword}{%
\chapter{Foreword}\label{foreword}}

Welcome to the Quantitative Content Analysis Textbook. This book was originally written as a collection of assignments and slides for the ECPR Winter and Summer Schools. Note that the book is still in active development.

The developments over the past 20 years have made research using quantitative text analysis a particularly exciting undertaking.

First of all, the enormous increase in computing power has made it possible to work with large bodies of text. Secondly, the development of R, a free, open-source, cross-platform statistical software has enabled many researchers and programmers to develop particular packages that implement statistical methods of working with text. In addition, the spread of the Internet has made available in digital format many interesting sources of textual data. To these, we should add the emergence of social media as a massive source of text which is generated daily, by millions of users across the world.

Yet, quantitative text analysis can be a daunting experience for someone who is not familiar with quantitative methods or programming. This book will guide you, with step-by-step explanation of the code, through a series of exercises illustrating a wide range of text analysis methods. Many of these exercises have been given to participants in the ECPR Summer and Winter Schools in Methods and Techniques whom, often, had no prior experience in text analysis, R, or quantitative methods. Therefore, we hope that you will find the exercises easy to understand but also engaging.

\hypertarget{installing-r}{%
\section{Installing R}\label{installing-r}}

R is an open-source programme that allows you to carry out a wide variety of statistical tasks. At its core, it is a modification of the programming languages S and Scheme, making it not only flexible but fast as well. R is available for Windows, Linux and OS X and receives regular updates. In its basic version, R uses a simple command-line interface. To give it a friendlier look, integrated development environments (IDEs) such as RStudio are available. Apart from looking better, these environments also provide some extra practical features.

\hypertarget{r-on-windows}{%
\subsection{R on Windows}\label{r-on-windows}}

To install R on Windows, go to \url{https://cran.r-project.org/bin/windows/base/}, download the file, double-click it and run it. Whilst installing, it is best to leave standard options (such as the installation folder) unchanged. This makes it easier for other programmes to know where to find R. Once installed, you will find two shortcuts for R on your desktop. These refer to each of the two versions of R that come with the installation - the 32-bit and the 64-bit version. Which version you need depends on your version of Windows. To see which version of Windows you have, go to This PC (or My Computer, right-click it, and select Properties. Here you should find the version of Windows installed on your PC. If you have the 64-bit version of Windows, you can use both versions. Yet, it is best to use the 64-bit version as this makes better use of the memory of your computer and thus runs smoother. If you have the 32-bit version of Windows, you have to use the 32-bit version of R.

To install RStudio, go to \url{https://www.rstudio.com/products/rstudio/download/}, and download the free version of RStudio at the bottom of the page. Make sure to choose \textbf{Installers for Supported Platforms} and pick the option for Windows. Once downloaded, install the programme, leaving all settings unchanged. If everything works out fine, RStudio will have found your installation of R and placed a shortcut on the desktop. Whether you have the 32-bit or 64-bit version of Windows or R does not matter for RStudio. What does matter are the slashes. R uses forward slashes (/) instead of the backslashes (\texttt{\textbackslash{}}) that Windows uses. Thus, whenever you specify a folder or file within R, make sure to invert the slashes. So, you should refer to a file which in Windows has the address \textbf{C:\textbackslash Users\textbackslash Desktop\textbackslash data.csv} as \textbf{C:/Users/Desktop/data.csv}.

\hypertarget{r-on-linux}{%
\subsection{R on Linux}\label{r-on-linux}}

How to install R on Linux depends on which flavour of Linux you have. In most cases, R is already part of your Linux distribution. You can check this by opening a terminal and typing \texttt{R}. If installed, R will launch in the terminal. If R is not part of your system, run the following in the terminal:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  sudo add-apt-repository `deb \url{https://cloud.r-project.org/bin/linux/ubuntu} focal-cran40/'
\item
  sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
\item
  sudo apt update
\item
  sudo apt install r-base r-base-core r-recommended r-base-dev
\end{enumerate}

As an alternative, you can use the Synaptic Package Manager and look for the \textbf{r-base-dev} and \textbf{r-base} packages. Select them, and install them.

To install RStudio, go to \url{https://www.rstudio.com/products/rstudio/download/}. At the bottom of the page, pick the installer that corresponds to your OS. Then, install the file either through an installation manager or via the terminal. After running the launcher, you can find RStudio in the Dash.

\hypertarget{r-on-macos}{%
\subsection{R on macOS}\label{r-on-macos}}

With OS X you must have OS X 10.6 (Snow Leopard) or above. Installing R otherwise is still possible, but you cannot use a certain number of packages (such as some we use here). To check this, click on the Apple icon in the top-left of your screen. Then, click on the ``About This Mac'' options. A window should then appear that tells you which version of OS X (or macOS) you have.

To install R, go to \url{https://cran.r-project.org/index.html} and click \textbf{Download R for (Mac) OS X}. Once there, download the .pkg file that corresponds to your version of OS X and install it. Besides, you have to download the \textbf{Clang 6.x compiler} and the \textbf{GNU Fortran compiler} from \url{https://cran.r-project.org/bin/macosx/tools/}. Install both and leave the selected options as they are. After the installation, check if R works by launching the programme.

To install RStudio, go to \url{https://www.rstudio.com/products/rstudio/download/} and download the OSX version at the bottom of the page. After downloading and installing, you can now find RStudio with your other programmes.

\hypertarget{r-in-the-cloud}{%
\subsection{R in the Cloud}\label{r-in-the-cloud}}

Aside form installing R on your own system, you can also choose to use its cloud version. This version is hosted by RStudio on \url{https://rstudio.cloud/}. To use it, go to the Sign-Up button in the top-right of the screen. Then, select the \emph{Cloud Free} option and once again select Sign-Up. Then, finish the procedure either by filling in your data, or connecting with your Google or GitHub account. Once done, log-in, and you will arrive at your workspace. To get started, you need to make a new project. To do so, click the \emph{New Project} button which takes you to an instance of RStudio. From here on, the programme functions the same as its Desktop version. Note that everything you do - or packages you install - in the project \emph{remain} in the project. Thus, you will have to re-install them if you want to create a new project. Besides, note that RStudio Cloud is quite dependent on both the number of users on the server and your internet connection. Thus, some actions (such as installing packages) might take longer to run.

\hypertarget{installing-packages}{%
\chapter{Installing Packages}\label{installing-packages}}

R on its own is a pretty bare-bones experience. What makes it work are the many packages that exist for it. These packages come in two kinds: officially released or in development.

\hypertarget{installing-from-cran}{%
\section{Installing from CRAN}\label{installing-from-cran}}

To be officially released, the package needs to be part of CRAN: the Comprehensive R Archive Network. CRAN is a website that collects and hosts all the material R needs, such as the different distributions, packages, and more. Besides, any package on CRAN has gone through a vetting process. This ensures that the package does not contain any major bugs, has README and NEWS files, and has a clear version number. Many official released packages also have additional documentation and motivating examples published in journals such as \emph{The R Journal} and \emph{The Journal of Statistical Software}. Also, a package being published in CRAN allows us to install the package using the \texttt{install.packages} command, or the \textbf{Packages} tab in RStudio. Besides, packages on CRAN often receive updates on a regular basis. These updates can add new features to the package, address bugs, or increase performance. To update your packages, go to the \textbf{Packages} tab in RStudio and click on the \textbf{Update} button.

\hypertarget{installing-from-github}{%
\section{Installing from GitHub}\label{installing-from-github}}

Some packages that have not yet had an official release are in development on GitHub (\url{https://github.com/}). As a result, these packages change very often and are more unstable as their official counterparts. We can, nevertheless, install packages from Github using the \texttt{devtools} package. To install this, type:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{, }\AttributeTok{dependencies=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{dependencies=TRUE} ensures that if we need other packages to make \texttt{devtools} work, R will install these as well. Depending on your operating system, you might have to install some other software for \texttt{devtools} to work.

On Windows, \texttt{devtools} requires the \emph{RTools} software. To install this, go to \url{https://cran.r-project.org/bin/windows/Rtools/}, download the latest \emph{recommended} version (in green), and install it. Then re-open R again and install \texttt{devtools} as shown above.

On Linux, how \texttt{devtools} installs depends on the flavour of Linux that you have. Most often, installing it as shown above will work fine. If not, the problem is most likely a missing package in your Linux distribution. To address this, close R, open a terminal and type:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  sudo apt-get update
\item
  sudo apt-get upgrade
\item
  sudo apt install build-essential libcurl4-gnutls-dev libxml2-dev libssl-dev
\item
  Close the terminal, open R, and install \texttt{devtools} as shown above.
\end{enumerate}

On OSX (or macOS), \texttt{devtools} requires the \emph{XCode} software. To install this, follow these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Launch the terminal (which you can find in /Applications/Utilities/), and type:
\item
  In the terminal, type: \textbf{xcode-select --install}
\item
  A software update window should pop up. Here, click ``Install'' and agree to the Terms of Service.
\item
  Go to R and install \texttt{devtools} as shown above.
\end{enumerate}

\hypertarget{quantitative-text-analysis-in-r}{%
\section{Quantitative Text Analysis in R}\label{quantitative-text-analysis-in-r}}

While we will be using several different packages to run quantitative text analysis, we will mostly use \texttt{quanteda} (\protect\hyperlink{ref-Benoit2018}{Benoit et al.} (\protect\hyperlink{ref-Benoit2018}{2018})). \texttt{quanteda} integrates many of the text analysis functions of R that were before spread out over many different packages (see, for example \protect\hyperlink{ref-Welbers2017}{Welbers, Van Atteveldt, and Benoit} (\protect\hyperlink{ref-Welbers2017}{2017})). Besides, it is easy to combine with other packages, has simple and logical commands, and a well-maintained website (www.quanteda.io).

The current version of \texttt{quanteda} as of writing is \texttt{packageVersion("quanteda")}. This version works best with R version 4.0.1 or higher. To check if your system has this, type \texttt{R.Version()} in your console. The result will be a list. Look for \texttt{\$version.string} to see which version number your version of R is. If you do not have the latest version, see the steps above on how to download this.

To install the package, type:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"quanteda"}\NormalTok{, }\AttributeTok{dependencies=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that because we wrote \texttt{dependencies=TRUE}, this command also installed three other \texttt{quanteda} helper packages that serve to expand upon the basic tools that are already within \texttt{quanteda}. In the future, more of these helper packages will be added to expand the package even more. Yet, before these packages get an official release, we can already find them, in development, on GitHub. Here, we install two of them - \texttt{quanteda.classifiers} which we will use for supervised learning methods, and \texttt{quanteda.dictionaries} which we will use for dictionary analysis:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(devtools)}
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"quanteda/quanteda.classifiers"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"kbenoit/quanteda.dictionaries"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Apart from \texttt{quanteda} we then need these other packages as well:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"mikegruz/kripp.boot"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"ca"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"combinat"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"DescTools"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"FactoMineR"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"factoextra"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"Hmisc"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"httr"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"jsonlite"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"manifestoR"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readr"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readtext"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"reshape2"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"RTextTools"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"R.temis"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"rvest"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"seededlda"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"stm"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After installation, you will find these packages, as well as the \texttt{quanteda} and \texttt{devtools} packages, under the \textbf{Packages} tab in RStudio.

\hypertarget{issues-bugs-and-errors}{%
\section{Issues, Bugs and Errors}\label{issues-bugs-and-errors}}

As it is free software, errors are not uncommon in R. Often they arise when you misspell the code or use the wrong code for the job at hand. In these cases, R prints a message (in red) telling you why it cannot do what you ask of it. Sometimes, this message is quite clear, such as telling you to install an extra package. Other times, it is more complicated and requires some extra work. In these cases, there are four questions you can ask yourself:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Did I load all the packages I need?
\item
  Are all packages up-to-date?
\item
  Did I spell the commands correct?
\item
  Is the data in the right shape or format?
\end{enumerate}

If none of these provides a solution, you can always look up online if others have run into the same issue. Often, copy-pasting your error into a search engine can provide you with other instances, and most often a solution. One well-known place for solutions is Stack Overflow (\url{https://stackoverflow.com/}). Here, you can share your problem with others and see if someone can offer a solution. Make sure though to read through the problems already posted first, to ensure that you do not post the same problem twice.

\hypertarget{importing-data}{%
\chapter{Importing Data}\label{importing-data}}

No analysis is possible unless we have some data to work with. In the following exercises, we will look at five different ways to get textual data into R: a) by using .pdf files, b) by using .txt files, c) by using .csv files, d) by using web scraping, and e) by using an API. Before we get to these methods, we will look at how R handles text and how we can work with it.

\hypertarget{text-in-r}{%
\section{Text in R}\label{text-in-r}}

R sees any form of text as a type of characters vector. In their simplest form, these vectors only have a single character in it. At their most complicated, they can contain many sentences or even whole stories. To see how many characters a vector has, we can use the \texttt{nchar} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector1 }\OtherTok{\textless{}{-}} \StringTok{"This is the first of our character vectors"}
\FunctionTok{nchar}\NormalTok{(vector1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 42
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(vector1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

This example also shows the logic of R. First, we assign the text we have to a certain object. We do so using the \texttt{\textless{}-} arrow. This arrow points from the text we have to the object R stores it in, which we here call \texttt{vector1}. We then ask R to give us the number of characters inside this object, 40 in this case. The \texttt{length} command returns something else, namely 1. This means that we have a single sentence, or word, in our object. If we want to, we can place more sentences inside our object using the \texttt{c()} option:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"This is an example"}\NormalTok{, }\StringTok{"This is another"}\NormalTok{, }\StringTok{"And so we can go on."}\NormalTok{)}
\FunctionTok{length}\NormalTok{(vector2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nchar}\NormalTok{(vector2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18 15 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(vector2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 53
\end{verbatim}

Another thing we can do is extract certain words from a sentence. For this, we use the \texttt{substr()} function. With this function, R gives us all the characters that occur between two specific positions. So, when we want the characters between the 4th and 10th characters, we write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector3 }\OtherTok{\textless{}{-}} \StringTok{"This is yet another sentence"}
\FunctionTok{substr}\NormalTok{(vector3, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "s is ye"
\end{verbatim}

We can also split a character vector into smaller parts. We often do this when we want to split a longer text into several sentences. To do so, we use the \texttt{strsplit} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector3 }\OtherTok{\textless{}{-}} \StringTok{"Here is a sentence {-} And a second"}
\NormalTok{parts1 }\OtherTok{\textless{}{-}} \FunctionTok{strsplit}\NormalTok{(vector3, }\StringTok{"{-}"}\NormalTok{)}
\NormalTok{parts1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] "Here is a sentence " " And a second"
\end{verbatim}

If we now look in the Environment window, we will see that R calls \texttt{parts1} a list. This is another type of object that R uses to store information. We will see it more often later on. For now, it is good to remember that lists in R can have many vectors (the layers of the list) and that in each of these vectors we can store many objects. Here, our list has only a single vector. To create a longer list, we have to add more vectors, and then join them together, again using the \texttt{c()} command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vector4 }\OtherTok{\textless{}{-}} \StringTok{"Here is another sentence {-} And one more"}
\NormalTok{parts2 }\OtherTok{\textless{}{-}} \FunctionTok{strsplit}\NormalTok{(vector4, }\StringTok{"{-}"}\NormalTok{)}
\NormalTok{parts3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(parts1, parts2)}
\end{Highlighting}
\end{Shaded}

We can now look at this new list in the Environment and check that it indeed has two elements. A further thing we can do is to join many vectors together. For this, we can use the \texttt{paste} function. Here, the \texttt{sep} argument defines how R will combine the elements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fruits }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"oranges"}\NormalTok{, }\StringTok{"lemons"}\NormalTok{, }\StringTok{"pears"}\NormalTok{, }\AttributeTok{sep =} \StringTok{"{-}"}\NormalTok{)}
\NormalTok{fruits}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "oranges-lemons-pears"
\end{verbatim}

Note that we can also use this command that paste objects that we made earlier together. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentences }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(vector3, vector4, }\AttributeTok{sep =} \StringTok{"."}\NormalTok{)}
\NormalTok{sentences}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Here is a sentence - And a second.Here is another sentence - And one more"
\end{verbatim}

Finally, we can change the case of the sentence. To do this, we can use \texttt{tolower} and \texttt{toupper}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tolower}\NormalTok{(sentences)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "here is a sentence - and a second.here is another sentence - and one more"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toupper}\NormalTok{(sentences)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "HERE IS A SENTENCE - AND A SECOND.HERE IS ANOTHER SENTENCE - AND ONE MORE"
\end{verbatim}

Again, we can also run the same command when we have more than a single element in our vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentences2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"This is a piece of example text"}\NormalTok{, }\StringTok{"This is another piece of example text"}\NormalTok{)}
\FunctionTok{toupper}\NormalTok{(sentences2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "THIS IS A PIECE OF EXAMPLE TEXT"      
## [2] "THIS IS ANOTHER PIECE OF EXAMPLE TEXT"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tolower}\NormalTok{(sentences2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "this is a piece of example text"      
## [2] "this is another piece of example text"
\end{verbatim}

And that is it. As you can see, the options for text analysis in basic R are rather limited. This is why packages such as \texttt{quanteda} exist in the first place. Note though, that even \texttt{quanteda} uses the same logic of character vectors and combinations that we saw here.

\hypertarget{import-.txt-files}{%
\section{Import .txt Files}\label{import-.txt-files}}

In case that we already have the .txt files somewhere, we can make the above process a bit easier, and begin at the last step:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readtext)}

\NormalTok{txt\_directory }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"/Texts"}\NormalTok{)}
\NormalTok{data\_texts }\OtherTok{\textless{}{-}} \FunctionTok{readtext}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(txt\_directory, }\StringTok{"*"}\NormalTok{), }\AttributeTok{encoding =} \StringTok{"UTF{-}8"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{import-.pdf-files}{%
\section{Import .pdf Files}\label{import-.pdf-files}}

One of the most popular formats for digital texts is the portable document format (.pdf). To read .pdf files into R, we need two packages. The \texttt{pdftools} package to convert the .pdf files into .txt files, and the \texttt{readtext} package to read the .txt files into R. Note that this only works if the .pdf files are \emph{readable}. This means that we can select (and copy-paste) the text in them. Thus, \texttt{readtext} does not work with .pdf files that the text in them cannot be selected (this is most likely because the pages of the document were scanned as images before turned into a .pdf file). If we have a .pdf file of this type, one solution is to use the \texttt{tesseract} package, which can use optical character recognition technology (OCR) to fix this issue.

To import the .pdf files, we start by loading the required libraries into R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pdftools)}
\FunctionTok{library}\NormalTok{(readtext)}
\end{Highlighting}
\end{Shaded}

Then, we go to our working directory (to see where this is, type \texttt{getwd()} into the Console). Here, we make two folders: one in which to store the .pdf files - called \emph{PDF} - and another new and empty folder in which to store the .txt files. We call this one \emph{Texts}. Ensure that all the .pdf files are in the \emph{PDF} folder. Then, we tell R about these folders:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setwd}\NormalTok{(}\StringTok{"Your Working Directory"}\NormalTok{)}
\NormalTok{pdf\_directory }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"/PDF"}\NormalTok{)}
\NormalTok{txt\_directory }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"/Texts"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, we ask R for a list of all the files in the .pdf directory. This is both to ensure that we are not overlooking anything and to tell R which files are in the folder. Here, setting \texttt{recurse=FALSE} means that we only list the files in the main folder and not any files that are in other folders in this main folder.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{files }\OtherTok{\textless{}{-}} \FunctionTok{list.files}\NormalTok{(pdf\_directory, }\AttributeTok{pattern =} \StringTok{".pdf"}\NormalTok{, }\AttributeTok{recursive =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{full.names =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{files}
\end{Highlighting}
\end{Shaded}

While we could convert a single document at a time, more often we have to deal with more than one document. To read all documents in at once, we have to write a little function. This function does the following. First, we tell R to make a new function that we label \texttt{extract}, and as input give it an element we call \texttt{filename}. This filename is at this point an empty element, but to which we will later refer the files we want to extract. Then, we tell it to print the file name to ensure that we are working with the right files while the function is running. In the next step, we tell it to try to read this filename using the \texttt{pdf\_text} function and save the result as a file called \texttt{text}. Afterwards, we tell it to do so for each of the files that end on .pdf that are in the element \texttt{files}. Then, we have it write this text file to a new file. This file is the extracted .pdf in .txt form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extract }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(filename) \{}
    \FunctionTok{print}\NormalTok{(filename)}
    \FunctionTok{try}\NormalTok{(\{}
\NormalTok{        text }\OtherTok{\textless{}{-}} \FunctionTok{pdf\_text}\NormalTok{(filename)}
\NormalTok{    \})}
\NormalTok{    title }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"(.*)/([\^{}/]*).pdf"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{2"}\NormalTok{, filename)}
    \FunctionTok{write}\NormalTok{(text, }\FunctionTok{file.path}\NormalTok{(txt\_directory, }\FunctionTok{paste0}\NormalTok{(title, }\StringTok{".txt"}\NormalTok{)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then use this function to extract all the pdf files in the \texttt{pdf\_directory} folder. To do so, we use a \texttt{for} loop. The logic of this loop is that for each individual \texttt{file} in the element \texttt{files}, we run the \texttt{extract} function we created. This will create an element called \texttt{file} for the file R is currently working on, and will create the .txt files in the \texttt{txt\_directory}:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (file }\ControlFlowTok{in}\NormalTok{ files) \{}
    \FunctionTok{extract}\NormalTok{(file)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can now read the .txt files into R. To do so, we use \texttt{paste0(txt\_directory,\ "*")} to tell \texttt{readtext} to look into our \texttt{txt\_directory}, and read any file in there. Besides this, we need to specify the encoding. Most often, this is \textbf{UTF-8}, though sometimes you might find \textbf{latin1} or \textbf{Windows-1252} encodings. While \texttt{readtext} will convert all these to \textbf{UTF-8}, you have to specify the original encoding. To find out which one you need, you have to look into the properties of the .txt file.

Assuming our texts are in UTF-8 encoding, we run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_texts }\OtherTok{\textless{}{-}} \FunctionTok{readtext}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(txt\_directory, }\StringTok{"*"}\NormalTok{), }\AttributeTok{encoding =} \StringTok{"UTF{-}8"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The result of this is a data frame of texts, which we can transform into a corpus for use in \texttt{quanteda} or keep as it is for other types of analyses.

\hypertarget{import-.csv-files}{%
\section{Import .csv Files}\label{import-.csv-files}}

We can also choose not to import the texts into R in a direct fashion, but import a .csv file with word counts instead. One way to generate these counts is by using JFreq (\protect\hyperlink{ref-Lowe2011b}{Lowe 2011}). This is a useful stand-alone programme written in Java that generates a .csv file where the rows represent the documents and the columns represent the individual words contained in the documents. The cells therefore, contain the wordcounts for each word within each document. JFreq also allows performing some basic pre-processing. JFreq is not actively maintained, but is available at \url{https://conjugateprior.org/software/jfreq/}.

To use JFreq, open the programme and drag and drop all the documents you want to process into the window of the programme. Once you do this, the document file names will appear in the document window. Then, you can choose from several pre-processing options. Amongst these are options to make all words lowercase or remove numbers, currency symbols, or stop words. The latter are words that often appear in texts which do not carry an important meaning. These are words such as \texttt{and\textquotesingle{}\textquotesingle{},}or'\,' and ``but'\,'. As stop words are language-specific and often context-specific as well, we need to tell JFreq what words are stop words. We can do so by putting all the stop words in a separate .txt file and load it in JFreq. You can also find many lists of stopwords for different languages online. For instance, many different lists of stopwords in English are available in this GitHub page: \url{https://github.com/igorbrigadir/stopwords} Finally, we can apply a stemmer which reduces words such as Europe and European to a single Europ* stem. JFreq allows us to use pre-defined stemmers by choosing the relevant language from a drop-down menu. In the following screenshot, you can see the JFreq at work importing the .txt files of a number of election manifestos.

\begin{center}\includegraphics[width=0.75\linewidth]{figures/jfreq} \end{center}

Note that here the encoding is UTF-8 while the locale is English (UK). Once we have specified all the options we want, we give a name for the output folder and press \emph{Process}. Now we go to that folder we named and copy-paste the ``data.csv'\,' file into your Working Directory. In R, we then run the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_manifestos }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data.csv"}\NormalTok{, }\AttributeTok{row.names =} \DecValTok{1}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By specifying \texttt{row.names=1}, we store the information of the first column in the data frame itself. This column, containing the names of the documents now belongs to the object of the data frame and does not appear as a separate column. The same is true for \texttt{header=TRUE} which ensures that the first row gives names to the columns (in this case containing the words).

\hypertarget{import-from-an-api}{%
\section{Import from an API}\label{import-from-an-api}}

Instead of importing the online data page-by-page, we can also use special programmes to download lots of data at once. We can do so with an Application Programming Interface (API). The main difference between using an API and regular webscraping is that APIs are specifically designed for this purpose. This means that it is easier for R to read the webpages, and that you can download a large amount of data at once. APIs are offered by many popular web sites like Wikipedia, social networking sites like Twitter and Facebook, newspapers such as \emph{The New York Times}, and so on.

While almost all websites can be read by the \texttt{rvest} package, for the APIs you often need a specific package. For example, for Twitter there is the \texttt{rtweet} package, for Facebook \texttt{rFacebook}, and \texttt{ggmap} for Google maps. Also, you often, if not always, need to register first before you can use an API. Note, however, that Facebook has recently taken steps in restricting access to their public APIs for research purposes, which means that research on Facebook users' posts is no longer an option (see \protect\hyperlink{ref-Freelon2018}{Freelon} (\protect\hyperlink{ref-Freelon2018}{2018}) and \protect\hyperlink{ref-Perriam2020}{Perriam, Birkbak, and Freeman} (\protect\hyperlink{ref-Perriam2020}{2020})).

Having said this, however, there are many APIs with associated R packages that are made by researchers and for researchers. Let's look at an example using an API for the New York Times. If you look at the website (\url{https://developer.nytimes.com/}), you find that we can get information ranging from articles to books and reviews.

Before we start here, we first have to maintain permission to use the API. For this, register on the site and log in. Then, make a new ``app'' under: \url{https://developer.nytimes.com/my-apps} and ensure you select the movie reviews. Then, you can click on the new app to see your key under ``API Keys.'' It is this string of codes and letters you will have to place at the {[}YOUR\_API\_KEY\_HERE{]} section.

Now, let us first load the packages:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(httr)}
\FunctionTok{library}\NormalTok{(jsonlite)}
\end{Highlighting}
\end{Shaded}

We can then build our request. As you can see on the site, the request requires us to give a search term (here we choose ``love''). Optionally, we can set a time frame from which we want to sample the reviews:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reviews }\OtherTok{\textless{}{-}}  \FunctionTok{fromJSON}\NormalTok{(}\StringTok{"https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=love\&opening{-}date=2000{-}01{-}01:2020{-}01{-}01\&api{-}key=[YOUR\_API\_KEY\_HERE]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The result is a JSON object that you can see in the environment. While JSON (JavaScript Object Notation) is a generic way in which information is easy to share - and is thus often used - it is not in an ideal form. So, we change the JSON information to a data frame using the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reviews\_df }\OtherTok{\textless{}{-}}  \FunctionTok{fromJSON}\NormalTok{(}\StringTok{"https://api.nytimes.com/svc/movies/v2/reviews/search.json?query=crisis\&opening{-}date=2000{-}01{-}01:2020{-}01{-}01\&api{-}key=xPs6hyXwH5UT3G4WT3jAIHSsbAkr5HlM"}\NormalTok{, }\AttributeTok{flatten =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{data.frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You can now find all the information in the new \texttt{reviews\_df} object, which contains information about the type of crime, location, month etc. This is one example of an API, but there are many others available, such as those of the EU, OpenStreetMaps, Weather Underground, etc. As we can see though, having a package makes things easier, though more limited.

\hypertarget{import-using-web-scraping}{%
\section{Import using Web Scraping}\label{import-using-web-scraping}}

If our text is online (e.g.~as part of a website) we can also choose to get it from there without copying it into a .txt file first. To do so, we have to employ web scraping. The logic of web scraping is that we use the structure of the underlying HTML document to find and download the text we want. Note though that not all websites encourage (or even allow) scraping. So, do have a look at their disclaimer before we do so. You can do this by either checking the website's \emph{terms and condition} page, or the robots.txt file that you can usually find appended at the home page (e.g.~\url{https://www.facebook.com/robots.txt} ).

In the following example we will see how one can download information from the Internet Movie Database (IMDb): \url{https://www.imdb.com} Note that the IMDb does not allow you to do any web scraping, so the following example is given for illustration purposes only! If you are interested in analyzing data from IMDB you can download the official datasets that are released by IMDB here: \url{https://datasets.imdbws.com/} The documentation for these datasets is available here: \url{https://www.imdb.com/interfaces/} If you would like to learn more about web scraping in the context of quantitative text analysis we suggest the textbook by \protect\hyperlink{ref-Munzert2014}{Munzert et al.} (\protect\hyperlink{ref-Munzert2014}{2014}).

In the following example we show how to download the user reviews that appear on the IMDB website. The first command, \texttt{read\_html} downloads this whole page. If you look at this page in your browser, you see that there are many other things on there besides the user review. To tell R which part is the text to download, we use the \texttt{html\_nodes} command. This command looks for a certain header on the HTML page and starts downloading from there. The \texttt{html\_text} command then reads that bit of text and puts it into the object. Note that the \texttt{\%\textgreater{}\%} command we use here is what we call a \emph{pipe}. What it does is that it transports the output of one command into another, without saving it to an intermediate object. So here, we first download the HTML, find the right header, and only then save it into an object. Having done this for three reviews, we then bind them together:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}

\NormalTok{review1 }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"http://www.imdb.com/title/tt1979376/"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"\#titleUserReviewsTeaser p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{review2 }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"http://www.imdb.com/title/tt6806448/"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"\#titleUserReviewsTeaser p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{review3 }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"http://www.imdb.com/title/tt7131622/"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"\#titleUserReviewsTeaser p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{reviews\_scraping }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(review1, review2, review3)}
\end{Highlighting}
\end{Shaded}

\hypertarget{reliability-and-validity}{%
\chapter{Reliability and validity}\label{reliability-and-validity}}

We could say that the central tenet of quantitative text analysis, which sets it apart from other approaches to analyzing text, is that it strives to be objective and replicable. In measurement theory, we use the terms \textbf{reliability} and \textbf{validity} to convey this message.

Reliability refers to consistency, that is, the degree to which we get similar results whenever we apply a measuring instrument to measure a given concept. This is similar to the concept of \emph{replicability}. Validity, on the other hand, refers to unbiasedness, that is, the degree to which our measure really measures the concept which intends to measure. In other words, validity looks whether the measuring instrument that we are using is objective.

\protect\hyperlink{ref-Carmines1979a}{Carmines and Zeller} (\protect\hyperlink{ref-Carmines1979a}{1979}) distinguish among three types of validity. \emph{Content Validity}, which refers to whether our measure represents all facets of the construct of interest; \emph{criterion Validity}, which looks at whether our measure correlates with other measures of the same concept, and \emph{construct Validity}, which looks at whether our measure behaves as expected within a given theoretical context.
I should also say here, that the three types of validity are not interchangeable. Ideally, one has to prove that their results pass all three validity tests. In the words of \protect\hyperlink{ref-Grimmer2013a}{Grimmer and Stewart} (\protect\hyperlink{ref-Grimmer2013a}{2013}): ``Validate, validate, validate!''

\protect\hyperlink{ref-Krippendorff2004a}{Krippendorff} (\protect\hyperlink{ref-Krippendorff2004a}{2004}) distinguishes among three types of reliability. \emph{Stability}, which he considers as the weakest form of coding reliability, and which can be measured when the same text is coded by the same coder more than once, \emph{reproducibility}, which is measured by the degree of agreement among independent coders, and \emph{accuracy}, which he considers as the strongest form of coding reliability, and which is measured by the agreement between coders and a given standard. However, in the absence of a benchmark, we are usually interested in measuring reliability as reproducibility, in other words as inter-coder agreement.

\hypertarget{inter-coder-agreement}{%
\section{Inter-Coder Agreement}\label{inter-coder-agreement}}

\protect\hyperlink{ref-Hayes2007a}{Hayes and Krippendorff} (\protect\hyperlink{ref-Hayes2007a}{2007, 79}) argue that a good measure of the agreement should at least address five criteria. The first is that it should apply to many coders, and not only two. Also, when we use the method for more coders, there should be no difference in how many coders we include. The second is that the method should only take into account the actual number of categories the coders used and not all that were available. This as while the designers designed the coding scheme on what they thought the data would look like, the coders use the scheme based on what the data is. Third, it should be numerical, meaning that we can use it to make a scale between 0 (absence of agreement) and 1 (perfect agreement). Fourth, it should be appropriate for the level of measurement. So, if our data is ordinal or nominal, we should not use a measure that assumes metric data. This ensures that the metric uses all the data and that it does not add or not use other information. Fifth, we should be able to compute (or know), the sampling behaviour of the measure.

With these criteria in mind, we see that popular methods, such as \% agreement or Pearson's \emph{r}, can be misleading. Especially for the latter - as it is a quite popular method - this often leads to problems, as this figure by \protect\hyperlink{ref-Krippendorff2004a}{Krippendorff} (\protect\hyperlink{ref-Krippendorff2004a}{2004}) shows:

\begin{center}\includegraphics[width=0.75\linewidth]{figures/observers} \end{center}

Here, the figure on the left shows two coders: A and B. The dots in the figure show the choices both coders made, while the dotted line shows the line of perfect agreement. If a dot is on this line, it means that both Coder A and Coder B made the same choice. In this case, they disagreed in all cases. When Coder A chose \emph{a}, Coder B chose \emph{e}, when Coder A chose \emph{b}, Coder B chose \emph{a}, and so on. Yet, when we would calculate Pearson's \emph{r} for this, we would find a result as shown in the right-hand side of the figure. Seen this way, the agreement between both coders does not seem a problem at all. The reason for this is that Pearson's \emph{r} works with the distances between the categories \emph{without} taking into account their location. So, for a positive relationship, the only thing Pearson's \emph{r} requires is that for every increase or decrease for one coder, there is a similar increase or decrease for the other. This happens here with four of the five categories. The result is thus a high Pearson's \emph{r}, though the actual agreement should be 0.

Pearson's \emph{r} thus cannot fulfil all our criteria. A measure that can is Krippendorff's \(\alpha\) (\protect\hyperlink{ref-Krippendorff2004a}{Krippendorff 2004}). This measure can not only give us the agreement we need, but can also do so for nominal, ordinal, interval, and ratio level data, as well as data with many coders and missing values. Besides, we can compute 95\% confidence intervals around \(\alpha\) using bootstrapping, which we can use to show the degree of uncertainty around our reliability estimates.

Despite this, Krippendorff's \(\alpha\) is not free of problems. One main problem occurs when coders agree on only a few categories and use these categories a considerable number of times. This leads to an inflation of \(\alpha\), making it is higher than it should be (\protect\hyperlink{ref-Krippendorff2004a}{Krippendorff 2004}), as in the following example:

\begin{center}\includegraphics[width=0.75\linewidth]{figures/kripp} \end{center}

Here, in the left-most figure, we see coders A and B who have to code into three categories: 0, 1, or 2. In this example, the categories 1 and 2 carry a certain meaning, while category 0 means that the coders did not know what to assign the case to. Of the 86 cases, both coders code 80 cases in the 0 category. This means that there are only 6 cases on which they can agree or disagree about a code that carries some meaning. Yet, if we calculate \(\alpha\), the result - 0.686 - takes into account all the categories. One solution for this is to add up the categories 1 and 2, as the figure in the middle shows. Here, the coders agree in 84 of the 86 cases (on the diagonal line) and disagree in only 2 of them. Calculating \(\alpha\) now shows that it would increase to 0.789. Finally, we can remove the 0 category and again view 1 and 2 as separate categories (as the most right-hand figure shows). Yet, the result of this is quite disastrous. While the coders agree in 3 of the 4 cases, the resulting \(\alpha\) equals 0.000, as coder B did not use category 1 at all.

Apart from these issues, Krippendorff's \(\alpha\) is a stable and useful measure. A value of \(\alpha\) = 1 indicates perfect reliability, while a value of \(\alpha\) = 0 indicates the absence of reliability. This means that if \(\alpha\) = 0, there is no relationship between the values. It is possible for \(\alpha\) \textless{} 0, which means that the disagreements between the values are larger than they would be by chance and are systematic. As for thresholds, \protect\hyperlink{ref-Krippendorff2004a}{Krippendorff} (\protect\hyperlink{ref-Krippendorff2004a}{2004}) proposes to use either 0.80 or 0.67 for results to be reliable. Such low reliability often has many causes. One thing might be that the coding scheme is not appropriate for the documents. This means that coders had categories that they had no use for, and lacked categories they needed. Another reason might be that the coders lacked training. Thus, they did not understand how to use the coding scheme or how the coding process works. This often leads to frustration on part of the coders, as in these cases the process often becomes time-consuming and too demanding to carry out.

To calculate Krippendorff's \(\alpha\), we can use the following software:

\begin{itemize}
\tightlist
\item
  KALPHA custom dialogue (SPSS)
\item
  \textbf{kalpha} user-written package (Stata)
\item
  KALPHA macro (SAS)
\item
  \textbf{kripp.alpha} command in \textbf{kripp.boot} package (R) - amongst others
\end{itemize}

Let us try this in R using an example. Here, we will look at the results of a coding reliability test where 12 coders assigned the sentences of the 1997 European Commission work programme in the 20 categories of a policy areas coding scheme. We can find the results for this on GitHub. To get the data, we tell R where to find it, then to read that file as a .csv file and write is to a new object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}

\NormalTok{urlfile }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/SCJBruinsma/qta{-}files/master/reliability\_results.csv"}
\NormalTok{reliability\_results }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\FunctionTok{url}\NormalTok{(urlfile))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   coder1 = col_double(),
##   coder2 = col_double(),
##   coder3 = col_double(),
##   coder4 = col_double(),
##   coder5 = col_double(),
##   coder6 = col_double(),
##   coder7 = col_double(),
##   coder8 = col_double(),
##   coder9 = col_double(),
##   coder10 = col_double(),
##   coder11 = col_double(),
##   coder12 = col_double()
## )
\end{verbatim}

Notice that in the data frame we just created, the coders are in the columns and the sentences in the rows. As the \texttt{kripp.boot} package requires it to be the other way around and in matrix form, we first transpose the data, and then place it in a matrix. Finally, we run the command and specify we want the nominal version:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"kripp.boot"}\NormalTok{)}

\NormalTok{reliability\_results\_t }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(reliability\_results)}
\NormalTok{reliability }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(reliability\_results\_t)}
\NormalTok{kalpha }\OtherTok{\textless{}{-}} \FunctionTok{kripp.boot}\NormalTok{(reliability, }\AttributeTok{iter=}\DecValTok{1000}\NormalTok{, }\AttributeTok{method =} \StringTok{"nominal"}\NormalTok{)}
\NormalTok{kalpha}\SpecialCharTok{$}\NormalTok{value}
\end{Highlighting}
\end{Shaded}

Note also that \texttt{kripp.boot} is a GitHub package. You can still calculate the value (but without the confidence interval) with another package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"DescTools"}\NormalTok{)}

\NormalTok{reliability\_results\_t }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(reliability\_results)}
\NormalTok{reliability }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(reliability\_results\_t)}
\NormalTok{kalpha }\OtherTok{\textless{}{-}} \FunctionTok{KrippAlpha}\NormalTok{(reliability, }\AttributeTok{method =} \StringTok{"nominal"}\NormalTok{)}
\NormalTok{kalpha}\SpecialCharTok{$}\NormalTok{value}
\end{Highlighting}
\end{Shaded}

As we can see, the results point out that the agreement among the coders is 0.634 with an upper limit of 0.650 and a lower limit of 0.618 which is short of Krippendorff's cut-off point of 0.667.

\hypertarget{visualizing-quality}{%
\section{Visualizing Quality}\label{visualizing-quality}}

\protect\hyperlink{ref-Lamprianou2020}{Lamprianou} (\protect\hyperlink{ref-Lamprianou2020}{2020}) notes that existing reliability indices may mask coding problems and that the reliability of coding is not stable across coding units (as illustrated in the example given for Krippendorff's alpha in Section 3.2 above). To investigate the quality of coding he proposes using social network analysis (SNA) and exponential random graph models (ERGM). Here, we illustrate a different approach, based on the idea of sensitivity analysis.

We therefore compare the codings of each coder against all others (and also against a benchmark or a gold standard). For this, we need to bootstrap the coding reliability results to create an uncertainty measure around each coder's results, following the approach proposed by \protect\hyperlink{ref-Benoit2009a}{Benoit, Laver, and Mikhaylov} (\protect\hyperlink{ref-Benoit2009a}{2009}). The idea is to use a non-parametric bootstrap for the codings of each coder (using 1000 draws with replacement) at the category level and then calculate the confidence intervals. Their width then depends on both the number of sentences coded by each coder (n) in each category and the number of coding categories that are not empty. Thus, larger documents and fewer empty categories result in narrower confidence intervals, while a small number of categories leads to wider intervals (\protect\hyperlink{ref-Lowe2011a}{Lowe and Benoit 2011}).

To start, the first thing we do is load two packages we need into R using the \texttt{library} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}
\FunctionTok{library}\NormalTok{(combinat)}
\end{Highlighting}
\end{Shaded}

In the following example we perform the sensitivity analysis on the coded sentences of the 1997 European Commission work programme, as given in Section 3.2. Here, however, the same data is arranged differently. Each row represents a coder, and each column represents a coding category (\emph{c0} to \emph{c19}). In each cell, we see the number of sentences that each coder coded in each category, with the column \emph{n} giving the sum of each row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coderid }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"coder1"}\NormalTok{, }\StringTok{"coder2"}\NormalTok{, }\StringTok{"coder3"}\NormalTok{, }\StringTok{"coder4"}\NormalTok{, }\StringTok{"coder5"}\NormalTok{,}
    \StringTok{"coder6"}\NormalTok{, }\StringTok{"coder7"}\NormalTok{, }\StringTok{"coder8"}\NormalTok{, }\StringTok{"coder9"}\NormalTok{, }\StringTok{"coder10"}\NormalTok{, }\StringTok{"coder11"}\NormalTok{,}
    \StringTok{"coder12"}\NormalTok{)}
\NormalTok{c0 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{c01 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{c02 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{c03 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{21}\NormalTok{)}
\NormalTok{c04 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{c05 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{c06 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{c07 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{c08 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{c09 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{c10 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{23}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{22}\NormalTok{)}
\NormalTok{c11 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{31}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{31}\NormalTok{)}
\NormalTok{c12 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{c13 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{c14 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{13}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{14}\NormalTok{)}
\NormalTok{c15 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{c16 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{c17 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{c18 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{c19 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{c20 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{164}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{163}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{155}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{164}\NormalTok{, }\DecValTok{164}\NormalTok{,}
    \DecValTok{164}\NormalTok{)}

\NormalTok{data\_uncertainty }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(coderid, c0, c01, c02, c03, c04,}
\NormalTok{    c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, c16,}
\NormalTok{    c17, c18, c19, c20, n, }\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then tell R how many coders we have. As this number is equal to the number of rows we have, we can get this number using the \texttt{nrow} command. We also specify the number of bootstraps we want to carry out (1000) and transform our data frame into an array. We do the latter as R needs the data in this format later on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nman }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(data\_uncertainty)}
\NormalTok{nrepl }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{manifBSn }\OtherTok{\textless{}{-}}\NormalTok{ manifBSnRand }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(data\_uncertainty[,}
    \DecValTok{2}\SpecialCharTok{:}\DecValTok{21}\NormalTok{]), }\FunctionTok{c}\NormalTok{(nman, }\DecValTok{20}\NormalTok{, nrepl }\SpecialCharTok{+} \DecValTok{1}\NormalTok{), }\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nman, }\FunctionTok{names}\NormalTok{(data\_uncertainty[,}
    \DecValTok{2}\SpecialCharTok{:}\DecValTok{21}\NormalTok{]), }\DecValTok{0}\SpecialCharTok{:}\NormalTok{nrepl))}
\end{Highlighting}
\end{Shaded}

We then bootstrap the sentence counts for each coder and compute percentages for each category using a multinomial draw. First, we define \texttt{p}, which is the proportion of each category over all the coders. Then, we input this value together with the total number of codes \texttt{n} into the \texttt{rmultinomial} command, which gives the random draws. As we want to do this a 1000 times, we place this command into a \texttt{for} loop:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ manifBSn[, , }\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{n}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nrepl) \{}
\NormalTok{    manifBSn[, , i] }\OtherTok{\textless{}{-}} \FunctionTok{rmultinomial}\NormalTok{(n, p)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With this data, we can then ask R to compute the quantities of interest. These are standard errors for each category, as well as the percentage coded for each category:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c0SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c0"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c01SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c01"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c02SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c02"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c03SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c03"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c04SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c04"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c05SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c05"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c06SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c06"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c07SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c07"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c08SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c08"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c09SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c09"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c10SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c10"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c11SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c11"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c12SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c12"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c13SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c13"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c14SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c14"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c15SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c15"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c16SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c16"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c17SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c17"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c18SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c18"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{c19SE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c19"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}

\NormalTok{per0 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c0"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per01 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c01"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per02 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c02"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per03 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c03"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per04 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c04"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per05 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c05"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per06 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c06"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per07 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c07"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per08 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c08"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per09 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c09"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per10 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c10"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per11 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c11"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per12 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c12"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per13 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c13"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per14 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c14"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per15 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c15"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per16 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c16"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per17 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c17"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per18 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c18"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{per19 }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"c19"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\end{Highlighting}
\end{Shaded}

We then bind all these quantities together in a single data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataBS }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(data\_uncertainty[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{22}\NormalTok{], c0SE, c01SE,}
\NormalTok{    c02SE, c03SE, c04SE, c05SE, c06SE, c07SE, c08SE, c09SE, c10SE,}
\NormalTok{    c11SE, c12SE, c13SE, c14SE, c15SE, c16SE, c17SE, c18SE, c19SE,}
\NormalTok{    per0, per01, per02, per03, per04, per05, per06, per07, per08,}
\NormalTok{    per09, per10, per11, per12, per13, per14, per15, per16, per17,}
\NormalTok{    per18, per19))}
\end{Highlighting}
\end{Shaded}

While we can now inspect the results by looking at the data, it becomes more clear when we visualise this. While R has some inbuilt tools for visualisation (in the \texttt{graphics} package), these tools are rather crude. Thus, here we will use the \texttt{ggplot2} package, which extends our options, and which has an intuitive structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

First, we make sure that the variable \texttt{coderid} is a factor and make sure that it is in the right order:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataBS}\SpecialCharTok{$}\NormalTok{coderid }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(dataBS}\SpecialCharTok{$}\NormalTok{coderid)}
\NormalTok{dataBS}\SpecialCharTok{$}\NormalTok{coderid }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(dataBS}\SpecialCharTok{$}\NormalTok{coderid, }\FunctionTok{levels}\NormalTok{(dataBS}\SpecialCharTok{$}\NormalTok{coderid)[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}
    \DecValTok{5}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

Then, we calculate the 95\% confidence intervals for each category. We do so using the percent of each category and the respective standard error, and add these values to our data-set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c0\_lo }\OtherTok{\textless{}{-}}\NormalTok{ per0 }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ c0SE)}
\NormalTok{c0\_hi }\OtherTok{\textless{}{-}}\NormalTok{ per0 }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ c0SE)}
\NormalTok{c01\_lo }\OtherTok{\textless{}{-}}\NormalTok{ per01 }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ c01SE)}
\NormalTok{c01\_hi }\OtherTok{\textless{}{-}}\NormalTok{ per01 }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ c01SE)}
\NormalTok{c02\_lo }\OtherTok{\textless{}{-}}\NormalTok{ per02 }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ c02SE)}
\NormalTok{c02\_hi }\OtherTok{\textless{}{-}}\NormalTok{ per02 }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ c02SE)}

\NormalTok{dataBS }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dataBS, c0\_lo, c0\_hi, c01\_lo, c01\_hi, c02\_lo,}
\NormalTok{    c02\_hi)}
\end{Highlighting}
\end{Shaded}

Finally, we generate the graphs for each individual category:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(dataBS, }\FunctionTok{aes}\NormalTok{(per0, coderid)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_errorbarh}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmax =}\NormalTok{ c0\_hi,}
    \AttributeTok{xmin =}\NormalTok{ c0\_lo), }\AttributeTok{height =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Percentage coded to category 0"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Coder ID"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-42-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(dataBS, }\FunctionTok{aes}\NormalTok{(per01, coderid)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_errorbarh}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmax =}\NormalTok{ c01\_hi,}
    \AttributeTok{xmin =}\NormalTok{ c01\_lo), }\AttributeTok{height =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Percentage coded to category 01"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Coder ID"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(dataBS, }\FunctionTok{aes}\NormalTok{(per02, coderid)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_errorbarh}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmax =}\NormalTok{ c02\_hi,}
    \AttributeTok{xmin =}\NormalTok{ c02\_lo), }\AttributeTok{height =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Percentage coded to category 02"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Coder ID"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-44-1.pdf}

Each figure shows the percentage that each of the coders coded in the respective category of the coding scheme. We thus use the confidence intervals around the estimates to look at the degree of uncertainty around each estimate. We can read the plots by looking if the dashed line is within the confidence intervals for each coder. The larger the coders deviate from the benchmark or standard, the less likely that they understood the coding scheme in the same way. It also means that it is more likely that a coder would have coded the work programme much different from the benchmark coder. Thus, such a sensitivity analysis is like having a single reliability coefficient for each coding category.

\hypertarget{preliminaries}{%
\chapter{Preliminaries}\label{preliminaries}}

Before we start with any kind of analyses, it pays to have a brief look at the preliminaries first. The goal of these preliminaries is to give us a better understanding of ``what'' are texts are about, who there authors are, and what kind of words and information we can expect to find in them. This is necessary for us to know our data. Not only is it standard academic practise to know your data well, it also helps us to decide whether results we will encounter later on really do make sense. Here, we look at three different preliminaries: the idea of keywords-in-context, several visualisations, and a range of textual statistics. Before that though, we will have a brief look at the idea of the ``corpus,'' as it is central to the idea of how \texttt{quanteda} works.

\hypertarget{the-corpus}{%
\section{The Corpus}\label{the-corpus}}

Within \texttt{quanteda}, the main way to store documents is in the form of a \texttt{corpus} object. This object contains all the information that comes with the texts and does not change during our analysis. Instead, we make copies of the main corpus, change them into the type we need, and run our analyses on them. The advantage of this is that we always can go back to our original data.

Apart from importing texts ourselves, \texttt{quanteda} contains several corpora as well. Here, we use one of these, which contains the inaugural speeches of all the US Presidents. For this, we first have to load the main package and then load the data into R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda)}

\FunctionTok{data}\NormalTok{(data\_corpus\_inaugural)}
\FunctionTok{head}\NormalTok{(data\_corpus\_inaugural)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Corpus consisting of 6 documents and 4 docvars.
## 1789-Washington :
## "Fellow-Citizens of the Senate and of the House of Representa..."
## 
## 1793-Washington :
## "Fellow citizens, I am again called upon by the voice of my c..."
## 
## 1797-Adams :
## "When it was first perceived, in early times, that no middle ..."
## 
## 1801-Jefferson :
## "Friends and Fellow Citizens: Called upon to undertake the du..."
## 
## 1805-Jefferson :
## "Proceeding, fellow citizens, to that qualification which the..."
## 
## 1809-Madison :
## "Unwilling to depart from examples of the most revered author..."
\end{verbatim}

You should now see the corpus appear in the Environment tab. If you click on it, you can see, amongst others, that the corpus comes with information on the Year of the release of the speech and the president it belongs to. As the corpus is quite large, we make it a bit more manageable by only selecting the speeches from 1900 onwards. We can do this by using the \texttt{corpus\_subset} command for both:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_inaugural }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_subset}\NormalTok{(data\_corpus\_inaugural, Year }\SpecialCharTok{\textgreater{}} \DecValTok{1900}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we have our corpus, we can start with the analysis. As noted, we try not to carry out any analysis on the corpus itself. Instead, we keep it as it is and work on its copies. Often, this means transforming the data into another shape. One of the more popular shapes is the data frequency matrix (dfm). This is a matrix which contains the documents in the rows and the word counts for each word in the columns.

Before we can do so however, we have to split up our texts into unique words. To do this, we first have to construct a \texttt{tokens} object. In the command that we use to do this, we can specify how we want our texts to be split (here we use the standard option), and in addition clean our data a bit. For example, we can specify that we want to convert all the texts into lowercase and remove any numbers and special characters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_inaugural\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens}\NormalTok{(corpus\_inaugural, }\AttributeTok{what =} \StringTok{"word"}\NormalTok{,}
    \AttributeTok{remove\_punct =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_symbols =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_numbers =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{remove\_url =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_separators =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{split\_hyphens =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{include\_docvars =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{padding =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Creating a tokens object from a corpus input...
\end{verbatim}

\begin{verbatim}
##  ...starting tokenization
\end{verbatim}

\begin{verbatim}
##  ...1901-McKinley to 2021-Biden.txt
\end{verbatim}

\begin{verbatim}
##  ...preserving hyphens
\end{verbatim}

\begin{verbatim}
##  ...preserving social media tags (#, @)
\end{verbatim}

\begin{verbatim}
##  ...segmenting into words
\end{verbatim}

\begin{verbatim}
##  ...7,013 unique types
\end{verbatim}

\begin{verbatim}
##  ...removing separators, punctuation, symbols, numbers, URLs
\end{verbatim}

\begin{verbatim}
##  ...complete, elapsed time: 0.15 seconds.
\end{verbatim}

\begin{verbatim}
## Finished constructing tokens from 31 documents.
\end{verbatim}

We can also remove certain stopwords so that words like ``and'' or ``the'' do not influence our analysis too much. We can either specify these words ourselves or we can use a list that is already present in R. To see this list, type \texttt{stopwords("english")} in the console. Stopwords for other languages are also available (such as German, French and Spanish). Even more stopwords can be found in the \texttt{stopword} package, that can easily be integrated with \texttt{quanteda}. For now, we will use the English ones. First, however, as all the stopwords are lower-case, we will have to lower case our words as well:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_inaugural\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_tolower}\NormalTok{(data\_inaugural\_tokens, }\AttributeTok{keep\_acronyms =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{data\_inaugural\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_select}\NormalTok{(data\_inaugural\_tokens, }\FunctionTok{stopwords}\NormalTok{(}\StringTok{"english"}\NormalTok{), }\AttributeTok{selection =} \StringTok{"remove"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, we can construct our dfm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_inaugural\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(data\_inaugural\_tokens)}
\end{Highlighting}
\end{Shaded}

\hypertarget{keywords-in-context}{%
\section{Keywords in Context}\label{keywords-in-context}}

Besides all the analytical techniques we have available, we can also use \texttt{quanteda} to look at various rather simpler ones. One popular option is known as keywords in context (kwic), in which we are interested with which other words a certain word appears in our texts. This is also known as looking at the ``concordance'' of our text. Here, we can easily find this with our tokens dataframe. Let's say we are interested in all those words that start with ``secure'' and we want to know which three words occur before and after this word. We can then run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens}\NormalTok{(corpus\_inaugural)}
\NormalTok{kwic\_output }\OtherTok{\textless{}{-}} \FunctionTok{kwic}\NormalTok{(tokens, }\AttributeTok{pattern =} \StringTok{"secure*"}\NormalTok{, }\AttributeTok{valuetype =} \StringTok{"glob"}\NormalTok{, }\AttributeTok{window =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the outputted object, we find a column labelled ``pre'' and another labelled ``post.'' These refer to the words that came either before or after the word "secure*". We can easily take these out and combine them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_pre }\OtherTok{\textless{}{-}}\NormalTok{ kwic\_output}\SpecialCharTok{$}\NormalTok{pre}
\NormalTok{text\_post }\OtherTok{\textless{}{-}}\NormalTok{ kwic\_output}\SpecialCharTok{$}\NormalTok{post}
\NormalTok{text\_word }\OtherTok{\textless{}{-}}\NormalTok{ kwic\_output}\SpecialCharTok{$}\NormalTok{keyword}
\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{paste}\NormalTok{(text\_pre, text\_word, text\_post))}
\end{Highlighting}
\end{Shaded}

We then combine this information with the name of the document it came from so that we know which text the word is from:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extracted }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(kwic\_output}\SpecialCharTok{$}\NormalTok{docname, text)}
\FunctionTok{names}\NormalTok{(extracted) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"docname"}\NormalTok{, }\StringTok{"text"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(extracted)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         docname                                      text
## 1 1901-McKinley be adapted to secure a government capable
## 2     1909-Taft               , and to secure at the same
## 3     1909-Taft          is needed to secure a more rapid
## 4     1909-Taft  . This should secure an adequate revenue
## 5     1909-Taft     business . To secure the needed speed
## 6     1909-Taft    duties as to secure an adequate income
\end{verbatim}

\hypertarget{visualisations-and-descriptives}{%
\section{Visualisations and Descriptives}\label{visualisations-and-descriptives}}

Another thing we can do with this dfm is to generate a frequency graph using the \texttt{topfeatures} function. For this, we first have to save the 50 most frequently occurring words in our texts (note that there is also the \texttt{textstat\_frequency} function in the \texttt{quanteda.textstats} helper package that can do this):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{features }\OtherTok{\textless{}{-}} \FunctionTok{topfeatures}\NormalTok{(data\_inaugural\_dfm, }\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then have to transform this object into a data frame, and sort it by decreasing frequency:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{features\_plot }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{term =} \FunctionTok{names}\NormalTok{(features),}\AttributeTok{frequency =} \FunctionTok{unname}\NormalTok{(features)))}
\NormalTok{features\_plot}\SpecialCharTok{$}\NormalTok{term }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(features\_plot, }\FunctionTok{reorder}\NormalTok{(term, }\SpecialCharTok{{-}}\NormalTok{frequency))}
\end{Highlighting}
\end{Shaded}

Then we can plot the results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(features\_plot) }\SpecialCharTok{+} 
 \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{term, }\AttributeTok{y=}\NormalTok{frequency)) }\SpecialCharTok{+}
 \FunctionTok{theme\_classic}\NormalTok{()}\SpecialCharTok{+}
 \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x=}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle=}\DecValTok{90}\NormalTok{, }\AttributeTok{hjust=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-55-1.pdf}

We can also generate word clouds. As these show all the words we have, we will trim our dfm first to remove all those words that occurred less than 40 times. We can do this with the \texttt{dfm\_trim} function. Then, we can use this newly trimmed dfm to generate the word cloud:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textplots)}

\NormalTok{wordcloud\_dfm\_trim }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_trim}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{min\_termfreq =} \DecValTok{40}\NormalTok{)}
\FunctionTok{textplot\_wordcloud}\NormalTok{(wordcloud\_dfm\_trim)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-56-1.pdf}

If we would want to, we can also split up this word cloud based on which words belong to which party. For this, we have to generate a new dfm and within it, specify the groups that well which words belong to which party. Given that we have only Democratic and Republican presidents, we end up with two groups:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textplots)}

\NormalTok{wordcloud\_dfm\_comp }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_group}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{groups =}\NormalTok{ Party)}
\NormalTok{wordcloud\_dfm\_comp }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_trim}\NormalTok{(wordcloud\_dfm\_comp, }\AttributeTok{min\_termfreq =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{max\_words =} \DecValTok{40}\NormalTok{)}
\FunctionTok{textplot\_wordcloud}\NormalTok{(wordcloud\_dfm\_comp, }\AttributeTok{comparison =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-57-1.pdf}

\hypertarget{text-statistics}{%
\section{Text Statistics}\label{text-statistics}}

Finally, \emph{quanteda} also allows us to calculate quite a number of textual statistics. These are all collected in the \emph{quanteda.textstats} helper package. Here, we will look at several of them, starting with a simple overview of our corpus in the terms of a summary that tells us the number of characters, sentences, tokens, etc. for each of the texts:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textstats)}
\NormalTok{corpus\_summary }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_summary}\NormalTok{(corpus\_inaugural)}
\end{Highlighting}
\end{Shaded}

If we want, we can then use this data to make some simple graphs telling us various things about the texts in our corpus. As an example, let's look at the number of sentences the various presidents put in their speeches:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{corpus\_summary, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{document, }\AttributeTok{y=}\NormalTok{sents, }\AttributeTok{group=}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Number of Characters"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"President/Year"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-59-1.pdf}

Another thing we can look at are the readability and lexical diversity of the texts. The former one of these refers to how readable a text is (i.e.~how easy or difficult it is to read), while the latter tells us how many different types of words are used in the texts and thus how ``diverse'' the text is in word choice and use. Given that there are many ways to calculate both metrics, please have a look at the help file to see which one works best for you. Here, we will use the most popular:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_readability }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_readability}\NormalTok{(corpus\_inaugural, }\AttributeTok{measure =} \FunctionTok{c}\NormalTok{(}\StringTok{"Flesch.Kincaid"}\NormalTok{, }\StringTok{"Dale.Chall.old"}\NormalTok{))}
\NormalTok{corpus\_lexdiv }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_lexdiv}\NormalTok{(data\_inaugural\_tokens, }\FunctionTok{c}\NormalTok{(}\StringTok{"CTTR"}\NormalTok{, }\StringTok{"TTR"}\NormalTok{, }\StringTok{"MATTR"}\NormalTok{), }\AttributeTok{MATTR\_window =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As before, we can easily plot this data in a graph to see how lexical diversity developed over time:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{corpus\_lexdiv, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{document, }\AttributeTok{y=}\NormalTok{CTTR, }\AttributeTok{group=}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Lexical Diversity (CTTR)"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"President/Year"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-61-1.pdf}

Another thing we can do is look at the similarities and distances between documents. With this, we can answer questions such as: how ``different'' are these documents from each other? And if different (or similar), how different (or similar)? The idea is that the larger the similarity is, the smaller the distance is as well. A good way to understand the idea of similarity is to consider how many operations you need to perform to change one text into the other. The more ``replace'' options you have to carry out, the more different the text. As for the distances, it is best to consider the texts as having positions on a Cartesian plane (with positions based on their word counts). The distance between these two points (either Euclidean, Manhattan or other) is then the distance between the texts.

Let's start with a look at these similarities (note again that there are many different methods to calculate this):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_similarties }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_simil}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{method =} \StringTok{"correlation"}\NormalTok{, }\AttributeTok{margin =} \StringTok{"documents"}\NormalTok{)}
\NormalTok{corpus\_similarties }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(corpus\_similarties)}
\end{Highlighting}
\end{Shaded}

As brief look at these results tells us that the 1981 and 1985 Reagan speeches show the highest degree of similarity, while the\\
1945 Roosevelt and 2017 Trump speeches are the most different. Note that while we look here at the documents, we could also look at individual words (set \texttt{margin="features}). For now, let us look at the distances between the documents, choosing the Euclidean distance between the documents as our metric:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_distances }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_dist}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{margin =} \StringTok{"documents"}\NormalTok{, }\AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\NormalTok{corpus\_distances\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(corpus\_distances)}
\end{Highlighting}
\end{Shaded}

Here, we find the 1905 and 1945 Roosevelt speeches (the two different Roosevelts) to be the closest, and the 1909 Taft and 1997 Clinton speeches to be furthest apart. If we want to, we can even convert this data into a dendrogram. We do this by taking the information on the distances out of the \texttt{corpus\_distances} object, make them into a triangular matrix, and plot them:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{hclust}\NormalTok{(}\FunctionTok{as.dist}\NormalTok{(corpus\_distances)), }\AttributeTok{hang =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-64-1.pdf}

Here, we can easily see that - amongst others - the 1909 Taft document is the ``farthest'' away from all the others, that the 1981 and 1985 Reagan speeches were very close, and that the 1997 Clinton speech was closer to Nixon's speeches, than his 1993 speech (which was close to the 2009 and 2013 Obama speeches).

Finally, let us look at the entropy of our texts. The entropy of a document measures the ``amount'' of information the is produced by each letter of the text. To get an idea of what this means, consider the ``e'' is a frequently occurring letter in an English text, while ``z'' is not. Thus, a word with a ``z'' in it, it more unique and thus likely to carry unique and interesting information. The ``higher'' the entropy of a text, the less ``information'' is in it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_entropy\_docs }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_entropy}\NormalTok{(data\_inaugural\_dfm, }\StringTok{"documents"}\NormalTok{)}
\NormalTok{corpus\_entropy\_docs }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(corpus\_entropy\_docs)}
\end{Highlighting}
\end{Shaded}

As we can see, the Roosevelt speeches had the lowest entropies, while the 1909 Taft and 1925 Coolidge speeches were the highest (in relative terms). While not as common as the other distances metrics, entropy is sometimes used to measure the similarity between texts and can be useful if we want to know the importance of certain words. This because if a certain word is not ``important,'' we might consider it to become a stop word:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_entropy\_feats }\OtherTok{\textless{}{-}} \FunctionTok{textstat\_entropy}\NormalTok{(data\_inaugural\_dfm, }\StringTok{"features"}\NormalTok{)}
\NormalTok{corpus\_entropy\_feats }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(corpus\_entropy\_feats)}
\NormalTok{corpus\_entropy\_feats }\OtherTok{\textless{}{-}}\NormalTok{ corpus\_entropy\_feats[}\FunctionTok{order}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{corpus\_entropy\_feats}\SpecialCharTok{$}\NormalTok{entropy),]}
\FunctionTok{head}\NormalTok{(corpus\_entropy\_feats, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     feature  entropy
## 164  people 4.766391
## 488    life 4.747627
## 385  nation 4.737440
## 5     great 4.654392
## 114     can 4.651396
## 317  future 4.639222
## 197   world 4.616910
## 212    time 4.616614
## 402    must 4.610073
## 231     god 4.601430
\end{verbatim}

Looking at the data, we find that ``people,'' ``life'' and ``nation'' have pretty high entropies, indicating that the words added little in terms of information to the documents, and it would be a candidate to remove from our corpus.

\hypertarget{dictionary-analysis}{%
\chapter{Dictionary Analysis}\label{dictionary-analysis}}

One of the simplest forms of quantitative text analysis is dictionary analysis. We can define dictionary methods as those which simply use the rate at which key words appear in a text to classify documents into categories or to measure the extent to which documents belong to particular categories, without making further assumptions. In many respects, dictionary methods present a non-statistical, categorical analysis approach.

One of the most well-known examples of using dictionary methods is the measuring the tone in newspaper articles, speeches, children's writings, and so on, by using the so-called sentiment analysis dictionaries. Another well-known example is the measuring of policy content in different documents as illustrated by the Policy Agendas Project dictionary (\protect\hyperlink{ref-Albaugh2013}{Albaugh et al.} (\protect\hyperlink{ref-Albaugh2013}{2013})). Here, we will carry out three such analyses, the first a standard analysis and the other two focusing on sentiment. For the former, we will use political party manifestos, while for the latter we will use movie reviews and Twitter data.

\hypertarget{classical-dictionary-analysis}{%
\section{Classical Dictionary Analysis}\label{classical-dictionary-analysis}}

As for our dictionaries, we can either make the dictionary ourselves or use an off-the-shelf version. For the latter, we can either import the files we already have into R or use some of the versions that come with the \texttt{quanteda.dictionaries} package. For this, we first load the package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.dictionaries)}
\end{Highlighting}
\end{Shaded}

We then apply one of these dictionaries to the document feature matrix we made earlier. As a dictionary, we will use the one made by \protect\hyperlink{ref-Laver2000a}{Laver and Garry} (\protect\hyperlink{ref-Laver2000a}{2000}), meant for estimating policy positions from political texts. We first load this dictionary into R and then run it on the dfm using the \texttt{dfm\_lookup} command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_dictionary\_LaverGarry}
\NormalTok{dictionary\_results }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_lookup}\NormalTok{(data\_inaugural\_dfm, data\_dictionary\_LaverGarry)}
\NormalTok{dictionary\_results}
\end{Highlighting}
\end{Shaded}

Apart from off-the-shelf dictionaries, it is also possible to create our own which could suit our research question better. One approach in dictionary construction is to use prior theory deductively to come up with different categories and their associated words. Another approach is to use reference texts in order to come up with categories and words inductively. We can also combine different dictionaries as illustrated by \protect\hyperlink{ref-Young2012}{Young and Soroka} (\protect\hyperlink{ref-Young2012}{2012}), or different dictionaries and keywords from categories in manual coding scheme (\protect\hyperlink{ref-Lind2019}{Lind et al.} (\protect\hyperlink{ref-Lind2019}{2019})). Finally, one can use expert or crowdcoding assessments to determine the words that best match different categories in a dictionary (\protect\hyperlink{ref-Haselmayer2017}{Haselmayer and Jenny} (\protect\hyperlink{ref-Haselmayer2017}{2017})).

If we want to create our own dictionary in \texttt{quanteda} we use the same commands as above, but we first have to create the dictionary. To do so, we specify the words in a named list. This list contains keys (the words we want to look for) and the categories to which they belong. We then transform this list into a dictionary. Here, we choose some words which we believe will allow us to easily identify the different parties:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dic\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{economy =} \FunctionTok{c}\NormalTok{(}\StringTok{"tax*"}\NormalTok{, }\StringTok{"vat"}\NormalTok{, }\StringTok{"trade"}\NormalTok{), }\AttributeTok{social =} \FunctionTok{c}\NormalTok{(}\StringTok{"Medicare"}\NormalTok{,}
    \StringTok{"GP"}\NormalTok{, }\StringTok{"health"}\NormalTok{), }\AttributeTok{devolution =} \FunctionTok{c}\NormalTok{(}\StringTok{"states"}\NormalTok{, }\StringTok{"senate"}\NormalTok{, }\StringTok{"independence"}\NormalTok{),}
    \AttributeTok{government =} \FunctionTok{c}\NormalTok{(}\StringTok{"Washington"}\NormalTok{, }\StringTok{"Congress"}\NormalTok{, }\StringTok{"White House"}\NormalTok{))}
\NormalTok{dic\_created }\OtherTok{\textless{}{-}} \FunctionTok{dictionary}\NormalTok{(dic\_list, }\AttributeTok{tolower =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{dic\_created}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Dictionary object with 4 key entries.
## - [economy]:
##   - tax*, vat, trade
## - [social]:
##   - Medicare, GP, health
## - [devolution]:
##   - states, senate, independence
## - [government]:
##   - Washington, Congress, White House
\end{verbatim}

If you compare the \texttt{dic\_list} file with the \texttt{data\_dictionary\_LaverGarry} file, you will find that it has the same structure. To see the result, we can use the same command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dictionary\_created }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_lookup}\NormalTok{(data\_inaugural\_dfm, dic\_created)}
\NormalTok{dictionary\_created}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Document-feature matrix of: 31 documents, 4 features (42.74% sparse) and 4 docvars.
##                 features
## docs             economy social devolution government
##   1901-McKinley        2      0          9          9
##   1905-Roosevelt       0      0          0          1
##   1909-Taft           17      0         12         14
##   1913-Wilson          1      2          1          0
##   1917-Wilson          1      0          2          0
##   1921-Harding         7      0          2          1
## [ reached max_ndoc ... 25 more documents ]
\end{verbatim}

\hypertarget{sentiment-analysis}{%
\section{Sentiment Analysis}\label{sentiment-analysis}}

The logic of dictionaries is that we can use them to see which kind of topics are present in our documents. Yet, we can also use them to provide us with measurements that are most often related to scaling. One way to do so is with \emph{sentiment} analysis. Here, we look at whether a certain piece of text is happy, angry, positive, negative, and so on. One case in which this can help us is with movie reviews. These reviews give us a description of a movie and then tell us their opinion. Another is when we look at Twitter data, to capture the ``mood of the moment.'' Here, we will look at both, starting with the movie reviews.

\hypertarget{movie-reviews}{%
\subsection{Movie Reviews}\label{movie-reviews}}

First, we load some reviews into R. The corpus we use here contains 50,000 movie reviews, each with a 1-10 rating (amongst others). As 50,000 reviews make the analysis quite slow, we will first select 30 reviews at random from this corpus. We do so via \texttt{corpus\_sample}, after which we transform it via a tokens object into a dfm:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.classifiers)}
\NormalTok{reviews }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_sample}\NormalTok{(data\_corpus\_LMRD, }\DecValTok{30}\NormalTok{)}
\NormalTok{reviews\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens}\NormalTok{(reviews)}
\NormalTok{reviews\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(reviews\_tokens)}
\end{Highlighting}
\end{Shaded}

The next step is to load in a sentiment analysis dictionary. Here, we will use the Lexicoder Sentiment Dictionary, included in \texttt{quanteda} and run it on the dfm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_dictionary\_LSD2015}
\NormalTok{results\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_lookup}\NormalTok{(reviews\_dfm, data\_dictionary\_LSD2015)}
\NormalTok{results\_dfm}
\end{Highlighting}
\end{Shaded}

The next step is to convert the results to a data frame and view them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentiment }\OtherTok{\textless{}{-}} \FunctionTok{convert}\NormalTok{(results\_dfm, }\AttributeTok{to=}\StringTok{"data.frame"}\NormalTok{)}
\NormalTok{sentiment}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   doc_id negative positive neg_positive neg_negative
## 1    test/pos/8273_9.txt        2        5            0            0
## 2    test/neg/6071_3.txt        5        8            0            0
## 3   test/pos/3535_10.txt        4       13            0            0
## 4   test/neg/10038_4.txt       15       10            0            0
## 5   train/pos/8503_8.txt       26       27            0            0
## 6    test/neg/9519_1.txt        5        4            0            0
## 7   train/neg/4256_4.txt        3       12            0            0
## 8   test/pos/1536_10.txt        5        5            0            0
## 9   train/neg/1414_3.txt        5        4            0            0
## 10  train/neg/4664_3.txt        5        5            0            0
## 11  train/neg/6683_1.txt       11        7            0            0
## 12   test/pos/4714_8.txt        7        2            0            0
## 13  train/neg/2414_3.txt       16       12            0            0
## 14  test/pos/9208_10.txt        6       10            0            0
## 15  train/pos/1102_8.txt       10       15            0            0
## 16  train/neg/9169_1.txt        7        2            0            0
## 17  train/pos/5187_7.txt        6       13            0            0
## 18   test/neg/3567_4.txt        7       10            0            0
## 19  train/neg/7802_1.txt       11        2            0            0
## 20  test/neg/10108_1.txt       17       12            0            0
## 21  train/neg/2454_4.txt       10       15            0            0
## 22 train/pos/9596_10.txt        1       10            0            0
## 23 train/pos/10286_9.txt        7       11            0            0
## 24 train/pos/6448_10.txt        3       12            0            0
## 25 train/pos/2569_10.txt        1        5            0            0
## 26  test/pos/7651_10.txt        4       16            0            0
## 27  test/neg/10588_1.txt        5        2            0            0
## 28   test/neg/8988_1.txt       15        8            0            0
## 29 train/neg/11305_1.txt       17        3            0            0
## 30  train/pos/108_10.txt        1        4            0            0
\end{verbatim}

Since movie reviews usually come with some sort of rating (often in the form of stars), we can see if this relates to the sentiment of the review. To do so, we have to take the rating out of the dfm and place it in a new data-frame with the positive and negative sentiments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{star\_data }\OtherTok{\textless{}{-}}\NormalTok{ reviews\_dfm}\SpecialCharTok{@}\NormalTok{docvars}\SpecialCharTok{$}\NormalTok{rating}
\NormalTok{stargraph }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(star\_data, sentiment}\SpecialCharTok{$}\NormalTok{negative, sentiment}\SpecialCharTok{$}\NormalTok{positive))}
\FunctionTok{names}\NormalTok{(stargraph) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"stars"}\NormalTok{,}\StringTok{"negative"}\NormalTok{,}\StringTok{"positive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To compare the sentiment with the stars, we first have to combine the senitments into a scale. Of the many ways to do so, the simplest is to take the difference between the positive and negative words (positive -- negative). Another option is to take the ratio of positive words against both positive and negative (positive/positive+negative). Here, we do both:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentiment\_difference }\OtherTok{\textless{}{-}}\NormalTok{ stargraph}\SpecialCharTok{$}\NormalTok{positive }\SpecialCharTok{{-}}\NormalTok{ stargraph}\SpecialCharTok{$}\NormalTok{negative}
\NormalTok{sentiment\_ratio }\OtherTok{\textless{}{-}}\NormalTok{ (stargraph}\SpecialCharTok{$}\NormalTok{positive}\SpecialCharTok{/}\NormalTok{(stargraph}\SpecialCharTok{$}\NormalTok{positive }\SpecialCharTok{+}
\NormalTok{    stargraph}\SpecialCharTok{$}\NormalTok{negative))}
\NormalTok{stargraph }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(stargraph, sentiment\_difference, sentiment\_ratio)}
\end{Highlighting}
\end{Shaded}

Then, we can plot the ratings and the scaled sentiment measures together with a linear regression line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(stargraph, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sentiment\_difference, }\AttributeTok{y =}\NormalTok{ stars)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Positive minus Negative"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylab}\NormalTok{(}\StringTok{"Stars"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-76-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(stargraph, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sentiment\_ratio, }\AttributeTok{y =}\NormalTok{ stars)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Ratio of Positive to Total"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Stars"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-77-1.pdf}

Finally, we would like to illustrate how one can make inferences by using the output of a dictionary analysis, by estimating confidence intervals around the point estimates. To do so, again the first step is to add a column which will be the total of positive and negative words scored by the dictionary. We do so by copying the data frame to a new data frame and adding a new column filled with NA values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reviews\_bootstrap   }\OtherTok{\textless{}{-}}\NormalTok{ sentiment}
\NormalTok{reviews\_bootstrap}\SpecialCharTok{$}\NormalTok{n }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

We then again specify the number of reviews, the replications that we want and change the data frame into an array:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(combinat)}

\NormalTok{nman }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(reviews\_bootstrap)}
\NormalTok{nrepl }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{manifBSn }\OtherTok{\textless{}{-}}\NormalTok{ manifBSnRand }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(reviews\_bootstrap[,}
    \DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]), }\FunctionTok{c}\NormalTok{(nman, }\DecValTok{2}\NormalTok{, nrepl }\SpecialCharTok{+} \DecValTok{1}\NormalTok{), }\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nman, }\FunctionTok{names}\NormalTok{(reviews\_bootstrap[,}
    \DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]), }\DecValTok{0}\SpecialCharTok{:}\NormalTok{nrepl))}
\end{Highlighting}
\end{Shaded}

Then, we bootstrap the word counts for each movie review and compute percentages for each category using a multinomial draw:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(manifBSn), , }\DecValTok{1}\NormalTok{], }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ manifBSn[, , }\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{n}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nrepl) \{}
\NormalTok{    manifBSn[, , i] }\OtherTok{\textless{}{-}} \FunctionTok{rmultinomial}\NormalTok{(n, p)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can then ask R to compute the quantities of interest. These are standard errors for each category, as well as the percentage coded for each category.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NegativeSE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"negative"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{PositiveSE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"positive"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, sd)}
\NormalTok{perNegative }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"negative"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\NormalTok{perPositive }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(manifBSn[, }\StringTok{"positive"}\NormalTok{, ]}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, mean)}
\end{Highlighting}
\end{Shaded}

We then save these quantities of interest in a new data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataBS }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(reviews\_bootstrap[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], NegativeSE,}
\NormalTok{    PositiveSE, perNegative, perPositive))}
\end{Highlighting}
\end{Shaded}

Then, we first calculate the confidence intervals and add these:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pos\_hi }\OtherTok{\textless{}{-}}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{perPositive }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{PositiveSE)}
\NormalTok{pos\_lo }\OtherTok{\textless{}{-}}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{perPositive }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{PositiveSE)}
\NormalTok{neg\_lo }\OtherTok{\textless{}{-}}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{perNegative }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{NegativeSE)}
\NormalTok{neg\_hi }\OtherTok{\textless{}{-}}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{perNegative }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ dataBS}\SpecialCharTok{$}\NormalTok{NegativeSE)}
\NormalTok{dataBS }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dataBS, pos\_hi, pos\_lo, neg\_lo, neg\_hi)}
\end{Highlighting}
\end{Shaded}

Finally, we can then make the graph. Here, we plot each of the positive and negative points and then overlay them with their error bars:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
 \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dataBS,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ perPositive, }\AttributeTok{y =}\NormalTok{ doc\_id), }\AttributeTok{shape =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
 \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dataBS,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ perNegative, }\AttributeTok{y =}\NormalTok{ doc\_id), }\AttributeTok{shape =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
 \FunctionTok{geom\_errorbarh}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dataBS,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ perPositive, }\AttributeTok{xmax =}\NormalTok{ pos\_hi,}\AttributeTok{xmin =}\NormalTok{ pos\_lo, }\AttributeTok{y =}\NormalTok{ doc\_id)) }\SpecialCharTok{+}
 \FunctionTok{geom\_errorbarh}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dataBS,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ perNegative, }\AttributeTok{xmax =}\NormalTok{ neg\_hi,}\AttributeTok{xmin =}\NormalTok{ neg\_lo, }\AttributeTok{y =}\NormalTok{ doc\_id)) }\SpecialCharTok{+}
 \FunctionTok{xlab}\NormalTok{(}\StringTok{"Percent positive/negative with 95\% CIs"}\NormalTok{) }\SpecialCharTok{+}
 \FunctionTok{ylab}\NormalTok{(}\StringTok{"review"}\NormalTok{)}\SpecialCharTok{+}
 \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-84-1.pdf}

As can be seen in this particular example, the fact that some documents are much less lengthier than others introduces a lot of uncertainty in the estimates. As evident from the overlapping confidence intervals in the figure, for most reviews, the percentage of negative words is not much different from the percentage of positive words. In other words: the sentiment for these reviews is rather mixed.

\hypertarget{twitter}{%
\subsection{Twitter}\label{twitter}}

Now, let us turn to an example using Twitter data. Here, we will look at the major problems that have occurred to several of the major US airlines. For this, data was scraped from Twitter between 16 and 24 February of 2015. Then, using using the Crowdflower platform, contributors were asked to classify each tweet (their sentiment) as either negative, positive, or neutral, and, if negative, what their reason was for classifying it as such. In addition, the data-set also contains information on how ``confident'' coders were on their classification and reason, as well as information on the Airline, and some info on the Tweet. Finally, we get some information on the ``gold'' tweets, which are used by Crowdflower to figure out how well there coders are doing (that is, these tweets have been expert coded).

We download the data from its website (\url{https://www.kaggle.com/crowdflower/twitter-airline-sentiment}), but for ease-of-use, we also placed it on GitHub, so we can directly import it into R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{urlfile }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/SCJBruinsma/qta{-}files/master/Tweets.csv"}
\NormalTok{tweets }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\FunctionTok{url}\NormalTok{(urlfile))}
\end{Highlighting}
\end{Shaded}

Given that this is Twitter data, we have to do quite some cleaning in order to filter out everything we do not want. While we earlier on saw that we can perform cleaning on a corpus, we can also clean text in a dataframe directly (basically in any string), with R's in-house \texttt{gsub} command, which basically replaces parts of the string. To understand how this works, say that we want to remove all the mentions of websites from our tweets. We then do as such:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"http.*"}\NormalTok{,}\StringTok{""}\NormalTok{,  tweets}\SpecialCharTok{$}\NormalTok{text)}
\end{Highlighting}
\end{Shaded}

Thus, we substitute those strings that start with "http.*" (the asterisk denotes a wildcard, which means that anything can follow) and replace it with "" (that is, nothing). We do this for any string that is in \texttt{tweets\$text}. Using this technique, we also remove slashes, punctuation, various symbols, ``RT'' (retweets), and references (``href''):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"https.*"}\NormalTok{,}\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text)}
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"@}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"[[:punct:]]"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"[ |}\SpecialCharTok{\textbackslash{}t}\StringTok{]\{2,\}"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"\^{} "}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" $"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"RT"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"href"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text)}
\end{Highlighting}
\end{Shaded}

We then transform our dataframe into a corpus (specifying that our text is in the \texttt{tweets\$text} field), and then transform this into a tokens object, lower all the words, remove the stop words, and finally make it into a dfm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_tweets }\OtherTok{\textless{}{-}} \FunctionTok{corpus}\NormalTok{(tweets, }\AttributeTok{text\_field =} \StringTok{"text"}\NormalTok{)}
\NormalTok{data\_tweets\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens}\NormalTok{(corpus\_tweets)}
\NormalTok{data\_tweets\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_tolower}\NormalTok{(data\_tweets\_tokens, }\AttributeTok{keep\_acronyms =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{data\_tweets\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_select}\NormalTok{(data\_tweets\_tokens, }\FunctionTok{stopwords}\NormalTok{(}\StringTok{"english"}\NormalTok{), }\AttributeTok{selection =} \StringTok{"remove"}\NormalTok{)}
\NormalTok{data\_tweets\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(data\_tweets\_tokens)}
\end{Highlighting}
\end{Shaded}

Now we can apply our dictionary. We can do this in two ways: applying it to the dfm, and applying it to the tokens object. Both should give roughly similar results. Yet, given that \texttt{dfm\_lookup()} cannot detect multi-word expressions (as the dfm gets rid of all word order), we can use the \texttt{tokens\_lookup()} and then convert this into a dfm, to compensate for this. One reason we might want to do this here, is because the LSD2015 dictionary contains some multi word expressions that \texttt{dfm\_lookup()} might miss. As a comparison, let's have a look at both:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_lookup}\NormalTok{(data\_tweets\_tokens, data\_dictionary\_LSD2015)}
\NormalTok{results\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(results\_tokens)}
\NormalTok{results\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{convert}\NormalTok{(results\_tokens, }\AttributeTok{to=}\StringTok{"data.frame"}\NormalTok{)}

\NormalTok{results\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_lookup}\NormalTok{(data\_tweets\_dfm, data\_dictionary\_LSD2015)}
\NormalTok{results\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{convert}\NormalTok{(results\_dfm, }\AttributeTok{to=}\StringTok{"data.frame"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, let us see how well our dictionary has done. To see this, we compare the sentiment of the tweet according to the dictionary with the sentiment assigned by the coder. We take this information out of our original data, and recode it (so it has got numerical values):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Caricamento del pacchetto richiesto: carData
\end{verbatim}

\begin{verbatim}
## 
## Caricamento pacchetto: 'car'
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto  mascherato da 'package:dplyr':
## 
##     recode
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto  mascherato da 'package:purrr':
## 
##     some
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{labels }\OtherTok{\textless{}{-}}\NormalTok{ tweets}\SpecialCharTok{$}\NormalTok{airline\_sentiment}
\NormalTok{labels }\OtherTok{\textless{}{-}}\NormalTok{ car}\SpecialCharTok{::}\FunctionTok{recode}\NormalTok{(labels, }\StringTok{"\textquotesingle{}positive\textquotesingle{}=1;\textquotesingle{}negative\textquotesingle{}={-}1;\textquotesingle{}neutral\textquotesingle{}=0"}\NormalTok{)}
\FunctionTok{table}\NormalTok{(labels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## labels
##   -1    0    1 
## 9178 3099 2363
\end{verbatim}

A quick look at the data (with \texttt{table()}) reveals that the majority of the tweets is negative (which is to be expected), a fair share neutral, and finally some positive ones. Now, let us bind this data to the output of our dictionary analysis, and calculate an overall score for each tweet by subtracting the positive score from the negative score (that is, the higher the score, the more positive the tweet):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{comparison\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(results\_tokens}\SpecialCharTok{$}\NormalTok{positive, results\_tokens}\SpecialCharTok{$}\NormalTok{negative, labels))}
\NormalTok{difference\_tokens }\OtherTok{\textless{}{-}}\NormalTok{ results\_tokens}\SpecialCharTok{$}\NormalTok{positive }\SpecialCharTok{{-}}\NormalTok{ results\_tokens}\SpecialCharTok{$}\NormalTok{negative}
\NormalTok{comparison\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(comparison\_tokens, difference\_tokens)}

\NormalTok{comparison\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(results\_dfm}\SpecialCharTok{$}\NormalTok{positive, results\_dfm}\SpecialCharTok{$}\NormalTok{negative, labels))}
\NormalTok{difference\_dfm }\OtherTok{\textless{}{-}}\NormalTok{ results\_dfm}\SpecialCharTok{$}\NormalTok{positive }\SpecialCharTok{{-}}\NormalTok{ results\_dfm}\SpecialCharTok{$}\NormalTok{negative}
\NormalTok{comparison\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(comparison\_dfm, difference\_dfm)}
\end{Highlighting}
\end{Shaded}

Finally, we can place this all in a graph, in which we plot both the human judgement scores and the scores calculated by subtracting the positive and negative codes. In addition, we plot a simple linear equation to better understand the relation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(comparison\_tokens ,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ difference\_tokens, }\AttributeTok{y =}\NormalTok{ labels)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{shape =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Positive minus Negative"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Human judgment"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Using Tokens"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-92-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(comparison\_dfm, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ difference\_dfm, }\AttributeTok{y =}\NormalTok{ labels)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{shape =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Positive minus Negative"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Human Judgment"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Using DFM"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-92-2.pdf}

As we can see, there is a slightly positive relation (0.6947 for the tokens and 0.6914 for the dfm), which is a relatively good thing considering our approach does not involve any human coders at all. Though, how good it is of course depends on the benchmark that we set ourselves.

\hypertarget{scaling}{%
\chapter{Scaling}\label{scaling}}

With a dictionary, we aimed to classify our texts into different categories based on the words they contain. While practical, there is no real way to compare these categories: one category is no better or worse than the other. If we do want to compare texts, we have to place them on some sort of scale. Here, we will look at three ways in which we can do so: \emph{Wordscores} (\protect\hyperlink{ref-Laver2003a}{Laver, Benoit, and Garry 2003}), \emph{Wordfish} (\protect\hyperlink{ref-Slapin2008a}{Slapin and Proksch 2008}), and Correspondence Analysis. The first two methods used to be part of the main \texttt{quanteda} package, but have now moved to the \texttt{quanteda.textmodels} package, while we find CA in the \texttt{FactoMineR} package.

\hypertarget{wordscores}{%
\section{Wordscores}\label{wordscores}}

The idea of Wordscores is to use reference texts (from which we know the position) to position our virgin texts (from which we do not know the position). Here, we will use the data from the 2001 and 2005 party manifestos of the five largest parties in the United Kingdom, and will use the 2001 documents as reference texts and the 2005 documents as virgin texts. Also the scale we want to position our documents on is the general left-right scale. Thus, we need to know the positions for the 2001 documents on this. Here, we will use the left-right scale from the 2002 Chapel Hill Expert Survey (\protect\hyperlink{ref-Bakker2012a}{Bakker et al. 2012}) to do so. So, we load our data, make the subset, transform it into a dfm, and clean it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda)}
\FunctionTok{library}\NormalTok{(quanteda.corpora)}

\FunctionTok{data}\NormalTok{(data\_corpus\_ukmanifestos)}
\NormalTok{corpus\_manifestos }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_subset}\NormalTok{(data\_corpus\_ukmanifestos,}
\NormalTok{    Year }\SpecialCharTok{==} \DecValTok{2001} \SpecialCharTok{|}\NormalTok{ Year }\SpecialCharTok{==} \DecValTok{2005}\NormalTok{)}
\NormalTok{corpus\_manifestos }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_subset}\NormalTok{(corpus\_manifestos, Party }\SpecialCharTok{==}
    \StringTok{"Lab"} \SpecialCharTok{|}\NormalTok{ Party }\SpecialCharTok{==} \StringTok{"LD"} \SpecialCharTok{|}\NormalTok{ Party }\SpecialCharTok{==} \StringTok{"Con"} \SpecialCharTok{|}\NormalTok{ Party }\SpecialCharTok{==} \StringTok{"SNP"} \SpecialCharTok{|}
\NormalTok{    Party }\SpecialCharTok{==} \StringTok{"PCy"}\NormalTok{)}

\NormalTok{data\_manifestos\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens}\NormalTok{(corpus\_manifestos, }\AttributeTok{what =} \StringTok{"word"}\NormalTok{,}
    \AttributeTok{remove\_punct =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_symbols =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_numbers =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{remove\_url =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_separators =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{split\_hyphens =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{include\_docvars =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{padding =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{data\_manifestos\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_tolower}\NormalTok{(data\_manifestos\_tokens,}
    \AttributeTok{keep\_acronyms =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{data\_manifestos\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{tokens\_select}\NormalTok{(data\_manifestos\_tokens,}
    \FunctionTok{stopwords}\NormalTok{(}\StringTok{"english"}\NormalTok{), }\AttributeTok{selection =} \StringTok{"remove"}\NormalTok{)}

\NormalTok{data\_manifestos\_dfm }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(data\_manifestos\_tokens)}
\end{Highlighting}
\end{Shaded}

Then , we check the order of the documents inside our dfm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_manifestos\_dfm}\SpecialCharTok{@}\NormalTok{Dimnames}\SpecialCharTok{$}\NormalTok{docs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "UK_natl_2001_en_Con" "UK_natl_2001_en_Lab" "UK_natl_2001_en_LD" 
##  [4] "UK_natl_2001_en_PCy" "UK_natl_2001_en_SNP" "UK_natl_2005_en_Con"
##  [7] "UK_natl_2005_en_Lab" "UK_natl_2005_en_LD"  "UK_natl_2005_en_PCy"
## [10] "UK_natl_2005_en_SNP"
\end{verbatim}

We can then set the scores for the reference texts. For the virgin texts, we set \texttt{NA} instead. Then, we run the wordscores model - providing the dfm and the reference scores - and save it into an object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textmodels)}

\NormalTok{scores }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{7.72}\NormalTok{,}\FloatTok{5.18}\NormalTok{,}\FloatTok{3.82}\NormalTok{,}\FloatTok{3.2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\ConstantTok{NA}\NormalTok{,}\ConstantTok{NA}\NormalTok{)}
\NormalTok{ws }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_wordscores}\NormalTok{(data\_manifestos\_dfm, scores)}
\FunctionTok{summary}\NormalTok{(ws)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## textmodel_wordscores.dfm(x = data_manifestos_dfm, y = scores)
## 
## Reference Document Statistics:
##                     score total min max   mean median
## UK_natl_2001_en_Con  7.72  7179   0  92 0.8606      0
## UK_natl_2001_en_Lab  5.18 16395   0 166 1.9654      0
## UK_natl_2001_en_LD   3.82 12337   0 101 1.4789      0
## UK_natl_2001_en_PCy  3.20  3508   0  72 0.4205      0
## UK_natl_2001_en_SNP  3.00  5693   0 108 0.6825      0
## UK_natl_2005_en_Con    NA  4350   0  46 0.5215      0
## UK_natl_2005_en_Lab    NA 13370   0 147 1.6027      0
## UK_natl_2005_en_LD     NA  9265   0 109 1.1106      0
## UK_natl_2005_en_PCy    NA  4204   0 148 0.5040      0
## UK_natl_2005_en_SNP    NA  1509   0  49 0.1809      0
## 
## Wordscores:
## (showing first 30 elements)
##         time       common        sense conservative    manifesto introduction 
##        5.838        6.540        7.376        7.161        4.478        3.982 
##        lives      raising       family       living       safely      earning 
##        6.047        4.427        5.519        4.719        5.743        6.046 
##      staying      healthy      growing        older      knowing        world 
##        6.946        4.294        4.745        6.280        7.720        4.366 
##       leader     stronger      society         town      country    civilised 
##        4.524        4.910        4.342        7.515        4.401        4.278 
##        proud    democracy   conclusion      present    ambitious    programme 
##        6.069        5.267        6.946        3.594        4.466        4.233
\end{verbatim}

When we run the \texttt{summary} command, we can see the word scores for each word. This is the position of that word on our scale of interest. We then only need to figure out how often these words occur in each of the texts, add up their scores, and divide this by the total number of words of the texts. This gives us the \emph{raw score} of the text. Yet, this raw score has some problems. Most important of which is that as some words occur in almost all texts, all the scores will be very clustered in the middle of our scale. To prevent this, we can spread out the scores again, so they look more like the scores of our reference texts. This rescaling has two versions. The first was the original as proposed by \protect\hyperlink{ref-Laver2003a}{Laver, Benoit, and Garry} (\protect\hyperlink{ref-Laver2003a}{2003}), and focuses on the variance of the scores. The idea here is that the distribution of the scores of the virgin texts has the correct mean, but an incorrect variance which needs rescaling. The second, proposed by \protect\hyperlink{ref-Martin2008a}{Martin and Vanberg} (\protect\hyperlink{ref-Martin2008a}{2008}), focuses on the extremes of the scores. What it does is to take the scores of the virgin texts and stretch them out to match the extremes of the scores of the reference texts. Here, we run both so we can compare them. For the MV transformation, we will calculate the standard errors for the scores as well:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_lbg }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ws, }\AttributeTok{rescaling =} \StringTok{"lbg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 2203 features in newdata not used in prediction.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_mv }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ws, }\AttributeTok{rescaling =} \StringTok{"mv"}\NormalTok{, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 2203 features in newdata not used in prediction.
\end{verbatim}

\begin{verbatim}
## Warning in predict.textmodel_wordscores(ws, rescaling = "mv", se.fit = TRUE, :
## More than two reference scores found with MV rescaling; using only min, max
## values.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_lbg}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## UK_natl_2001_en_Con UK_natl_2001_en_Lab  UK_natl_2001_en_LD UK_natl_2001_en_PCy 
##            8.794566            5.440327            3.971305            1.921840 
## UK_natl_2001_en_SNP UK_natl_2005_en_Con UK_natl_2005_en_Lab  UK_natl_2005_en_LD 
##            2.166928            5.656940            5.128174            5.047475 
## UK_natl_2005_en_PCy UK_natl_2005_en_SNP 
##            3.752962            4.289754
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_mv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $fit
##                          fit      lwr      upr
## UK_natl_2001_en_Con 7.720000 7.633952 7.806048
## UK_natl_2001_en_Lab 5.331214 5.295467 5.366960
## UK_natl_2001_en_LD  4.285022 4.243678 4.326365
## UK_natl_2001_en_PCy 2.825456 2.750505 2.900406
## UK_natl_2001_en_SNP 3.000000 2.932910 3.067090
## UK_natl_2005_en_Con 5.485479 5.387391 5.583567
## UK_natl_2005_en_Lab 5.108908 5.060253 5.157564
## UK_natl_2005_en_LD  5.051437 4.989127 5.113747
## UK_natl_2005_en_PCy 4.129525 4.039903 4.219146
## UK_natl_2005_en_SNP 4.511812 4.323620 4.700003
## 
## $se.fit
## UK_natl_2001_en_Con UK_natl_2001_en_Lab  UK_natl_2001_en_LD UK_natl_2001_en_PCy 
##          0.04390309          0.01823830          0.02109396          0.03824078 
## UK_natl_2001_en_SNP UK_natl_2005_en_Con UK_natl_2005_en_Lab  UK_natl_2005_en_LD 
##          0.03423029          0.05004577          0.02482464          0.03179121 
## UK_natl_2005_en_PCy UK_natl_2005_en_SNP 
##          0.04572606          0.09601792
\end{verbatim}

Note that this does not only predict the 2005 texts, but also the 2001 texts. As such, we can use these scores to see how well this procedure can recover the original scores. One reason why this might be a problem is because of a warning you most likely received. This says that ``\emph{n} features in newdata not used in prediction.'' This is as the method does not use all the words from the reference texts to score the virgin texts. Instead, it only uses the words that occur in them both. Thus, when we compare the reference scores with the scores the method gives to the reference documents, can see how well the method does.

To compare the scores, we will use the Concordance Correlation Coefficient as developed by \protect\hyperlink{ref-Lin1989a}{Lin} (\protect\hyperlink{ref-Lin1989a}{1989}). This coefficient estimates how far two sets of data deviate from a line of 45 degrees (which indicates perfect agreement). To calculate this, we take the scores (here we take the LBG version) from the object we created and combine them with the original scores. From this, we only select the first five texts (those from 2001) and calculate the CCC:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{comparison }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(pred\_lbg, scores))}
\NormalTok{comparison }\OtherTok{\textless{}{-}}\NormalTok{ comparison[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, ]}

\FunctionTok{CCC}\NormalTok{(comparison}\SpecialCharTok{$}\NormalTok{scores, comparison}\SpecialCharTok{$}\NormalTok{pred\_lbg, }\AttributeTok{ci =} \StringTok{"z{-}transform"}\NormalTok{,}
    \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $rho.c
##         est    lwr.ci   upr.ci
## 1 0.9239205 0.8242101 0.968064
## 
## $s.shift
## [1] 1.443978
## 
## $l.shift
## [1] -0.05966866
## 
## $C.b
## [1] 0.9345491
## 
## $blalt
##       mean      delta
## 1 8.257283 -1.0745660
## 2 5.310163 -0.2603267
## 3 3.895653 -0.1513052
## 4 2.560920  1.2781600
## 5 2.583464  0.8330719
\end{verbatim}

The result here is not bad, though the confidence intervals are rather large. We can have a further look at why this is the case by plotting the data. In this plot, we will show the position of the texts, as well as a 45-degree line. Also, we plot the reduced major axis, which shows the symmetrical relationship between the two variables. This line is a linear regression, which we compute first using the \texttt{lm} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{lm\_line }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(comparison}\SpecialCharTok{$}\NormalTok{scores }\SpecialCharTok{\textasciitilde{}}\NormalTok{ comparison}\SpecialCharTok{$}\NormalTok{pred\_lbg)}

\FunctionTok{ggplot}\NormalTok{(comparison, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{scores, }\AttributeTok{y=}\NormalTok{pred\_lbg)) }\SpecialCharTok{+} 
 \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
 \FunctionTok{xlab}\NormalTok{(}\StringTok{"Original Scores"}\NormalTok{)}\SpecialCharTok{+}
 \FunctionTok{ylab}\NormalTok{(}\StringTok{"LBG Scores"}\NormalTok{)}\SpecialCharTok{+}
 \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{)}\SpecialCharTok{+}
 \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{)}\SpecialCharTok{+}
 \FunctionTok{geom\_abline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{0}\NormalTok{,}
                 \AttributeTok{slope =}\DecValTok{1}\NormalTok{,}
                 \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{))}\SpecialCharTok{+}
 \FunctionTok{geom\_abline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ lm\_line}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{],}
                 \AttributeTok{slope =}\NormalTok{ lm\_line}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{],}
                 \AttributeTok{linetype =} \StringTok{"solid"}\NormalTok{ ))}\SpecialCharTok{+}
 \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{name =} \StringTok{""}\NormalTok{,}
                    \AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{breaks=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                    \AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"Line of perfect concordance"}\NormalTok{ , }\StringTok{"Reduced major axis"}\NormalTok{))}\SpecialCharTok{+}
 \FunctionTok{scale\_linetype\_manual}\NormalTok{(}\AttributeTok{name =} \StringTok{""}\NormalTok{,}
                       \AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                       \AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"Line of perfect concordance"}\NormalTok{ , }\StringTok{"Reduced major axis"}\NormalTok{))}\SpecialCharTok{+}
 \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-99-1.pdf}

This graph allows us to spot the problem. That is that while we gave the manifesto for Plaid Cymru (PCy) a reference score of 3.20, Wordscores gave it 1.91. Removing this manifesto from our data-set would thus improve our estimates.

Apart from positioning the texts, we can also have a look at the words themselves. We can do this with the \texttt{textplot\_scale1d} command, for which we also specify some words to highlight:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textplots)}

\FunctionTok{textplot\_scale1d}\NormalTok{(ws, }\AttributeTok{margin =} \StringTok{"features"}\NormalTok{, }\AttributeTok{highlighted =} \FunctionTok{c}\NormalTok{(}\StringTok{"british"}\NormalTok{,}
    \StringTok{"vote"}\NormalTok{, }\StringTok{"europe"}\NormalTok{, }\StringTok{"taxes"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-100-1.pdf}

Finally, we can have a look at the confidence intervals around the scores we created. For this, we use the same command as above, though instead of specifying \texttt{features} (referring to the words), we specify \texttt{texts}. Note that we can only do this for the MV scores, as only here we also calculated the standard errors:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{textplot\_scale1d}\NormalTok{(pred\_mv, }\AttributeTok{margin =} \StringTok{"documents"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-101-1.pdf}

Note that we can also make this graph ourselves. This requires some data-wrangling using the \texttt{dplyr} package. This package allows us to use pipes, which are denoted by the \texttt{\%\textgreater{}\%} command. This pipe transports an output of a command to another one before saving it. This saves us from constructing too many intermediate data-sets. Thus, here we first bind together the row names of the fit (which denote the documents), the fit itself, and the standard error of the fit (which also includes the lower and upper bound). We then transform this into a tibble (which is similar to a dataframe), rename the first and fifth columns, and finally ensure that all the values (which are still characters), are numeric (and year a factor):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{data\_textplot }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(pred\_mv}\SpecialCharTok{$}\NormalTok{se.fit)), pred\_mv}\SpecialCharTok{$}\NormalTok{fit, pred\_mv}\SpecialCharTok{$}\NormalTok{se.fit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{id =} \DecValTok{1}\NormalTok{,}
         \AttributeTok{se =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{as.numeric}\NormalTok{(fit),}
         \AttributeTok{lwr =} \FunctionTok{as.numeric}\NormalTok{(lwr),}
         \AttributeTok{upr =} \FunctionTok{as.numeric}\NormalTok{(upr),}
         \AttributeTok{se =} \FunctionTok{as.numeric}\NormalTok{(se),}
         \AttributeTok{year =} \FunctionTok{as.factor}\NormalTok{(stringr}\SpecialCharTok{::}\FunctionTok{str\_sub}\NormalTok{(id, }\AttributeTok{start =} \DecValTok{9}\NormalTok{, }\AttributeTok{end =} \DecValTok{12}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.
## Using compatibility `.name_repair`.
\end{verbatim}

If we now look at our \texttt{data\_textplot} object, we see that we have all the data we need: the fit (the average value), the lower and upper bounds, the year and the id that tells us with which party and year we are dealing. The only thing that we perhaps can do is to give the parties slightly better names. To see the current ones, type \texttt{data\_textplot\$id} in the console. We can then give them different names (just ensure that the order remains the same). We then sort them in decreasing order based on their fit:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_textplot}\SpecialCharTok{$}\NormalTok{id }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"CON 2001"}\NormalTok{, }\StringTok{"LAB 2001"}\NormalTok{, }\StringTok{"LD 2001"}\NormalTok{, }\StringTok{"PCY 2001"}\NormalTok{, }\StringTok{"SNP 2001"}\NormalTok{, }\StringTok{"CON 2005"}\NormalTok{,}\StringTok{"LAB 2005"}\NormalTok{, }\StringTok{"LD 2005"}\NormalTok{,}\StringTok{"PCY 2005"}\NormalTok{, }\StringTok{"SNP 2005"}\NormalTok{))}
\NormalTok{data\_textplot}\SpecialCharTok{$}\NormalTok{id }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(data\_textplot,  }\FunctionTok{reorder}\NormalTok{(id, fit))}
\end{Highlighting}
\end{Shaded}

Then, we can plot this data using \texttt{ggplot}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data\_textplot, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit, }\AttributeTok{y =}\NormalTok{ id, }\AttributeTok{colour =}\NormalTok{ year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbarh}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data\_textplot, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmax =}\NormalTok{ upr, }\AttributeTok{xmin =}\NormalTok{ lwr,  }\AttributeTok{y =}\NormalTok{ id, }\AttributeTok{colour =}\NormalTok{ year), }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#ffa600"}\NormalTok{, }\StringTok{"\#ff6361"}\NormalTok{),}
                      \AttributeTok{name =} \StringTok{"Years:"}\NormalTok{,}
                      \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\StringTok{"2001"}\NormalTok{, }\StringTok{"2005"}\NormalTok{),}
                      \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"2001"}\NormalTok{, }\StringTok{"2005"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Left{-}Right Distribution of UK Party Manifestos"}\NormalTok{,}
       \AttributeTok{subtitle =} \StringTok{"with 95\% confidence intervals"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Left {-} Right Score"}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{20}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \AttributeTok{legend.position =} \StringTok{"top"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-104-1.pdf}

\hypertarget{wordfish}{%
\section{Wordfish}\label{wordfish}}

Different from Wordscores, for Wordfish we do not need any reference text. Instead of this, the method using a model (based on a Poisson distribution) to calculate the scores for the texts. The only thing we have to tell Wordfish is which texts define the extremes of our scale. While this might seems very practical, it also leaves us with a problem: which scale do we want? For example, let us say that we consider our corpus of inaugural speeches of American presidents we saw earlier. What scale should we be interested in? Let us for now say that we care about a general left-right position. As benchmarks, we then set the 1965 Johnson speech as the most ``left'' and the 1985 Reagan speech as the most ``right.'' Also, we set a seed as the model draws random numbers and we want our work to be replicable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_inaugural\_dfm}\SpecialCharTok{@}\NormalTok{Dimnames}\SpecialCharTok{$}\NormalTok{docs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "1901-McKinley"   "1905-Roosevelt"  "1909-Taft"       "1913-Wilson"    
##  [5] "1917-Wilson"     "1921-Harding"    "1925-Coolidge"   "1929-Hoover"    
##  [9] "1933-Roosevelt"  "1937-Roosevelt"  "1941-Roosevelt"  "1945-Roosevelt" 
## [13] "1949-Truman"     "1953-Eisenhower" "1957-Eisenhower" "1961-Kennedy"   
## [17] "1965-Johnson"    "1969-Nixon"      "1973-Nixon"      "1977-Carter"    
## [21] "1981-Reagan"     "1985-Reagan"     "1989-Bush"       "1993-Clinton"   
## [25] "1997-Clinton"    "2001-Bush"       "2005-Bush"       "2009-Obama"     
## [29] "2013-Obama"      "2017-Trump"      "2021-Biden.txt"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wordfish }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_wordfish}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{dir =} \FunctionTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{,}
    \DecValTok{22}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(wordfish)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## textmodel_wordfish.dfm(x = data_inaugural_dfm, dir = c(17, 22))
## 
## Estimated Document Positions:
##                   theta      se
## 1901-McKinley    1.5265 0.03498
## 1905-Roosevelt   0.5503 0.07556
## 1909-Taft        2.1184 0.01580
## 1913-Wilson      0.9487 0.05162
## 1917-Wilson      0.8646 0.05786
## 1921-Harding     1.3072 0.03094
## 1925-Coolidge    1.4402 0.02757
## 1929-Hoover      1.5401 0.02740
## 1933-Roosevelt   1.1203 0.04539
## 1937-Roosevelt   0.6489 0.05203
## 1941-Roosevelt   0.1151 0.06610
## 1945-Roosevelt  -0.1673 0.10037
## 1949-Truman      0.8432 0.04453
## 1953-Eisenhower  0.2793 0.04693
## 1957-Eisenhower  0.1598 0.05757
## 1961-Kennedy    -0.5388 0.05631
## 1965-Johnson    -0.7804 0.05190
## 1969-Nixon      -0.9598 0.03916
## 1973-Nixon      -0.4417 0.05235
## 1977-Carter     -0.3437 0.06414
## 1981-Reagan     -0.6960 0.04169
## 1985-Reagan     -0.6409 0.04007
## 1989-Bush       -0.8890 0.03935
## 1993-Clinton    -1.1441 0.04009
## 1997-Clinton    -0.8663 0.03910
## 2001-Bush       -0.7422 0.04953
## 2005-Bush       -0.4094 0.04772
## 2009-Obama      -1.0796 0.03436
## 2013-Obama      -1.0532 0.03719
## 2017-Trump      -1.3810 0.03639
## 2021-Biden.txt  -1.3289 0.02999
## 
## Estimated Feature Scores:
##      fellow-citizens assembled    4th   march   great anxiety  regard currency
## beta           2.084   -0.2363  1.308 -0.0627 -0.1155  0.5387  0.8081    2.057
## psi           -4.874   -2.7959 -4.776 -1.7305  1.5216 -2.7801 -2.2013   -3.848
##       credit    none  exists     now treasury receipts inadequate    meet
## beta  0.8941  0.4225 -0.7698 -0.5027    1.468    2.291      0.180 -0.3738
## psi  -2.1652 -2.1725 -2.4684  1.2740   -4.312   -5.629     -2.235  0.1264
##      current obligations government sufficient   public   needs surplus instead
## beta   1.455      0.5204     0.1712     0.5477  0.42723 -0.2554   1.308 -1.0352
## psi   -3.600     -1.0900     1.6961    -1.4018 -0.07204 -0.7792  -4.776 -0.9148
##      deficit    felt constrained convene congress extraordinary
## beta -0.1073  0.3725       1.308   1.308   0.7189        0.7233
## psi  -1.4313 -2.3578      -4.776  -4.776  -0.2453       -2.9579
\end{verbatim}

Here, \emph{theta} gives us the position of the text. As with Wordscores, we can also calculate the confidence intervals (note that \emph{theta} is now called \emph{fit}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_wordfish }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(wordfish, }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{)}
\NormalTok{pred\_wordfish}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $fit
##                        fit         lwr        upr
## 1901-McKinley    1.5264549  1.45790117  1.5950087
## 1905-Roosevelt   0.5503249  0.40223178  0.6984181
## 1909-Taft        2.1183666  2.08740335  2.1493298
## 1913-Wilson      0.9487067  0.84753556  1.0498779
## 1917-Wilson      0.8645950  0.75120033  0.9779896
## 1921-Harding     1.3071772  1.24653710  1.3678173
## 1925-Coolidge    1.4402120  1.38616969  1.4942542
## 1929-Hoover      1.5400842  1.48638959  1.5937789
## 1933-Roosevelt   1.1202695  1.03130137  1.2092377
## 1937-Roosevelt   0.6488960  0.54692619  0.7508658
## 1941-Roosevelt   0.1151337 -0.01441162  0.2446789
## 1945-Roosevelt  -0.1673134 -0.36404388  0.0294171
## 1949-Truman      0.8431987  0.75592790  0.9304694
## 1953-Eisenhower  0.2792604  0.18727234  0.3712484
## 1957-Eisenhower  0.1598369  0.04700292  0.2726709
## 1961-Kennedy    -0.5387860 -0.64916020 -0.4284118
## 1965-Johnson    -0.7804433 -0.88215746 -0.6787291
## 1969-Nixon      -0.9598015 -1.03655447 -0.8830485
## 1973-Nixon      -0.4416846 -0.54428665 -0.3390825
## 1977-Carter     -0.3436878 -0.46940309 -0.2179725
## 1981-Reagan     -0.6960482 -0.77775955 -0.6143369
## 1985-Reagan     -0.6409237 -0.71946264 -0.5623847
## 1989-Bush       -0.8890203 -0.96614097 -0.8118997
## 1993-Clinton    -1.1441087 -1.22269262 -1.0655248
## 1997-Clinton    -0.8663188 -0.94294521 -0.7896924
## 2001-Bush       -0.7422147 -0.83928343 -0.6451460
## 2005-Bush       -0.4094409 -0.50296128 -0.3159205
## 2009-Obama      -1.0796208 -1.14695763 -1.0122840
## 2013-Obama      -1.0531958 -1.12609628 -0.9802953
## 2017-Trump      -1.3810308 -1.45235286 -1.3097088
## 2021-Biden.txt  -1.3288773 -1.38765502 -1.2700996
\end{verbatim}

As with Wordscores, we can also plot graphs for Wordfish, using the same commands. The first graph we will again be looking at is the distribution of the words, which here forms an ``Eifel Tower'' like graph:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{textplot\_scale1d}\NormalTok{(wordfish, }\AttributeTok{margin =} \StringTok{"features"}\NormalTok{, }\AttributeTok{highlighted =} \FunctionTok{c}\NormalTok{(}\StringTok{"america"}\NormalTok{,}
    \StringTok{"jobs"}\NormalTok{, }\StringTok{"taxes"}\NormalTok{, }\StringTok{"election"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-107-1.pdf}

And then we can do the same for the documents as well. Note that we can also make a similar graph to the one we made ourselves above (just replace \texttt{pred\_mv} with \texttt{pred\_wordfish}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{textplot\_scale1d}\NormalTok{(wordfish, }\AttributeTok{margin =} \StringTok{"documents"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-108-1.pdf}

Looking at the results here gives us an interesting picture. Remember that we chose our benchmark texts to look at the left-right position of our texts? Here, we see that both these texts (the 1965 Johnson and 1985 Reagan) are actually quite close to each other. Sticking with our interpretation that Reagan is more right-wing than Johnson, this would mean that the 1909 Taft address was the most right-wing and the 2017 Trump text the most left wing. Whether this is true is of course up to our own interpretation.

\hypertarget{correspondence-analysis}{%
\section{Correspondence Analysis}\label{correspondence-analysis}}

Correspondence Analysis has a similar logic as Principal Component Analysis. Yet, while PCA requires metric data, CA only requires nominal data (such as text). The idea behind both is to reduce the complexity of the data by looking for new dimensions. These dimensions should then explain as much of the original variance that is present in the data as possible. Within R many packages can run CA (such as the \texttt{ca} and \texttt{FactoMineR} packages and even \texttt{quanteda.textmodels}). One interesting package is the \texttt{R.temis} package. The \texttt{R.temis} package is interesting as it aims to bring the techniques of qualitative text analysis into R. Thus, the package focus on the import of corpus from programs such as Alceste (\url{https://www.image-zafar.com/Logicieluk.html}) and sites such as LexisNexis (\url{https://www.lexisnexis.com}) - programs that are often used in qualitative text analysis. The package itself is build on the popular \texttt{tm} package and has a largely similar logic.

To carry out the Correspondence Analysis, \texttt{R.temis} uses the \texttt{FactoMineR} and \texttt{factoextra} packages. Here, we will look at an example with these packages using data from the an article on the stylistic variations in the Twitter data of Donald Trump between 2009 and 2018 (\protect\hyperlink{ref-Clarke2019a}{Clarke and Grieve 2019}). Here, the authors aimed to figure out whether the way Trump's tweets were written fluctuated over time. To do so, they downloaded 21,739 tweets and grouped them into 63 categories over 4 dimensions based on their content. Given that all the data used in the article is available for inspection, we can attempt to replicate part of the analysis here.

First, we load the packages we need for the Correspondence Analysis:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(FactoMineR)}
\FunctionTok{library}\NormalTok{(factoextra)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\end{Highlighting}
\end{Shaded}

Then, we import the data. You can do so either by downloading the replication data yourselves, or use the file we already put up on GitHub:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{urlfile }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/SCJBruinsma/qta{-}files/master/TRUMP\_DATA.txt"}
\NormalTok{tweets }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\FunctionTok{url}\NormalTok{(urlfile))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   .default = col_character(),
##   TWEETID = col_double(),
##   WORDCOUNT = col_double(),
##   DATE = col_date(format = ""),
##   TIME = col_time(format = ""),
##   RETWEET = col_double(),
##   FAV = col_double()
## )
## i Use `spec()` for the full column specifications.
\end{verbatim}

This data-set contains quite some information we do not need. To begin with, we remove all those variables that do not contain information about the 63 categories and the length of the tweet in words. Also, for clarity's sake, we sample 200 of the tweets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets }\OtherTok{\textless{}{-}}\NormalTok{ tweets[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(tweets), }\DecValTok{200}\NormalTok{), ]}
\NormalTok{tweets\_mat }\OtherTok{\textless{}{-}}\NormalTok{ tweets[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{65}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

We can then run the MCA with the \texttt{FactoMineR} package. For this, we have to give the data-set and the number of dimensions we think are in the data. We can set the latter either by establishing the dimensions as in a regular PCA (for example through a scree plot) or based on theory. Here we combine both and use the 5 dimensions established in the article. In addition, we set a supplementary quantitative variable as \texttt{quanti.sup=1}. As this is a quantitative variable, it is not taken into consideration by the MCA, but does allow us to assess later on how it correlates with each of the five dimensions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mca\_tweets }\OtherTok{\textless{}{-}} \FunctionTok{MCA}\NormalTok{(tweets\_mat, }\AttributeTok{ncp=}\DecValTok{5}\NormalTok{, }\AttributeTok{quanti.sup=}\DecValTok{1}\NormalTok{, }\AttributeTok{graph =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First, let's start by looking at the association of the wordlength with the five dimensions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mca\_tweets}\SpecialCharTok{$}\NormalTok{quanti.sup}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $coord
##               Dim 1      Dim 2      Dim 3      Dim 4       Dim 5
## WORDCOUNT 0.8668104 -0.2107786 0.02090331 0.03345767 -0.05383935
\end{verbatim}

As we can see, the word length has a strong correlation with Dimension 1. This basically means that this dimension captures the length of the words and not a seperate dimension we are interested in. Thus, when we want to look at the correspondence between the categories and the dimensions, we can ignore this dimension. Thus, for the MCA, we will look at dimensions 2 and 3:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_mca\_var}\NormalTok{(mca\_tweets,}
             \AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{,}
             \AttributeTok{geom =} \FunctionTok{c}\NormalTok{(}\StringTok{"point"}\NormalTok{),}
             \AttributeTok{axes =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),             }
             \AttributeTok{ggtheme =} \FunctionTok{theme\_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-114-1.pdf}

Here, we only plot the points as adding the labels as well will make the picture quite cluttered. In the article, Dimension 2 is identified as ``Conversational Style'' and Dimension 3 as ``Campaigning Style.'' The plot thus nicely shows us that some categories belong to one of these dimensions and not to the other. To see for which cases this is mostly the case (the ones that have the most extreme positions), we can have a look at their coordinates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var }\OtherTok{\textless{}{-}} \FunctionTok{get\_mca\_var}\NormalTok{(mca\_tweets)}
\NormalTok{coordinates }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(var}\SpecialCharTok{$}\NormalTok{coord)}
\NormalTok{coordinates }\OtherTok{\textless{}{-}}\NormalTok{ coordinates[}\FunctionTok{order}\NormalTok{(coordinates}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Dim 2}\StringTok{\textasciigrave{}}\NormalTok{),]}
\FunctionTok{head}\NormalTok{(coordinates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     Dim 1      Dim 2      Dim 3        Dim 4       Dim 5
## POSESPRPN_P    0.03383985 -0.9689511  0.5011988  0.036488377  0.36083073
## PROGRESSIVE_P  0.79201463 -0.9059134  0.7499092 -0.674316668  0.46134193
## SUPERLATIVE_P  0.37755724 -0.8728057  0.6594311 -0.147514294 -0.41714831
## GERUND_P       0.20213101 -0.8295159  0.4209137  0.089642937  0.82377111
## MULTIWVB_P     0.42162442 -0.7694130 -0.1558600 -0.003865414  0.37457619
## COLON_P       -0.24915096 -0.7493897 -0.3866968  1.023830211 -0.07073923
\end{verbatim}

Here, remember to look only at the results from the second column onward. Here, we see that one extreme category for the second dimension (Conversational Style) was the use of a colon (:) or possessive proper nouns (such as Hillary's). This seems to fit well with the idea of conversational style. We can also see that the latter one also corresponds quite well with Dimension 3 (Campaigning Style), while the first one does not.

\hypertarget{supervised-methods}{%
\chapter{Supervised Methods}\label{supervised-methods}}

While with scaling we try to place our texts on a scale, with supervised methods we go back to what we did with dictionary analysis: classification. Within \texttt{quanteda} there are many different models for supervised methods, of which we will cover two. These are Support Vector Machines (SVM) and Naive Bayes (NB). The first classifies texts by looking at their position on a hyperplane, the second by their (Bayesian) probabilities.

\hypertarget{support-vector-machines}{%
\section{Support Vector Machines}\label{support-vector-machines}}

To show how SVM works, we will look at an example of SVM in \texttt{quanteda} and one in \texttt{RTextTools}, and an example of NB in \texttt{quanteda}.

\hypertarget{svm-with-rtexttools}{%
\subsection{SVM with RTextTools}\label{svm-with-rtexttools}}

For the SVM, we will start with an example using our Twitter data and the \texttt{RTextTools} package. First, we load the Twitter data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"RTextTools"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Caricamento del pacchetto richiesto: SparseM
\end{verbatim}

\begin{verbatim}
## 
## Caricamento pacchetto: 'SparseM'
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto  mascherato da 'package:base':
## 
##     backsolve
\end{verbatim}

\begin{verbatim}
## Registered S3 method overwritten by 'tree':
##   method     from
##   print.tree cli
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}

\NormalTok{urlfile }\OtherTok{=} \StringTok{"https://raw.githubusercontent.com/SCJBruinsma/qta{-}files/master/Tweets.csv"}
\NormalTok{tweets }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\FunctionTok{url}\NormalTok{(urlfile))}
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"http.*"}\NormalTok{,}\StringTok{""}\NormalTok{,  tweets}\SpecialCharTok{$}\NormalTok{text)}
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"https.*"}\NormalTok{,}\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text)}
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"@}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"[[:punct:]]"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"[ |}\SpecialCharTok{\textbackslash{}t}\StringTok{]\{2,\}"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"\^{} "}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" $"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"RT"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text) }
\NormalTok{tweets}\SpecialCharTok{$}\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"href"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets}\SpecialCharTok{$}\NormalTok{text)}

\NormalTok{labels }\OtherTok{\textless{}{-}}\NormalTok{ tweets}\SpecialCharTok{$}\NormalTok{airline\_sentiment}
\NormalTok{labels }\OtherTok{\textless{}{-}}\NormalTok{ car}\SpecialCharTok{::}\FunctionTok{recode}\NormalTok{(labels, }\StringTok{"\textquotesingle{}positive\textquotesingle{}=1;\textquotesingle{}negative\textquotesingle{}={-}1;\textquotesingle{}neutral\textquotesingle{}=0"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The goal of the supervised learning task is to use part of this dataset to train a certain algorithm, and then use the trained algorithm to assign categories to the remaining sentences. Since we know the coded categories for the remaining sentences, we will be able to evaluate how well this training was in guessing/estimating what the codes for these sentences were. We start by creating a document term matrix;

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{create\_matrix}\NormalTok{(tweets}\SpecialCharTok{$}\NormalTok{text, }\AttributeTok{language =} \StringTok{"english"}\NormalTok{, }\AttributeTok{removeNumbers =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{stemWords =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{removeSparseTerms =} \FloatTok{0.998}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are
## ignored
\end{verbatim}

\begin{verbatim}
## Warning in TermDocumentMatrix.SimpleCorpus(x, control): custom tokenizer is
## ignored
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## <<DocumentTermMatrix (documents: 14640, terms: 694)>>
## Non-/sparse entries: 84547/10075613
## Sparsity           : 99%
## Maximal term length: 18
## Weighting          : term frequency (tf)
\end{verbatim}

Note that \texttt{RTextTools} gives you plenty of options in preprocessing. Apart from the options used above, you can also strip whitespace, remove punctuation, and remove stopwords from lists that are already defined in the package. Stemming and stopword removal is language specific, so when you select the language in the option as above \texttt{(language=\textquotesingle{}\textquotesingle{}english\textquotesingle{}\textquotesingle{})}, the stemming and stopword removal will be done according to the language of your choice. At the moment, the stopwords included are those for Danish, Dutch, English, Finnish, French, German, Italian, Norwegian, Portuguese, Russian, Spanish, and Swedish.

We then create a container parsing the document matrix into a training set, and a test set. The training set will be used to train the algorithm and the test set to test how well this algorithm was trained. The following command instructs R to use the first 4000 sentences for the training set the remaining 449 sentences for the test set. Moreover, we specify to append to the document matrix the variable that contains the assigned coders:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{container }\OtherTok{\textless{}{-}} \FunctionTok{create\_container}\NormalTok{(doc\_matrix, labels, }\AttributeTok{trainSize =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10000}\NormalTok{, }\AttributeTok{testSize =} \DecValTok{10001}\SpecialCharTok{:}\DecValTok{14640}\NormalTok{, }\AttributeTok{virgin =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can then train a model using one of the available algorithms. For instance, we can use the Support Vector Machines algorithm (SVM) as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SVM }\OtherTok{\textless{}{-}} \FunctionTok{train\_model}\NormalTok{(container, }\StringTok{"SVM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Other algorithms available are glmnet (GLMNET), maximum entropy (MAXENT), scaled linear discriminant analysis (SLDA), bagging (BAGGING), boosting (BOOSTING), random forest (RF), neural networks (NNET), classification tree (TREE).

We then use the model we just trained to classify the texts in the test set. The following command instructs R to classify the documents in the test set of the container using the SVM model that we previously trained.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SVM\_CLASSIFY }\OtherTok{\textless{}{-}} \FunctionTok{classify\_model}\NormalTok{(container, SVM)}
\end{Highlighting}
\end{Shaded}

We can also view the classification that was performed by the SVM model as follows. The first columns corresponds to the label that was assigned to each of the tweets in the training set, while the second column gives the probability that the sentence was assigned to that particular category by the SVM algorithm. As you can see, while the probability for some sentences is quite highfor others is quite low even though the classification always chooses the category with the highest probability.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(SVM\_CLASSIFY)}
\end{Highlighting}
\end{Shaded}

The next step is to check the performance of the model we just tested in terms of classification. To do this, we first request a function which returns a container with different summaries. For instance, we can request summaries on the basis of the labels that were attached to the sentences, the documents (or in this case, the sentences) by label, or on the basis of the algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analytics }\OtherTok{\textless{}{-}} \FunctionTok{create\_analytics}\NormalTok{(container, SVM\_CLASSIFY)}
\FunctionTok{summary}\NormalTok{(analytics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ENSEMBLE SUMMARY
## 
##        n-ENSEMBLE COVERAGE n-ENSEMBLE RECALL
## n >= 1                   1               0.8
## 
## 
## ALGORITHM PERFORMANCE
## 
## SVM_PRECISION    SVM_RECALL    SVM_FSCORE 
##     0.6800000     0.6733333     0.6733333
\end{verbatim}

Precision gives the proportion of bills that were classified as belonging to a category and actually belong to this category (true positives) to all the bills that were classified in that category (irrespective of where they belong). Recall is the proportion of bills that were classified as belonging to a category and actually belong to this category (true positives) to all the bills that belong to this category (true positives plus false negatives). The F score is a weighted average between precision and recall ranging fro 0 to 1.

Finally, we can compare the scores between the labels given by the coders and those based on our SVM:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compare }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(labels[}\DecValTok{10001}\SpecialCharTok{:}\DecValTok{14640}\NormalTok{], SVM\_CLASSIFY}\SpecialCharTok{$}\NormalTok{SVM\_LABEL))}
\FunctionTok{table}\NormalTok{(compare)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     V2
## V1     -1    0    1
##   -1 3013  296  110
##   0   289  347   55
##   1   131   59  340
\end{verbatim}

\hypertarget{svm-with-quanteda}{%
\subsection{SVM with Quanteda}\label{svm-with-quanteda}}

Instead of using a separate package, we can also use \texttt{quanteda} to carry out an SVM. For this, we load some movie reviews, select 1000 of them at random, and place them into our corpus:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\FunctionTok{library}\NormalTok{(quanteda)}
\FunctionTok{library}\NormalTok{(quanteda.classifiers)}
\NormalTok{corpus\_reviews }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_sample}\NormalTok{(data\_corpus\_LMRD, }\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our aim here will be to see how well the SVM algorithm can predict the rating of the reviews. To do this, we first have to create a new variable \texttt{prediction}. This variable contains the same scores as the original rating. Then, we remove 30\% of the scores and replace them with NA. We do so by creating a \texttt{missing} variable what contains 30\% 0s and 70\% 1s. We then place the 0s with NAs. These NA scores are then the ones we want the algorithm to predict. Finally, we add the new variable to the corpus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction }\OtherTok{\textless{}{-}}\NormalTok{ corpus\_reviews}\SpecialCharTok{$}\NormalTok{rating}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{missing }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.7}\NormalTok{)}
\NormalTok{prediction[missing }\SpecialCharTok{==} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\FunctionTok{docvars}\NormalTok{(corpus\_reviews, }\StringTok{"prediction"}\NormalTok{) }\OtherTok{\textless{}{-}}\NormalTok{ prediction}
\end{Highlighting}
\end{Shaded}

We then transform the corpus into a data frame, and also remove stopwords, numbers and punctuation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfm\_reviews }\OtherTok{\textless{}{-}} \FunctionTok{dfm}\NormalTok{(corpus\_reviews, }\AttributeTok{remove =} \FunctionTok{stopwords}\NormalTok{(}\StringTok{"english"}\NormalTok{), }\AttributeTok{remove\_punct =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{remove\_numbers =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 'dfm.corpus()' is deprecated. Use 'tokens()' first.
\end{verbatim}

\begin{verbatim}
## Warning: '...' should not be used for tokens() arguments; use 'tokens()' first.
\end{verbatim}

\begin{verbatim}
## Warning: 'remove' is deprecated; use dfm_remove() instead
\end{verbatim}

Now we can run the SVM algorithm. To do so, we tell the model on which dfm we want to run our model, and which variable contains the scores to train the algorithm. Here, this is our \texttt{prediction} variable with the missing data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.textmodels)}
\NormalTok{svm\_reviews }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_svm}\NormalTok{(dfm\_reviews, }\AttributeTok{y =} \FunctionTok{docvars}\NormalTok{(dfm\_reviews, }\StringTok{"prediction"}\NormalTok{))}
\NormalTok{svm\_reviews}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## textmodel_svm.dfm(x = dfm_reviews, y = docvars(dfm_reviews, "prediction"))
## 
## 707 training documents; 129,216 fitted features.
## Method: L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)
\end{verbatim}

Here we see that the algorithm used 720 texts to train the model (the one with a score) and fitted 133,728 features. The latter refers to the total number of words in the training texts and not only the unique ones. Now we can use this model to predict the ratings we removed earlier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_predict }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(svm\_reviews)}
\end{Highlighting}
\end{Shaded}

While we can of course look at the resulting numbers, we can also place them in a two-way table with the actual rating, to see how well the algorithm did:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rating     }\OtherTok{\textless{}{-}}\NormalTok{ corpus\_reviews}\SpecialCharTok{$}\NormalTok{rating}
\NormalTok{table\_data }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(svm\_predict, rating))}
\FunctionTok{table}\NormalTok{(table\_data}\SpecialCharTok{$}\NormalTok{svm\_predict,table\_data}\SpecialCharTok{$}\NormalTok{rating)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##        1   2   3   4   7   8   9  10
##   1  175  14  10  11   4   3   1   6
##   2   13  65   5   3   0   0   0   3
##   3    5   2  82   4   1   4   0   3
##   4    4   5   5  90   1   5   2   6
##   7    0   1   2   2  75   6   0   1
##   8    2   1   1   0   3  83   5   7
##   9    3   0   1   6   4  11  74   7
##   10   1   3   3   2   3  10  14 137
\end{verbatim}

Here, the table shows the prediction of the algorithm from top to bottom and the original rating from left to right. What we want is that all cases are on the diagonal: in that case, the prediction is the same as the original rating. Here, this happens in the majority of cases. Also, only in a few cases is the algorithm far off.

\hypertarget{naive-bayes}{%
\section{Naive Bayes}\label{naive-bayes}}

For the NB example, we will use data from the Manifesto Project (\protect\hyperlink{ref-Volkens2019a}{Volkens et al. 2019}), also known as the Comparative Manifesto Project (CMP), Manifesto Research Group (MRG), and MARPOR (Manifesto Research on Political Representation)). After you have signed up and downloaded the API key, load the package and set the key:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(manifestoR)}
\FunctionTok{mp\_setapikey}\NormalTok{(}\StringTok{"manifesto\_apikey.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

While we can download the whole dataset, as it is rather large, it makes more sense to only download download a part of it. Here, we take the manifestos for the United Kingdom in 2015. To tell R we want only these documents, we make a small dataframe listing the party and the year we want, and then place this into the \texttt{mp\_corpus} command. Note that instead of the names of the parties, the Manifesto Project assigns unique codes to each party. To see which code belongs to which party, see: \url{https://manifesto-project.wzb.eu/down/data/2019a/codebooks/parties_MPDataset_MPDS2019a.pdf}. Also note that the date includes both the year and month of the election:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manifestos }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{party=}\FunctionTok{c}\NormalTok{(}\DecValTok{51320}\NormalTok{, }\DecValTok{51620}\NormalTok{, }\DecValTok{51110}\NormalTok{, }\DecValTok{51421}\NormalTok{, }\DecValTok{51901}\NormalTok{, }\DecValTok{51902}\NormalTok{, }\DecValTok{51951}\NormalTok{), }\AttributeTok{date=}\FunctionTok{c}\NormalTok{(}\DecValTok{201505}\NormalTok{, }\DecValTok{201505}\NormalTok{, }\DecValTok{201505}\NormalTok{, }\DecValTok{201505}\NormalTok{, }\DecValTok{201505}\NormalTok{, }\DecValTok{201505}\NormalTok{, }\DecValTok{201505}\NormalTok{))}
\NormalTok{manifesto\_corpus }\OtherTok{\textless{}{-}} \FunctionTok{mp\_corpus}\NormalTok{(manifestos)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Connecting to Manifesto Project DB API... corpus version: 2020-2 
## Connecting to Manifesto Project DB API... corpus version: 2020-2
\end{verbatim}

For now, we are only interested in the (quasi)-sentences the of the manifestos, the codes the coders gave them, and names of the parties. To make everything more clear, we will take these elements from the corpus, combine them into a new data-frame, and remove all the NA values. We do this because otherwise the data would also include the headers and titles of the document, which do not have any codes assigned to them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_51320 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}
\NormalTok{text\_51620 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{2}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}
\NormalTok{text\_51110 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{3}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}
\NormalTok{text\_51421 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{4}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}
\NormalTok{text\_51901 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{5}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}
\NormalTok{text\_51902 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{6}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}
\NormalTok{text\_51951 }\OtherTok{\textless{}{-}}\NormalTok{ manifesto\_corpus}\SpecialCharTok{$}\NormalTok{content[[}\DecValTok{7}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{content}\SpecialCharTok{$}\NormalTok{text}

\NormalTok{texts }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(text\_51320,text\_51620,text\_51110,text\_51421,text\_51901,text\_51902,text\_51951)}

\NormalTok{party\_51320 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51320}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51320))}
\NormalTok{party\_51620 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51620}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51620))}
\NormalTok{party\_51110 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51110}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51110))}
\NormalTok{party\_51421 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51421}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51421))}
\NormalTok{party\_51901 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51901}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51901))}
\NormalTok{party\_51902 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51902}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51902))}
\NormalTok{party\_51951 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{51951}\NormalTok{,}\AttributeTok{length.out=}\FunctionTok{length}\NormalTok{(text\_51951))}

\NormalTok{party }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(party\_51320,party\_51620,party\_51110,party\_51421,party\_51901,party\_51902,party\_51951)}

\NormalTok{cmp\_code }\OtherTok{\textless{}{-}} \FunctionTok{codes}\NormalTok{(manifesto\_corpus)}

\NormalTok{manifesto\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(texts,cmp\_code,party)}
\end{Highlighting}
\end{Shaded}

To get an idea of how much a party ``owns'' a code, we can calculate the row percentages. These inform us how much of the appearance of a certain code is due to a single party. To calculate these, we use the \texttt{prop.table} command. Here, the \texttt{,1} at the end tells R to look at the rows (no value would give the cell proportions, and 2 would give the column proportions). We then multiply the proportions by 100 to get the percentages. Then, we place the output in a data-frame, and provide some names to the columns using the \texttt{names} command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_row }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{((}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(manifesto\_data}\SpecialCharTok{$}\NormalTok{cmp\_code,}
\NormalTok{    manifesto\_data}\SpecialCharTok{$}\NormalTok{party), }\DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{))}
\FunctionTok{names}\NormalTok{(prop\_row) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Code"}\NormalTok{, }\StringTok{"Party"}\NormalTok{, }\StringTok{"Percentage"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

While we can look at the results by looking at the \texttt{prop\_row} object, it is clearer to do this in a graph. To build this graph, in the command we first specify the data, the x variable (the codes), the y variable (the percentages), and the filling of the bar (which should be the party colours). These party colours we provide in the next line (in hexadecimal notation). Then we tell \texttt{ggplot} to draw the bar chart and \emph{stack} the bars on top of each other (the alternative is to \emph{dodge}, in which R places the bars next to each other). Then, we specify our theme, turn the text for the codes 90 degrees, and move the codes a little bit so they are under their respective bars:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ prop\_row, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Code, }\AttributeTok{y =}\NormalTok{ Percentage, }\AttributeTok{fill =}\NormalTok{ Party)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_manual}\NormalTok{(}\StringTok{""}\NormalTok{, }\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#0087DC"}\NormalTok{, }\StringTok{"\#67B437"}\NormalTok{, }\StringTok{"\#DC241F"}\NormalTok{,}
        \StringTok{"\#FAA61A"}\NormalTok{, }\StringTok{"\#008142"}\NormalTok{, }\StringTok{"\#FDF38E"}\NormalTok{, }\StringTok{"\#780077"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{,}
    \AttributeTok{position =} \StringTok{"stack"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-135-1.pdf}

Now, we can see that some parties dominate some categories, while for others the spread is more even. For example, UKIP dominates the categories 406 and 407 - dealing with positive and negative mentions of protectionism, while the Conservatives do the same with category 103 (\emph{Anti-Imperialism}). Note though, that these are percentages. This means that the reason the Conservatives dominate category 103 is as they have two (quasi)-sentences with that category. The others do not have the category at all (702 on \emph{Negative Mentioning of Labour Groups} has the same issue). Other categories, such as 403 (\emph{Market Regulation}) and 502 (\emph{Positive Mentions of Culture}) are way better spread out over all the parties.

Another thing we can look at is what part of a party's manifesto belongs to any of the codes. This can help us answer the question: ``what are the parties talking about?'' To see this, we have to calculate the column percentages:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_col }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{((}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(manifesto\_data}\SpecialCharTok{$}\NormalTok{cmp\_code,}
\NormalTok{    manifesto\_data}\SpecialCharTok{$}\NormalTok{party), }\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{))}
\FunctionTok{names}\NormalTok{(prop\_col) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Code"}\NormalTok{, }\StringTok{"Party"}\NormalTok{, }\StringTok{"Percentage"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we now type \texttt{prop\_col}, we can see what percentage of a party manifesto was about a certain code. Yet, given that there are 57 possible codes, it is more practical to cluster these in some way. Here, we do this using the Domains to which they belonged in the codebook. In total there are 7 domains (\url{https://manifesto-project.wzb.eu/down/papers/handbook_2014_version_5.pdf}), and a category which houses the 0 code. To cluster the codes, we make a new variable called \texttt{Domain}. To do so, we first transform the codes into numeric format, create an empty variable called \texttt{Domain}, and then replace the NA values in this empty category with the name of the domain based on the values in the Code variable. This we do using various operators R uses: \texttt{\textgreater{}=} means greater than and equal to, while \texttt{\textless{}=} means smaller than and equal to. Then, we make this new variable into a factor, and sort this factor in the way the codes occur:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Code }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(prop\_col}\SpecialCharTok{$}\NormalTok{Code))}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{101} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{110}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"External Relations"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{201} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{204}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Freedom and Democracy"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{301} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{305}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Political System"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{401} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{416}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Economy"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{501} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{507}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Welfare and Quality of Life"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{601} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{608}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Fabric of Society"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textgreater{}=} \DecValTok{701} \SpecialCharTok{\&}\NormalTok{ prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{\textless{}=} \DecValTok{706}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Social Groups"}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain[prop\_col}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{==} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"NA"}

\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(prop\_col}\SpecialCharTok{$}\NormalTok{Domain)}
\NormalTok{prop\_col}\SpecialCharTok{$}\NormalTok{Domain }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(prop\_col}\SpecialCharTok{$}\NormalTok{Domain, }\FunctionTok{levels}\NormalTok{(prop\_col}\SpecialCharTok{$}\NormalTok{Domain)[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}
    \DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

We then construct a plot as we did above:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ prop\_col, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Party, }\AttributeTok{y =}\NormalTok{ Percentage, }\AttributeTok{fill =}\NormalTok{ Domain)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"stack"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}
    \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-138-1.pdf}

Here, we see that the Domain of \emph{Welfare and Quality of Life} was the most dominant in all the manifestos, with \emph{Economy} coming second. Also, especially UKIP paid a lot of attention to \emph{External Relations}, while the Green party paid little attention to the \emph{Fabric of Society}. In all, this gives us a good idea of what type of data we are actually dealing with.

Now let's get back to the classification. For this, we need to transform the corpus from the \texttt{manifestoR} package into a corpus for the \texttt{quanteda} package. To do so, we first have to transform the former into a data frame, and then turn it into a corpus. We then look at the first 10 entries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corpus\_data }\OtherTok{\textless{}{-}} \FunctionTok{mp\_corpus}\NormalTok{(manifestos) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{(}\AttributeTok{with.meta=}\ConstantTok{TRUE}\NormalTok{)}

\NormalTok{manifesto\_corpus }\OtherTok{\textless{}{-}} \FunctionTok{corpus}\NormalTok{(corpus\_data)}

\FunctionTok{summary}\NormalTok{(manifesto\_corpus, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, we see that the corpus treats each sentence as a separate document (which is confusing). We can still identify to which party they belong due to the \emph{party} variable, which shows the party code. The \emph{cmp\_code} variable shows the code assigned to the sentence (here it is all NA as the first sentences have the 0 category). To run the NB, instead of providing our training documents using a vector with NA values, we have to split our data-set into a training and a test set. For this, we first generate a string of 8000 random numbers between 0 and 10780 (the total number of sentences). We do so to prevent our training or test set to exist only of sentences from a single party document:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{id\_train }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10780}\NormalTok{, }\DecValTok{8000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(id\_train, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  2369  5273  9290  1252  8826 10289   356  7700  3954 10095
\end{verbatim}

Then we generate a unique number for each of the 10780 sentences in our corpus. This so we can later match them to the sentences we would like to place in our training set or our test set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{docvars}\NormalTok{(manifesto\_corpus, }\StringTok{"id\_numeric"}\NormalTok{) }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ndoc}\NormalTok{(manifesto\_corpus)}
\end{Highlighting}
\end{Shaded}

We should now see this new variable \emph{id\_numeric} appear in our corpus. We can now construct our training and test set using these id's. For the training set, the logic is to create a sub set of the main corpus, and to take only those sentences whose \emph{id\_numeric} is also in \emph{id\_train}. For the test set, we do the same, only now taking only those sentences whose \emph{id\_numeric} is not in \emph{id\_train} (note that the ! mark signifies this). Then, we use the \texttt{\%\textgreater{}\%} pipe to transform the resulting object via a tokens object into a dfm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manifesto\_train }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_subset}\NormalTok{(manifesto\_corpus, id\_numeric }\SpecialCharTok{\%in\%}\NormalTok{ id\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tokens}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm}\NormalTok{()}

\NormalTok{manifesto\_test }\OtherTok{\textless{}{-}} \FunctionTok{corpus\_subset}\NormalTok{(manifesto\_corpus, }\SpecialCharTok{!}\NormalTok{id\_numeric }\SpecialCharTok{\%in\%}\NormalTok{ id\_train) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokens}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{dfm}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We then run the model using the \texttt{textmodel\_nb} command, and ask it to use as classifiers the codes in the \emph{cmp\_code} variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manifesto\_nb }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_nb}\NormalTok{(manifesto\_train, }\FunctionTok{docvars}\NormalTok{(manifesto\_train, }\StringTok{"cmp\_code"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(manifesto\_nb)}
\end{Highlighting}
\end{Shaded}

Notice that the textmodel gives us a prediction of how likely it is that an individual word belongs to a certain code (the estimated feature scores). While this can be interesting, what we want to know here is how good the algorithm was. This is when we move from the training of the model using the training set to the prediction of the test set.

A problem is that Naive Bayes can only use featuers that were both in the training and the test set. To ensure this happens, we use the \texttt{dfm\_match} option, which matches all the features in our dfm to a specified vector of features:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manifesto\_matched }\OtherTok{\textless{}{-}} \FunctionTok{dfm\_match}\NormalTok{(manifesto\_test, }\AttributeTok{features =} \FunctionTok{featnames}\NormalTok{(manifesto\_train))}
\end{Highlighting}
\end{Shaded}

If we look at this new corpus we see that little has changed (there are still 2780 features). This means that all features that were in the test set were also there in the training set. This is good news as this means the algorithm has all the information needed for a good prediction. Yet, the lower the number of sentences, the less likely this is to occur, so matching is always a good idea.

Now we can predict the missing codes in the test set (now the manifesto\_matched dfm) using the model we trained earlier. The resulting classes are what the model predicts (we already set this when we trained the model). If we would then open the \texttt{predicted\_class} object we can see to which code R assigned each sentence. Yet, as before, this is a little too much information. Moreover, we do not want to know what the model assigned the sentence to, but how this corresponds to the original code. To see this, we take the actual classes from the \texttt{manifesto\_matched} dfm and place them with the predicted classes into a cross table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predicted\_class }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(manifesto\_nb, }\AttributeTok{newdata =}\NormalTok{ manifesto\_matched)}
\NormalTok{actual\_class }\OtherTok{\textless{}{-}} \FunctionTok{docvars}\NormalTok{(manifesto\_matched, }\StringTok{"cmp\_code"}\NormalTok{)}
\NormalTok{table\_class }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(actual\_class, predicted\_class)}
\NormalTok{table\_class}
\end{Highlighting}
\end{Shaded}

While this is already better (we have to pay attention to the diagonal), the large number of codes still makes this hard to read. So, as before, we can better visualise these results - here with the help of a heatmap. To do this, we first tranform our table into a dataframe which gives us all the possible combinations of codes and their occurrence. We put this into the command and also use a scaling gradient that gets darker when the value in a cell is higher:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{table\_class }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(table\_class)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ table\_class, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ predicted\_class, }\AttributeTok{y =}\NormalTok{ actual\_class)) }\SpecialCharTok{+}
    \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ Freq)) }\SpecialCharTok{+} \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{high =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{low =} \StringTok{"white"}\NormalTok{, }\AttributeTok{name =} \StringTok{"Value"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Predicted Class"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Actual Class"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{scale\_y\_discrete}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{vjust =} \FloatTok{0.4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-146-1.pdf}

Here, we can see that a high number of cases are on the diagonal, which indicates that the algorithm did a good job. Yet, it also classified a large number of sentences into the 503 and 504 categories, while they belonged to any of the other categories.

Besides this, we can also summarize how good the algorithm is by means of Krippendorff's \(\alpha\). To do so, we take the predicted codes, transform them from factors to numeric values, and store them in an object. Then, we bind them together with the actual codes and place them into a data frame. Finally, we transpose the data frame (so that rows are now columns) and make it into a matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(predicted\_class))}
\NormalTok{reliability }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(actual\_class, predict))}
\NormalTok{reliability\_t }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(reliability)}
\NormalTok{reliability }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(reliability\_t)}
\end{Highlighting}
\end{Shaded}

Then, we load the \texttt{kripp.boot} package, and calculate the nominal version of Krippendorff's \(\alpha\), as we are working with nominal codes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kripp.boot)}
\FunctionTok{kripp.boot}\NormalTok{(reliability, }\AttributeTok{iter =} \DecValTok{500}\NormalTok{, }\AttributeTok{method =} \StringTok{"nominal"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternatively, we can use the \texttt{DescTools} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{KrippAlpha}\NormalTok{(reliability, }\AttributeTok{method =} \StringTok{"nominal"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here we see that the number of subjects was 2780 (the number of sentences in the test set), the number of coders 2 (the actual and the predicted codes), and the value of \(\alpha\) 0.318 with an interval between 0.297 and 0.337. While this might not look particularly encouraging, when we realise that \protect\hyperlink{ref-Mikhaylov2012a}{Mikhaylov, Laver, and Benoit} (\protect\hyperlink{ref-Mikhaylov2012a}{2012}) estimate the agreement among trained coders by the Manifesto Project to be between 0.350 and 0.400, then 0.305 is quite a good score for a simple model!

\hypertarget{unsupervised-methods}{%
\chapter{Unsupervised Methods}\label{unsupervised-methods}}

While supervised models often work fine for text classification, one disadvantage is that we need to set specifics for the model. As an alternative, we can not specify anything and have R find out which classifications work. There are various algorithms to do so, of which we here will focus on Latent Dirichlet Allocation (LDA), a ``seeded'' version of LDA that uses information from other sources to improve the results of the LDA, and finally we will look at a Structural Topic Model.

\hypertarget{latent-dirichlet-allocation}{%
\section{Latent Dirichlet Allocation}\label{latent-dirichlet-allocation}}

Latent Dirichlet Allocation, or LDA, relies on the idea is that each text is in fact a mix of topics, and each word belongs to one these. To run LDA, we will use the \texttt{topicmodels} package, and use the inaugural speeches as an example. First, we will use the \texttt{convert} function to convert the data frequency matrix to a data term matrix as this is what \texttt{topicmodels} uses:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(topicmodels)}
\NormalTok{inaugural\_dtm }\OtherTok{\textless{}{-}} \FunctionTok{convert}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{to =} \StringTok{"topicmodels"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, we fit an LDA model with 10 topics. First, we have to define some a priori parameters for the model. Here, we will use the Gibbs sampling method to fit the LDA model (\protect\hyperlink{ref-Griffiths2004a}{Griffiths and Steyvers 2004}) over the alternative VEM approach (\protect\hyperlink{ref-Blei2003a}{Blei, Ng, and Jordan 2003}). Gibbs sampling performs a random walk over the distribution so we need to set a seed to ensure reproducible results. In this particular example, we set five seeds for five independent runs. We also set a burn-in period of 2000 as the first iterations will not reflect the distribution well, and take the 200th iteration of the following 1000:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{burnin }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{iter }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{thin }\OtherTok{\textless{}{-}} \DecValTok{200}
\NormalTok{seed }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\DecValTok{42}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{158}\NormalTok{, }\DecValTok{2500}\NormalTok{)}
\NormalTok{nstart }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{best }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\end{Highlighting}
\end{Shaded}

The LDA algorithm estimates topic-word probabilities as well as topic-document probabilities that we can extract and visualize. Here, we will start with the topic-word probabilities called \emph{beta}. To do this, we will use the \texttt{tidytext} package which is part of the tidyverse family of packages. Central to the logic of tidyverse packages is that \texttt{tidytext} does not rely on a document term matrix but represents the data in a long format (\protect\hyperlink{ref-Welbers2017}{Welbers, Van Atteveldt, and Benoit 2017, 252}). Although this makes it less memory efficient, such data arrangement lends itself to effective visualisation. The whole logic of these packages is that it works with data which has columns (variables) and rows with single observations. While this is the logic most people know, but it is not always the quickest (and is also not used by \texttt{quanteda}). Yet, it always allows you to look at your data in a way most will understand. First, we run the LDA and have a look at the first 10 terms:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inaugural\_lda10 }\OtherTok{\textless{}{-}} \FunctionTok{LDA}\NormalTok{(inaugural\_dtm, }\AttributeTok{k =} \DecValTok{10}\NormalTok{, }\AttributeTok{method =} \StringTok{"Gibbs"}\NormalTok{,}
    \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{burnin =}\NormalTok{ burnin, }\AttributeTok{iter =}\NormalTok{ iter, }\AttributeTok{thin =}\NormalTok{ thin,}
        \AttributeTok{seed =}\NormalTok{ seed, }\AttributeTok{nstart =}\NormalTok{ nstart, }\AttributeTok{best =}\NormalTok{ best))}

\FunctionTok{terms}\NormalTok{(inaugural\_lda10, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Topic 1   Topic 2   Topic 3     Topic 4     Topic 5   Topic 6        
##  [1,] "peace"   "us"      "business"  "americans" "every"   "never"        
##  [2,] "world"   "new"     "may"       "citizens"  "great"   "must"         
##  [3,] "nations" "people"  "congress"  "freedom"   "nation"  "republic"     
##  [4,] "free"    "america" "policy"    "country"   "men"     "civilization" 
##  [5,] "freedom" "must"    "states"    "president" "life"    "order"        
##  [6,] "can"     "world"   "executive" "never"     "good"    "war"          
##  [7,] "shall"   "can"     "made"      "common"    "part"    "concern"      
##  [8,] "life"    "nation"  "necessary" "courage"   "upon"    "understanding"
##  [9,] "may"     "one"     "trade"     "day"       "action"  "tasks"        
## [10,] "hope"    "time"    "hope"      "across"    "purpose" "production"   
##       Topic 7      Topic 8       Topic 9      Topic 10   
##  [1,] "change"     "united"      "government" "first"    
##  [2,] "generation" "liberty"     "upon"       "need"     
##  [3,] "journey"    "human"       "can"        "love"     
##  [4,] "hands"      "democracy"   "people"     "days"     
##  [5,] "weapons"    "believe"     "country"    "things"   
##  [6,] "forth"      "states"      "progress"   "back"     
##  [7,] "powerful"   "alone"       "must"       "hand"     
##  [8,] "enduring"   "security"    "law"        "friends"  
##  [9,] "greatness"  "millions"    "system"     "unity"    
## [10,] "words"      "opportunity" "political"  "president"
\end{verbatim}

Here, we can see that the first topic is most concerned with words referring to peace and freedom, the second with references to the people, the third with businesses, as so on. While we can interpret our topics this way, a better way might be to visualise the results. For this, we will use the \texttt{tidy} command to prepare the dataset for visualisation. Then, we tell the command to use the information from the \emph{beta} column, which contains the probability of a word occurring in a certain topic:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{inaugural\_lda10\_topics }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(inaugural\_lda10, }\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we would look into the dataset now, we would see that it has 63130 observations with 3 variables. These are the number of the topic, the word (the term) and the \textbf{beta} - the chance that the word occurs in that topic. We now want to visualise only the top ten words for each topic in a bar-plot. Also, we want the graphs of each of these ten topics to appear in a single graph. To make this happen, we first have to select the top ten words for each topic. We do so gain using a pipe (which is the \texttt{\%\textgreater{}\%} command). This pipe transports an output of a command to another one before saving it. So here, we take our data-set and group it by topic using the \texttt{group\_by} command. This command groups the dataset into 10 groups, each for every topic. What this allows us is to calculate things that we otherwise calculate for the whole data-set but here calculate for the groups instead. We then do so and select the top 10 terms (based on their beta value), using \texttt{top\_n}. We then ungroup again (to make R view it as a single data-set), and use the \texttt{arrange} function to ensure the data-set has the topics sorted in an increasing fashion and the beta values in a decreasing fashion. Finally, we save this into a new object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inaugural\_lda10\_topterms }\OtherTok{\textless{}{-}}\NormalTok{ inaugural\_lda10\_topics }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(topic) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, beta) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(topic, }\SpecialCharTok{{-}}\NormalTok{beta)}
\end{Highlighting}
\end{Shaded}

If we now look at the data-set, we see that it is much smaller and has the topics ordered. Yet, before we can plot this we have to ensure that (seen from top to bottom), all the beta for the first topic come first, then for the second topic, and so on. To do so, we use the \texttt{mutate} command, and redefine the term variable so that it is re-ordered based first on the term and then on the beta value. The result is a data frame with first the first topic, then the second topic etc., and with the beta values ordered within each topic. We then make the figure, with the terms on the horizontal axis and the beta values and the vertical axes, and have the bars this generates coloured by topic. Also, we switch off the legend (which we do not need) and use the \texttt{facet\_wrap} command to split up the total graph (which would have 107 bars otherwise - 107 bars and not a 100 because some terms had the same value for beta). We set the options for the scales to be \textbf{free} as it might be that the beta values for some topics are larger or smaller than for the others. Finally, we flip the graphs and make the x-axis the y-axis and vice versa, as this makes the picture more clear:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inaugural\_lda10\_topterms }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term =} \FunctionTok{reorder}\NormalTok{(term, beta)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(term, beta, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(topic))) }\SpecialCharTok{+} \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{topic, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-155-1.pdf}

What is clear here is that looking at only the words in each topic only says so much. In the first topic, the term ``peace'' is more important than anything else, and so is ``us'' in topic number 2. Also, in topic number ten, we see that both ``first'' and ``need'' are equally important.

Another question we can ask is how much of each topic is in each of the documents. Put in another way: do certain documents talk more about certain topics than others? To see this, we first generate a new data frame with this information, known as the \emph{gamma} value for each document:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inaugural\_lda10\_documents }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(inaugural\_lda10, }\AttributeTok{matrix =} \StringTok{"gamma"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then go through similar steps to make the data-set ready for use and prepare the graph. For the graph, the only steps we do different are to force R to label each topic on the axis (as otherwise it will treat it as a continuous variable and come up with useless values such as 7.5), and to give it a different look (using the \texttt{theme\_classic()} command):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inaugural\_lda10\_toptopics }\OtherTok{\textless{}{-}}\NormalTok{ inaugural\_lda10\_documents }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(document) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, gamma) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(topic, }\SpecialCharTok{{-}}\NormalTok{gamma)}

\NormalTok{inaugural\_lda10\_toptopics }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term =} \FunctionTok{reorder}\NormalTok{(topic, gamma)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(topic, gamma, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(topic))) }\SpecialCharTok{+} \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{,}
        \DecValTok{10}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{document, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-157-1.pdf}

Here, we see that in 1929 Hoover talked mostly about topic 9 (focusing on government), Biden in 2021 focused on words like ``us'' and ``people,'' while in 1945 Roosevelt seemed to favour both the people and topics referring to peace. Again, our exact conclusions of course depend on how we interpret the topics.

\hypertarget{seeded-latent-dirichlet-allocation}{%
\section{Seeded Latent Dirichlet Allocation}\label{seeded-latent-dirichlet-allocation}}

An alternative to the above approach is one known as seeded-LDA. This approach uses seed words that can steer the LDA into the right direction. One origin of these seed words can be a dictionary that tells the algorithm which words belong together in various categories. To use it, we will first load the packages and set a seed:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(seededlda)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Caricamento pacchetto: 'seededlda'
\end{verbatim}

\begin{verbatim}
## I seguenti oggetti sono mascherati da 'package:topicmodels':
## 
##     terms, topics
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto  mascherato da 'package:stats':
## 
##     terms
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda.dictionaries)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we need to specify a selection of seed words in a dictionary form. While we can construct a dictionary ourselves, here we choose to use the Laver and Garry dictionary we saw earlier. We then use this dictionary to run our seeded LDA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dict }\OtherTok{\textless{}{-}} \FunctionTok{dictionary}\NormalTok{(data\_dictionary\_LaverGarry)}
\NormalTok{seededmodel }\OtherTok{\textless{}{-}} \FunctionTok{textmodel\_seededlda}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{dictionary=}\NormalTok{dict)}
\FunctionTok{terms}\NormalTok{(seededmodel, }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       CULTURE     ECONOMY       ENVIRONMENT      GROUPS    INSTITUTIONS    
##  [1,] "people"    "work"        "production"     "women"   "president"     
##  [2,] "us"        "economic"    "productive"     "race"    "administration"
##  [3,] "art"       "opportunity" "planet"         "racial"  "continue"      
##  [4,] "music"     "children"    "population"     "woman"   "office"        
##  [5,] "operating" "economy"     "products"       "racism"  "executive"     
##  [6,] "operation" "industrial"  "environment"    "ethnic"  "rule"          
##  [7,] "new"       "trade"       "clean"          "racing"  "voices"        
##  [8,] "america"   "confidence"  "productivity"   "day"     "legislation"   
##  [9,] "nation"    "equal"       "produce"        "body"    "authority"     
## [10,] "let"       "cost"        "product"        "built"   "democratic"    
## [11,] "american"  "poverty"     "cleaner"        "fire"    "modern"        
## [12,] "today"     "care"        "productions"    "task"    "agencies"      
## [13,] "must"      "welfare"     "produced"       "evil"    "rules"         
## [14,] "world"     "education"   "cleanse"        "mind"    "election"      
## [15,] "one"       "commerce"    "productiveness" "spirit"  "sovereignty"   
## [16,] "know"      "age"         "car"            "whether" "reforms"       
## [17,] "americans" "health"      "chemical"       "put"     "voice"         
## [18,] "now"       "private"     "warming"        "speaks"  "elected"       
## [19,] "time"      "equality"    "depletion"      "serve"   "process"       
## [20,] "can"       "jobs"        "republic"       "carried" "reform"        
##       LAW_AND_ORDER   RURAL          URBAN      VALUES        
##  [1,] "force"         "agriculture"  "town"     "history"     
##  [2,] "forces"        "feed"         "towns"    "human"       
##  [3,] "determined"    "agricultural" "story"    "rights"      
##  [4,] "determination" "farm"         "thank"    "principles"  
##  [5,] "conviction"    "farms"        "friends"  "past"        
##  [6,] "court"         "forests"      "young"    "leadership"  
##  [7,] "determine"     "farmers"      "hear"     "humanity"    
##  [8,] "terror"        "villages"     "learned"  "maintain"    
##  [9,] "forced"        "horseback"    "bless"    "preserve"    
## [10,] "courts"        "farmer"       "prayer"   "defend"      
## [11,] "dealing"       "countryside"  "must"     "leaders"     
## [12,] "seize"         "village"      "together" "principle"   
## [13,] "drugs"         "lanes"        "days"     "proud"       
## [14,] "officers"      "landscape"    "lost"     "threat"      
## [15,] "penalties"     "man"          "dreams"   "pride"       
## [16,] "convictions"   "change"       "protect"  "heritage"    
## [17,] "guarding"      "light"        "hearts"   "historic"    
## [18,] "lawless"       "greatness"    "right"    "preserved"   
## [19,] "illegal"       "hands"        "back"     "integrity"   
## [20,] "victims"       "friends"      "personal" "preservation"
\end{verbatim}

Note that using the dictionary has ensured that only the categories of the dictionary are used. We can therefore have a look which topics each inaugural speech and which terms were most likely for each of the topics. Let us start with the topics first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{topics }\OtherTok{\textless{}{-}} \FunctionTok{topics}\NormalTok{(seededmodel)}
\NormalTok{topics\_table }\OtherTok{\textless{}{-}} \FunctionTok{ftable}\NormalTok{(topics)}
\NormalTok{topics\_prop\_table }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(topics\_table))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{topics\_prop\_table, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{topics, }\AttributeTok{y=}\NormalTok{Freq))}\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Topics"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Topic Percentage"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{10}\NormalTok{, }\AttributeTok{angle=}\DecValTok{45}\NormalTok{,}\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-160-1.pdf}

Here, we find that Culture was the most favoured topic, followed by the Economy and Values. Finally, we can then have a look at the most likely terms for each topic, sorted by each of the categories in the dictionary:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{terms }\OtherTok{\textless{}{-}} \FunctionTok{terms}\NormalTok{(seededmodel)}
\NormalTok{terms\_table }\OtherTok{\textless{}{-}} \FunctionTok{ftable}\NormalTok{(terms)}
\NormalTok{terms\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(terms\_table)}
\FunctionTok{head}\NormalTok{(terms\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Var1    Var2      Freq
## 1    A CULTURE    people
## 2    B CULTURE        us
## 3    C CULTURE       art
## 4    D CULTURE     music
## 5    E CULTURE operating
## 6    F CULTURE operation
\end{verbatim}

Here, we find that in the first cluster (denoted as ``A''), the word ``people'' was most likely (from all words that belonged to Culture). Thus, within this cluster, talking about culture often contained references to the people, and we can make similar observations about the other categories.

\hypertarget{structural-topic-model}{%
\section{Structural Topic Model}\label{structural-topic-model}}

Besides LDA, various other methods for unsupervised classification exist, such as hierarchical clustering, k-means, and various other mixed membership models. Each of them have their specific advantages and problems, and it often depends on the goal of the researcher to decide which method to use. One (relatively) new and flexible method is known as the Structural Topic Model or STM. In R, this method is implemented in the \texttt{stm} package (\protect\hyperlink{ref-Roberts2019a}{Roberts, Stewart, and Tingley 2019}).

One of the outstanding features of stm is known as topical prevalence. This refers to the idea that we have the possibility to include any amount of supplemental information in the form of covariates that can help to identify the correct model or help to better understand the topics the model generates (\protect\hyperlink{ref-Roberts2014a}{Roberts et al. 2014}). For example, information on time can be added to study how topics change over the years; actors on how they differ between different authors; and any other possible variable to see how they differ between them. One of the main advantages of STM is that, unlike in LDA, we are not required to set any parameters in advance. In LDA, these parameters - \(\alpha\) (the degree of mixture of topics a document has) and \(\beta\) (the degree of mixture of words that a topic has) - have to be set beforehand based on previous knowledge. Yet, this knowledge is not always present and several iterations might be required before a correct number is settled upon. In STM, we use the metadata to set these parameters.

\begin{figure}
\includegraphics[width=0.9\linewidth]{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-162-1} \caption{Plate diagram for a Structucal Topic Model.}\label{fig:unnamed-chunk-162}
\end{figure}

In this plate diagram, \(X\) refers to the prevalence metadata; \(\gamma\), the metadata weights; \(\Sigma\), the topic covariances; \(\theta\), the document prevalence; \(z\), the per-word topic; \(w\), the observed word; \(Y\), the content metadata; \(\beta\), the topic content; \(N\), the number of words in a document; and \(M\), the number of documents in the corpus. `So, to begin with, we load the package, set a seed, convert our dfm to the stm format and place our documents, vocabulary (the tokens) and any other data in three separate objects (for later convenience):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## stm v1.3.6 successfully loaded. See ?stm for help. 
##  Papers, resources, and other materials at structuraltopicmodel.com
\end{verbatim}

\begin{verbatim}
## 
## Caricamento pacchetto: 'stm'
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto  mascherato da 'package:lattice':
## 
##     cloud
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(quanteda)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{data\_inaugural\_stm   }\OtherTok{\textless{}{-}} \FunctionTok{convert}\NormalTok{(data\_inaugural\_dfm, }\AttributeTok{to =} \StringTok{"stm"}\NormalTok{)}

\NormalTok{documents  }\OtherTok{\textless{}{-}}\NormalTok{ data\_inaugural\_stm}\SpecialCharTok{$}\NormalTok{documents}
\NormalTok{vocabulary }\OtherTok{\textless{}{-}}\NormalTok{ data\_inaugural\_stm}\SpecialCharTok{$}\NormalTok{vocab}
\NormalTok{meta       }\OtherTok{\textless{}{-}}\NormalTok{ data\_inaugural\_stm}\SpecialCharTok{$}\NormalTok{meta}
\end{Highlighting}
\end{Shaded}

The first thing we have to do is find the number of topics we need. In the \texttt{stm} package, we can do this by using a function called \texttt{searchK}. Here, we specify a range of values we think could include the ``correct'' number of topics, which are then run and collected. Subsequently, we can have a look at multiple goodness-of-fit measures to assess which number of topics (which k) has the best fit for the data. These measures include the exclusivity, semantic coherence, held-out likelihood, bound, lbound, and residual dispersion. Here, we run this for 2 to 15 possible topics.

In our code, we specify our documents, our tokens (the vocabulary), and our meta-data. Moreover, as our prevalence, we include parameters for Year and Party, as we expect the content of the topics to differ between both the Republican and Democratic party, as well as over time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\NormalTok{findingk }\OtherTok{\textless{}{-}} \FunctionTok{searchK}\NormalTok{(documents, vocabulary, k, }\AttributeTok{prevalence =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Party }\SpecialCharTok{+}
    \FunctionTok{s}\NormalTok{(Year), }\AttributeTok{data =}\NormalTok{ meta, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{findingk\_results }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(findingk}\SpecialCharTok{$}\NormalTok{results),}
    \AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(findingk}\SpecialCharTok{$}\NormalTok{results[}\DecValTok{1}\NormalTok{]))))}
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(findingk}\SpecialCharTok{$}\NormalTok{results)}
\FunctionTok{names}\NormalTok{(findingk\_results) }\OtherTok{\textless{}{-}}\NormalTok{ names}
\end{Highlighting}
\end{Shaded}

Looking at \texttt{findingk\_results} we find various values. The first, exclusivity, refers to the occurrence when words have a high probability under one topic, they have a low probability under others. Related to this is semantic coherence which happens when the most probable words in a topic should occur in the same document. Held-out (or held-out log-likelihood) is the likelihood of our model on data that was not used in the initial estimation (the lower the better), residuals refers to the difference between a data point and the mean value that the model predicts for that data point (which we want to be 1, indicating a standard distribution). Finally, bound and lbound refer to a model's internal measure of fit. Here, we will be looking for the number of topics, that balance the exclusivity and the semantic coherence, have a residual around 1, and a low held-out. To make this simpler, we visualise our data. In the first graph we plot all the values, while in the second, we only look at the exclusivity and the semantic coherence (as they are the most important):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reshape2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Caricamento pacchetto: 'reshape2'
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto  mascherato da 'package:tidyr':
## 
##     smiths
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{findingk\_melt          }\OtherTok{\textless{}{-}} \FunctionTok{melt}\NormalTok{(findingk\_results, }\AttributeTok{id=}\StringTok{"K"}\NormalTok{) }
\NormalTok{findingk\_melt}\SpecialCharTok{$}\NormalTok{variable }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(findingk\_melt}\SpecialCharTok{$}\NormalTok{variable)}
\NormalTok{findingk}\SpecialCharTok{$}\NormalTok{K       }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(findingk\_results}\SpecialCharTok{$}\NormalTok{K)}

\FunctionTok{ggplot}\NormalTok{(findingk\_melt, }\FunctionTok{aes}\NormalTok{(K, value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ variable, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-165-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(findingk\_results, }\FunctionTok{aes}\NormalTok{(semcoh, exclus)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_label}\NormalTok{(}\AttributeTok{data=}\NormalTok{findingk\_results, }\AttributeTok{label=}\NormalTok{findingk}\SpecialCharTok{$}\NormalTok{K)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-165-2.pdf}

Based on these graphs, we decide upon 8 topics. The main reason for this is that for this number of topics, there is a relatively high semantic coherence given the exclusivity. We can now run our stm model, using spectral initialization and a topical prevalence including both the Party and the Year of the inauguration. Also, we have a look at the topics, and the words with the highest probability attached to them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_topics }\OtherTok{\textless{}{-}} \DecValTok{8}
\NormalTok{output\_stm }\OtherTok{\textless{}{-}} \FunctionTok{stm}\NormalTok{(documents, vocabulary, }\AttributeTok{K =}\NormalTok{ n\_topics, }\AttributeTok{prevalence =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Party }\SpecialCharTok{+}
    \FunctionTok{s}\NormalTok{(Year), }\AttributeTok{data =}\NormalTok{ meta, }\AttributeTok{init.type =} \StringTok{"Spectral"}\NormalTok{, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{labelTopics}\NormalTok{(output\_stm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Topic 1 Top Words:
##       Highest Prob: free, peace, world, shall, freedom, must, faith 
##       FREX: strive, free, peoples, everywhere, truth, man's, learned 
##       Lift: abhorring, absorbing, abstractions, acquire, aggressor, amass, andes 
##       Score: anguished, productivity, strive, trial, learned, europe, defines 
## Topic 2 Top Words:
##       Highest Prob: us, new, world, let, can, people, america 
##       FREX: let, century, together, new, weapons, voices, abroad 
##       Lift: 200th, 20th, dawn, explore, micah, moon, music 
##       Score: attempting, nuclear, let, celebrate, voices, abroad, dawn 
## Topic 3 Top Words:
##       Highest Prob: us, must, world, government, people, america, can 
##       FREX: civilization, republic, experiment, normal, relationship, order, industrial 
##       Lift: abnormal, acclaim, accompanied, accord, accumulation, acknowledgment, addressing 
##       Score: accompanied, supreme, regards, deliberate, inspiration, unshaken, righteousness 
## Topic 4 Top Words:
##       Highest Prob: us, america, nation, can, must, new, people 
##       FREX: story, thank, president, defend, everyone, children, america 
##       Lift: blowing, breeze, democracy's, january, obama, other's, page 
##       Score: allowing, story, breeze, talk, crucial, everyone, virus 
## Topic 5 Top Words:
##       Highest Prob: freedom, nation, people, america, government, know, democracy 
##       FREX: speaks, mind, democracy, liberty, seen, came, millions 
##       Lift: abreast, absence, admiration, agent, amount, aspires, attempts 
##       Score: charta, speaks, paint, disaster, mind, defended, seen 
## Topic 6 Top Words:
##       Highest Prob: us, must, nation, people, can, new, every 
##       FREX: generation, journey, union, change, covenant, creed, enduring 
##       Lift: demanded, mastery, span, storms, absolutism, abundantly, afghanistan 
##       Score: abundantly, covenant, journey, mastery, storms, demanded, span 
## Topic 7 Top Words:
##       Highest Prob: can, world, people, peace, nations, must, government 
##       FREX: settlement, enforcement, countries, desire, party, international, property 
##       Lift: aided, eighteenth, abilities, abound, abounding, absurd, acceptance 
##       Score: abound, enforcement, contributed, settlement, property, major, eighteenth 
## Topic 8 Top Words:
##       Highest Prob: upon, government, shall, can, must, great, may 
##       FREX: army, interstate, negro, executive, tariff, business, proper 
##       Lift: affected, amendments, antitrust, army, attention, avail, banking 
##       Score: tariff, interstate, army, negro, policy, proper, business
\end{verbatim}

Here, we see that the word ``us'' is relatively dominant in all our topics, making it a candidate for removal as a stop words in a future analysis. Looking at is more closely, we find that the first topic broadly refers to peace, the second, third and seventh to the world, the fourth and sixth to america, and the eighth to the government.

Finally, we can see whether there is any relation between these topics and any of the parameters we included. Here, let us look at any existing differences between the two parties:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est\_assoc\_effect }\OtherTok{\textless{}{-}} \FunctionTok{estimateEffect}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Party, output\_stm, }\AttributeTok{metadata =}\NormalTok{ meta,}
    \AttributeTok{prior =} \FloatTok{1e{-}05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot.estimateEffect}\NormalTok{(est\_assoc\_effect, }\StringTok{"Party"}\NormalTok{, }\AttributeTok{method =} \StringTok{"pointestimate"}\NormalTok{,}
    \AttributeTok{model =}\NormalTok{ output\_stm)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Quantitative-Text-Analysis_files/figure-latex/unnamed-chunk-169-1.pdf}

Here, we find that while the average for the topic do seem to differ a little between both of the parties, all the intervals are overlapping, thus indicating that they are not significantly different.

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-Albaugh2013}{}%
Albaugh, Quinn, Julie Sevenans, Stuart Soroka, and Peter John Loewen. 2013. {``The Automated Coding of Policy Agendas: A Dictionary-Based Approach.''} In \emph{6th Annual Comparative Agendas Conference, Antwerp, Belgium}.

\leavevmode\hypertarget{ref-Bakker2012a}{}%
Bakker, Ryan, Catherine de Vries, Erica Edwards, Liesbet Hooghe, Seth Jolly, Gary Marks, Jonathan Polk, Jan Rovny, Marco R. Steenbergen, and Milada Anna Vachudova. 2012. {``Measuring Party Positions in Europe: The Chapel Hill Expert Survey Trend File, 1999-2010: The Chapel Hill Expert Survey Trend File, 1999--2010.''} \emph{Party Politics} 21 (1): 1--15. \url{https://doi.org/10.1177/1354068812462931}.

\leavevmode\hypertarget{ref-Benoit2009a}{}%
Benoit, Kenneth, Michael Laver, and Slava Mikhaylov. 2009. {``Treating Words as Data with Error: Uncertainty in Text Statements of Policy Positions.''} \emph{American Journal of Political Science} 53 (2): 495--513. \url{https://doi.org/10.1111/j.1540-5907.2009.00383.x}.

\leavevmode\hypertarget{ref-Benoit2018}{}%
Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Mller, and Akitaka Matsuo. 2018. {``Quanteda: An r Package for the Quantitative Analysis of Textual Data.''} \emph{Journal of Open Source Software} 3 (30): 774.

\leavevmode\hypertarget{ref-Blei2003a}{}%
Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. {``Latent Dirichlet Allocation.''} \emph{Journal of Machine Learning Research} 3 (Jan): 993--1022.

\leavevmode\hypertarget{ref-Carmines1979a}{}%
Carmines, Edward G., and Richard A. Zeller. 1979. \emph{Reliability and Validity Assessment}. Beverly Hills, CA: Sage.

\leavevmode\hypertarget{ref-Clarke2019a}{}%
Clarke, Isobelle, and Jack Grieve. 2019. {``Stylistic Variation on the Donald Trump Twitter Account: A Linguistic Analysis of Tweets Posted Between 2009 and 2018.''} \emph{PLOS ONE} 14 (9): 1--27. \url{https://doi.org/10.1371/journal.pone.0222062}.

\leavevmode\hypertarget{ref-Freelon2018}{}%
Freelon, Deen. 2018. {``Computational Research in the Post-API Age.''} \emph{Political Communication} 35 (4): 665--68.

\leavevmode\hypertarget{ref-Griffiths2004a}{}%
Griffiths, Thomas L, and Mark Steyvers. 2004. {``Finding Scientific Topics.''} \emph{Proceedings of the National Academy of Sciences} 101 (suppl 1): 5228--35.

\leavevmode\hypertarget{ref-Grimmer2013a}{}%
Grimmer, Justin, and Brandon M. Stewart. 2013. {``Text as Data: The Promise and Pitfals of Automatic Content Analysis Methods for Political Texts.''} \emph{Political Analysis} 21 (3): 267--97. \url{https://doi.org/10.1093/pan/mps028}.

\leavevmode\hypertarget{ref-Haselmayer2017}{}%
Haselmayer, Martin, and Marcelo Jenny. 2017. {``Sentiment Analysis of Political Communication: Combining a Dictionary Approach with Crowdcoding.''} \emph{Quality \& Quantity} 51 (6): 2623--46.

\leavevmode\hypertarget{ref-Hayes2007a}{}%
Hayes, Andrew F., and Klaus Krippendorff. 2007. {``Answering the Call for a Standard Reliability Measure for Coding Data.''} \emph{Communication Methods and Measures} 1 (1): 77--89. \url{https://doi.org/10.1080/19312450709336664}.

\leavevmode\hypertarget{ref-Krippendorff2004a}{}%
Krippendorff, Klaus. 2004. \emph{Content Analysis - an Introduction to Its Methodology}. 2nd ed. Thousand Oaks, CA: SAGE Publications.

\leavevmode\hypertarget{ref-Lamprianou2020}{}%
Lamprianou, Iasonas. 2020. {``Measuring and Visualizing Coders' Reliability: New Approaches and Guidelines from Experimental Data.''} \emph{Sociological Methods \& Research}, 0049124120926198.

\leavevmode\hypertarget{ref-Laver2003a}{}%
Laver, Michael, Kenneth Benoit, and John Garry. 2003. {``Extracting Policy Positions from Political Texts Using Words as Data.''} \emph{The American Political Science Review} 97 (2): 311--31. \url{https://doi.org/10.1017/S0003055403000698}.

\leavevmode\hypertarget{ref-Laver2000a}{}%
Laver, Michael, and John Garry. 2000. {``Estimating Policy Positions from Political Texts.''} \emph{American Journal of Political Science} 44 (3): 619--34. \url{https://doi.org/10.2307/2669268}.

\leavevmode\hypertarget{ref-Lin1989a}{}%
Lin, L. 1989. {``A Concordance Correlation Coefficient to Evaluate Reproducibility.''} \emph{Biometrics} 45: 255--68.

\leavevmode\hypertarget{ref-Lind2019}{}%
Lind, Fabienne, Jakob-Moritz Eberl, Tobias Heidenreich, Hajo G Boomgaarden, Eva Luisa Gmez Montero, Beatriz Herrero, Rosa Berganza, Will Allen, and Peter Bajomi-Lazar. 2019. {``Multilingual Dictionary Construction: A Roadmap to Measuring Migration Frames in European Media Discourse.''}

\leavevmode\hypertarget{ref-Lowe2011b}{}%
Lowe, Will. 2011. \emph{JFreq: Count Words, Quickly}. \url{http://www.conjugateprior.org/software/jfreq/}.

\leavevmode\hypertarget{ref-Lowe2011a}{}%
Lowe, Will, and Kenneth Benoit. 2011. {``Estimating Uncertainty in Quantitative Text Analysis.''} \emph{Paper Prepared for Presentation at the Annual Conference of the Midwest Political Science Association}, 1--34.

\leavevmode\hypertarget{ref-Martin2008a}{}%
Martin, Lanny W., and Georg Vanberg. 2008. {``Reply to Benoit and Laver.''} \emph{Political Analysis} 16 (1): 112--14. \url{https://doi.org/10.1093/pan/mpm018}.

\leavevmode\hypertarget{ref-Mikhaylov2012a}{}%
Mikhaylov, Slava, Michael Laver, and Kenneth Benoit. 2012. {``Coder Reliability and Misclassification in the Human Coding of Party Manifestos.''} \emph{Political Analysis} 20 (1): 78--91. \url{https://doi.org/10.1093/pan/mpr047}.

\leavevmode\hypertarget{ref-Munzert2014}{}%
Munzert, Simon, Christian Rubba, Peter Meiner, and Dominic Nyhuis. 2014. \emph{Automated Data Collection with r: A Practical Guide to Web Scraping and Text Mining}. John Wiley \& Sons.

\leavevmode\hypertarget{ref-Perriam2020}{}%
Perriam, Jessamy, Andreas Birkbak, and Andy Freeman. 2020. {``Digital Methods in a Post-API Environment.''} \emph{International Journal of Social Research Methodology} 23 (3): 277--90.

\leavevmode\hypertarget{ref-Roberts2019a}{}%
Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2019. {``{stm: An R Package for Structural Topic Models}.''} \emph{Journal of Statistical Software} 91 (2). \url{https://doi.org/10.18637/jss.v091.i02}.

\leavevmode\hypertarget{ref-Roberts2014a}{}%
Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G. Rand. 2014. {``Structural Topic Models for Open-Ended Survey Responses.''} \emph{American Journal of Political Science} 58 (4): 1064--82. \url{https://doi.org/10.1111/ajps.12103}.

\leavevmode\hypertarget{ref-Slapin2008a}{}%
Slapin, Jonathan B., and Sven-Oliver Proksch. 2008. {``A Scaling Model for Estimating Time-Series Party Positions from Texts.''} \emph{American Journal of Political Science} 52 (3): 705--22.

\leavevmode\hypertarget{ref-Volkens2019a}{}%
Volkens, Andrea, Werner Krause, Pola Lehmann, Theres Matthie, Nicolas Merz, Sven Regel, and Bernhard Weels. 2019. {``{The Manifesto Data Collection. Manifesto Project (MRG/CMP/MARPOR)}.''} Berlin: Wissenschaftszentrum Berlin f{}r Sozialforschung (WZB). 2019. \url{https://doi.org/10.25522/manifesto.mpds.2019b}.

\leavevmode\hypertarget{ref-Welbers2017}{}%
Welbers, Kasper, Wouter Van Atteveldt, and Kenneth Benoit. 2017. {``Text Analysis in r.''} \emph{Communication Methods and Measures} 11 (4): 245--65.

\leavevmode\hypertarget{ref-Young2012}{}%
Young, Lori, and Stuart Soroka. 2012. {``Lexicoder Sentiment Dictionary.''}

\end{CSLReferences}

\end{document}
