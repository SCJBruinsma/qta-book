<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Preliminaries | Quantitative Text Analysis</title>
  <meta name="description" content="Theory and Methods for Quantitative Text Analysis" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Preliminaries | Quantitative Text Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Theory and Methods for Quantitative Text Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Preliminaries | Quantitative Text Analysis" />
  
  <meta name="twitter:description" content="Theory and Methods for Quantitative Text Analysis" />
  

<meta name="author" content="Kostas Gemenis &amp; Bastiaan Bruinsma" />


<meta name="date" content="2021-07-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="reliability-and-validity.html"/>
<link rel="next" href="dictionary-analysis.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro to Quantitative Text Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i><b>1</b> Foreword</a>
<ul>
<li class="chapter" data-level="1.1" data-path="foreword.html"><a href="foreword.html#installing-r"><i class="fa fa-check"></i><b>1.1</b> Installing R</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="foreword.html"><a href="foreword.html#r-on-windows"><i class="fa fa-check"></i><b>1.1.1</b> R on Windows</a></li>
<li class="chapter" data-level="1.1.2" data-path="foreword.html"><a href="foreword.html#r-on-linux"><i class="fa fa-check"></i><b>1.1.2</b> R on Linux</a></li>
<li class="chapter" data-level="1.1.3" data-path="foreword.html"><a href="foreword.html#r-on-macos"><i class="fa fa-check"></i><b>1.1.3</b> R on macOS</a></li>
<li class="chapter" data-level="1.1.4" data-path="foreword.html"><a href="foreword.html#r-in-the-cloud"><i class="fa fa-check"></i><b>1.1.4</b> R in the Cloud</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="installing-packages.html"><a href="installing-packages.html"><i class="fa fa-check"></i><b>2</b> Installing Packages</a>
<ul>
<li class="chapter" data-level="2.1" data-path="installing-packages.html"><a href="installing-packages.html#installing-from-cran"><i class="fa fa-check"></i><b>2.1</b> Installing from CRAN</a></li>
<li class="chapter" data-level="2.2" data-path="installing-packages.html"><a href="installing-packages.html#installing-from-github"><i class="fa fa-check"></i><b>2.2</b> Installing from GitHub</a></li>
<li class="chapter" data-level="2.3" data-path="installing-packages.html"><a href="installing-packages.html#quantitative-text-analysis-in-r"><i class="fa fa-check"></i><b>2.3</b> Quantitative Text Analysis in R</a></li>
<li class="chapter" data-level="2.4" data-path="installing-packages.html"><a href="installing-packages.html#issues-bugs-and-errors"><i class="fa fa-check"></i><b>2.4</b> Issues, Bugs and Errors</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>3</b> Importing Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="importing-data.html"><a href="importing-data.html#text-in-r"><i class="fa fa-check"></i><b>3.1</b> Text in R</a></li>
<li class="chapter" data-level="3.2" data-path="importing-data.html"><a href="importing-data.html#import-.txt-files"><i class="fa fa-check"></i><b>3.2</b> Import .txt Files</a></li>
<li class="chapter" data-level="3.3" data-path="importing-data.html"><a href="importing-data.html#import-.pdf-files"><i class="fa fa-check"></i><b>3.3</b> Import .pdf Files</a></li>
<li class="chapter" data-level="3.4" data-path="importing-data.html"><a href="importing-data.html#import-.csv-files"><i class="fa fa-check"></i><b>3.4</b> Import .csv Files</a></li>
<li class="chapter" data-level="3.5" data-path="importing-data.html"><a href="importing-data.html#import-from-an-api"><i class="fa fa-check"></i><b>3.5</b> Import from an API</a></li>
<li class="chapter" data-level="3.6" data-path="importing-data.html"><a href="importing-data.html#import-using-web-scraping"><i class="fa fa-check"></i><b>3.6</b> Import using Web Scraping</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html"><i class="fa fa-check"></i><b>4</b> Reliability and Validity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#inter-coder-agreement"><i class="fa fa-check"></i><b>4.1</b> Inter-Coder Agreement</a></li>
<li class="chapter" data-level="4.2" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#visualizing-quality"><i class="fa fa-check"></i><b>4.2</b> Visualizing Quality</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>5</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="5.1" data-path="preliminaries.html"><a href="preliminaries.html#the-corpus"><i class="fa fa-check"></i><b>5.1</b> The Corpus</a></li>
<li class="chapter" data-level="5.2" data-path="preliminaries.html"><a href="preliminaries.html#keywords-in-context"><i class="fa fa-check"></i><b>5.2</b> Keywords in Context</a></li>
<li class="chapter" data-level="5.3" data-path="preliminaries.html"><a href="preliminaries.html#visualisations-and-descriptives"><i class="fa fa-check"></i><b>5.3</b> Visualisations and Descriptives</a></li>
<li class="chapter" data-level="5.4" data-path="preliminaries.html"><a href="preliminaries.html#text-statistics"><i class="fa fa-check"></i><b>5.4</b> Text Statistics</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html"><i class="fa fa-check"></i><b>6</b> Dictionary Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html#classical-dictionary-analysis"><i class="fa fa-check"></i><b>6.1</b> Classical Dictionary Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html#sentiment-analysis"><i class="fa fa-check"></i><b>6.2</b> Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html#movie-reviews"><i class="fa fa-check"></i><b>6.2.1</b> Movie Reviews</a></li>
<li class="chapter" data-level="6.2.2" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html#twitter"><i class="fa fa-check"></i><b>6.2.2</b> Twitter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="scaling.html"><a href="scaling.html"><i class="fa fa-check"></i><b>7</b> Scaling</a>
<ul>
<li class="chapter" data-level="7.1" data-path="scaling.html"><a href="scaling.html#wordscores"><i class="fa fa-check"></i><b>7.1</b> Wordscores</a></li>
<li class="chapter" data-level="7.2" data-path="scaling.html"><a href="scaling.html#wordfish"><i class="fa fa-check"></i><b>7.2</b> Wordfish</a></li>
<li class="chapter" data-level="7.3" data-path="scaling.html"><a href="scaling.html#correspondence-analysis"><i class="fa fa-check"></i><b>7.3</b> Correspondence Analysis</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="supervised-methods.html"><a href="supervised-methods.html"><i class="fa fa-check"></i><b>8</b> Supervised Methods</a>
<ul>
<li class="chapter" data-level="8.1" data-path="supervised-methods.html"><a href="supervised-methods.html#support-vector-machines"><i class="fa fa-check"></i><b>8.1</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="supervised-methods.html"><a href="supervised-methods.html#svm-with-rtexttools"><i class="fa fa-check"></i><b>8.1.1</b> SVM with RTextTools</a></li>
<li class="chapter" data-level="8.1.2" data-path="supervised-methods.html"><a href="supervised-methods.html#svm-with-quanteda"><i class="fa fa-check"></i><b>8.1.2</b> SVM with Quanteda</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="supervised-methods.html"><a href="supervised-methods.html#naive-bayes"><i class="fa fa-check"></i><b>8.2</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>9</b> Unsupervised Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>9.1</b> Latent Dirichlet Allocation</a></li>
<li class="chapter" data-level="9.2" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#seeded-latent-dirichlet-allocation"><i class="fa fa-check"></i><b>9.2</b> Seeded Latent Dirichlet Allocation</a></li>
<li class="chapter" data-level="9.3" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html#structural-topic-model"><i class="fa fa-check"></i><b>9.3</b> Structural Topic Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="bruinsma@soz.uni-frankfurt.de" target="blank">Bastiaan Bruinsma</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Text Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="preliminaries" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Preliminaries</h1>
<p>Before we start with any kind of analyses, it pays to have a brief look at the preliminaries first. The goal of these preliminaries is to give us a better understanding of “what” are texts are about, who there authors are, and what kind of words and information we can expect to find in them. This is necessary for us to know our data. Not only is it standard academic practise to know your data well, it also helps us to decide whether results we will encounter later on really do make sense. Here, we look at three different preliminaries: the idea of keywords-in-context, several visualisations, and a range of textual statistics. Before that though, we will have a brief look at the idea of the “corpus,” as it is central to the idea of how <code>quanteda</code> works.</p>
<div id="the-corpus" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> The Corpus</h2>
<p>Within <code>quanteda</code>, the main way to store documents is in the form of a <code>corpus</code> object. This object contains all the information that comes with the texts and does not change during our analysis. Instead, we make copies of the main corpus, change them into the type we need, and run our analyses on them. The advantage of this is that we always can go back to our original data.</p>
<p>Apart from importing texts ourselves, <code>quanteda</code> contains several corpora as well. Here, we use one of these, which contains the inaugural speeches of all the US Presidents. For this, we first have to load the main package and then load the data into R:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="preliminaries.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb60-2"><a href="preliminaries.html#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="preliminaries.html#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(data_corpus_inaugural)</span>
<span id="cb60-4"><a href="preliminaries.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_corpus_inaugural)</span></code></pre></div>
<pre><code>## Corpus consisting of 6 documents and 4 docvars.
## 1789-Washington :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;
## 
## 1793-Washington :
## &quot;Fellow citizens, I am again called upon by the voice of my c...&quot;
## 
## 1797-Adams :
## &quot;When it was first perceived, in early times, that no middle ...&quot;
## 
## 1801-Jefferson :
## &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot;
## 
## 1805-Jefferson :
## &quot;Proceeding, fellow citizens, to that qualification which the...&quot;
## 
## 1809-Madison :
## &quot;Unwilling to depart from examples of the most revered author...&quot;</code></pre>
<p>You should now see the corpus appear in the Environment tab. If you click on it, you can see, amongst others, that the corpus comes with information on the Year of the release of the speech and the president it belongs to. As the corpus is quite large, we make it a bit more manageable by only selecting the speeches from 1900 onwards. We can do this by using the <code>corpus_subset</code> command for both:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="preliminaries.html#cb62-1" aria-hidden="true" tabindex="-1"></a>corpus_inaugural <span class="ot">&lt;-</span> <span class="fu">corpus_subset</span>(data_corpus_inaugural, Year <span class="sc">&gt;</span> <span class="dv">1900</span>)</span></code></pre></div>
<p>Now we have our corpus, we can start with the analysis. As noted, we try not to carry out any analysis on the corpus itself. Instead, we keep it as it is and work on its copies. Often, this means transforming the data into another shape. One of the more popular shapes is the data frequency matrix (dfm). This is a matrix which contains the documents in the rows and the word counts for each word in the columns.</p>
<p>Before we can do so however, we have to split up our texts into unique words. To do this, we first have to construct a <code>tokens</code> object. In the command that we use to do this, we can specify how we want our texts to be split (here we use the standard option), and in addition clean our data a bit. For example, we can specify that we want to convert all the texts into lowercase and remove any numbers and special characters.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="preliminaries.html#cb63-1" aria-hidden="true" tabindex="-1"></a>data_inaugural_tokens <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_inaugural, <span class="at">what =</span> <span class="st">&quot;word&quot;</span>,</span>
<span id="cb63-2"><a href="preliminaries.html#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">remove_punct =</span> <span class="cn">TRUE</span>, <span class="at">remove_symbols =</span> <span class="cn">TRUE</span>, <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>,</span>
<span id="cb63-3"><a href="preliminaries.html#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">remove_url =</span> <span class="cn">TRUE</span>, <span class="at">remove_separators =</span> <span class="cn">TRUE</span>, <span class="at">split_hyphens =</span> <span class="cn">FALSE</span>,</span>
<span id="cb63-4"><a href="preliminaries.html#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">include_docvars =</span> <span class="cn">TRUE</span>, <span class="at">padding =</span> <span class="cn">FALSE</span>, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Creating a tokens object from a corpus input...</code></pre>
<pre><code>##  ...starting tokenization</code></pre>
<pre><code>##  ...1901-McKinley to 2021-Biden.txt</code></pre>
<pre><code>##  ...preserving hyphens</code></pre>
<pre><code>##  ...preserving social media tags (#, @)</code></pre>
<pre><code>##  ...segmenting into words</code></pre>
<pre><code>##  ...7,013 unique types</code></pre>
<pre><code>##  ...removing separators, punctuation, symbols, numbers, URLs</code></pre>
<pre><code>##  ...complete, elapsed time: 0.192 seconds.</code></pre>
<pre><code>## Finished constructing tokens from 31 documents.</code></pre>
<p>We can also remove certain stopwords so that words like “and” or “the” do not influence our analysis too much. We can either specify these words ourselves or we can use a list that is already present in R. To see this list, type <code>stopwords("english")</code> in the console. Stopwords for other languages are also available (such as German, French and Spanish). Even more stopwords can be found in the <code>stopword</code> package, that can easily be integrated with <code>quanteda</code>. For now, we will use the English ones. First, however, as all the stopwords are lower-case, we will have to lower case our words as well:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="preliminaries.html#cb74-1" aria-hidden="true" tabindex="-1"></a>data_inaugural_tokens <span class="ot">&lt;-</span> <span class="fu">tokens_tolower</span>(data_inaugural_tokens, <span class="at">keep_acronyms =</span> <span class="cn">FALSE</span>)</span>
<span id="cb74-2"><a href="preliminaries.html#cb74-2" aria-hidden="true" tabindex="-1"></a>data_inaugural_tokens <span class="ot">&lt;-</span> <span class="fu">tokens_select</span>(data_inaugural_tokens, <span class="fu">stopwords</span>(<span class="st">&quot;english&quot;</span>), <span class="at">selection =</span> <span class="st">&quot;remove&quot;</span>)</span></code></pre></div>
<p>Then, we can construct our dfm:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="preliminaries.html#cb75-1" aria-hidden="true" tabindex="-1"></a>data_inaugural_dfm <span class="ot">&lt;-</span> <span class="fu">dfm</span>(data_inaugural_tokens)</span></code></pre></div>
</div>
<div id="keywords-in-context" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Keywords in Context</h2>
<p>Besides all the analytical techniques we have available, we can also use <code>quanteda</code> to look at various rather simpler ones. One popular option is known as keywords in context (kwic), in which we are interested with which other words a certain word appears in our texts. This is also known as looking at the “concordance” of our text. Here, we can easily find this with our tokens dataframe. Let’s say we are interested in all those words that start with “secure” and we want to know which three words occur before and after this word. We can then run:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="preliminaries.html#cb76-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_inaugural)</span>
<span id="cb76-2"><a href="preliminaries.html#cb76-2" aria-hidden="true" tabindex="-1"></a>kwic_output <span class="ot">&lt;-</span> <span class="fu">kwic</span>(tokens, <span class="at">pattern =</span> <span class="st">&quot;secure*&quot;</span>, <span class="at">valuetype =</span> <span class="st">&quot;glob&quot;</span>, <span class="at">window =</span> <span class="dv">3</span>)</span></code></pre></div>
<p>In the outputted object, we find a column labelled “pre” and another labelled “post.” These refer to the words that came either before or after the word "secure*". We can easily take these out and combine them:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="preliminaries.html#cb77-1" aria-hidden="true" tabindex="-1"></a>text_pre <span class="ot">&lt;-</span> kwic_output<span class="sc">$</span>pre</span>
<span id="cb77-2"><a href="preliminaries.html#cb77-2" aria-hidden="true" tabindex="-1"></a>text_post <span class="ot">&lt;-</span> kwic_output<span class="sc">$</span>post</span>
<span id="cb77-3"><a href="preliminaries.html#cb77-3" aria-hidden="true" tabindex="-1"></a>text_word <span class="ot">&lt;-</span> kwic_output<span class="sc">$</span>keyword</span>
<span id="cb77-4"><a href="preliminaries.html#cb77-4" aria-hidden="true" tabindex="-1"></a>text <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">paste</span>(text_pre, text_word, text_post))</span></code></pre></div>
<p>We then combine this information with the name of the document it came from so that we know which text the word is from:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="preliminaries.html#cb78-1" aria-hidden="true" tabindex="-1"></a>extracted <span class="ot">&lt;-</span> <span class="fu">cbind</span>(kwic_output<span class="sc">$</span>docname, text)</span>
<span id="cb78-2"><a href="preliminaries.html#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(extracted) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;docname&quot;</span>, <span class="st">&quot;text&quot;</span>)</span>
<span id="cb78-3"><a href="preliminaries.html#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(extracted)</span></code></pre></div>
<pre><code>##         docname                                      text
## 1 1901-McKinley be adapted to secure a government capable
## 2     1909-Taft               , and to secure at the same
## 3     1909-Taft          is needed to secure a more rapid
## 4     1909-Taft  . This should secure an adequate revenue
## 5     1909-Taft     business . To secure the needed speed
## 6     1909-Taft    duties as to secure an adequate income</code></pre>
</div>
<div id="visualisations-and-descriptives" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Visualisations and Descriptives</h2>
<p>Another thing we can do with this dfm is to generate a frequency graph using the <code>topfeatures</code> function. For this, we first have to save the 50 most frequently occurring words in our texts (note that there is also the <code>textstat_frequency</code> function in the <code>quanteda.textstats</code> helper package that can do this):</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="preliminaries.html#cb80-1" aria-hidden="true" tabindex="-1"></a>features <span class="ot">&lt;-</span> <span class="fu">topfeatures</span>(data_inaugural_dfm, <span class="dv">50</span>)</span></code></pre></div>
<p>We then have to transform this object into a data frame, and sort it by decreasing frequency:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="preliminaries.html#cb81-1" aria-hidden="true" tabindex="-1"></a>features_plot <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">list</span>(<span class="at">term =</span> <span class="fu">names</span>(features),<span class="at">frequency =</span> <span class="fu">unname</span>(features)))</span>
<span id="cb81-2"><a href="preliminaries.html#cb81-2" aria-hidden="true" tabindex="-1"></a>features_plot<span class="sc">$</span>term <span class="ot">&lt;-</span> <span class="fu">with</span>(features_plot, <span class="fu">reorder</span>(term, <span class="sc">-</span>frequency))</span></code></pre></div>
<p>Then we can plot the results:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="preliminaries.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb82-2"><a href="preliminaries.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(features_plot) <span class="sc">+</span> </span>
<span id="cb82-3"><a href="preliminaries.html#cb82-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>term, <span class="at">y=</span>frequency)) <span class="sc">+</span></span>
<span id="cb82-4"><a href="preliminaries.html#cb82-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">theme_classic</span>()<span class="sc">+</span></span>
<span id="cb82-5"><a href="preliminaries.html#cb82-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">theme</span>(<span class="at">axis.text.x=</span><span class="fu">element_text</span>(<span class="at">angle=</span><span class="dv">90</span>, <span class="at">hjust=</span><span class="dv">1</span>))</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>We can also generate word clouds. As these show all the words we have, we will trim our dfm first to remove all those words that occurred less than 40 times. We can do this with the <code>dfm_trim</code> function. Then, we can use this newly trimmed dfm to generate the word cloud:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="preliminaries.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textplots)</span>
<span id="cb83-2"><a href="preliminaries.html#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="preliminaries.html#cb83-3" aria-hidden="true" tabindex="-1"></a>wordcloud_dfm_trim <span class="ot">&lt;-</span> <span class="fu">dfm_trim</span>(data_inaugural_dfm, <span class="at">min_termfreq =</span> <span class="dv">40</span>)</span>
<span id="cb83-4"><a href="preliminaries.html#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="fu">textplot_wordcloud</span>(wordcloud_dfm_trim)</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>If we would want to, we can also split up this word cloud based on which words belong to which party. For this, we have to generate a new dfm and within it, specify the groups that well which words belong to which party. Given that we have only Democratic and Republican presidents, we end up with two groups:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="preliminaries.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textplots)</span>
<span id="cb84-2"><a href="preliminaries.html#cb84-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-3"><a href="preliminaries.html#cb84-3" aria-hidden="true" tabindex="-1"></a>wordcloud_dfm_comp <span class="ot">&lt;-</span> <span class="fu">dfm_group</span>(data_inaugural_dfm, <span class="at">groups =</span> Party)</span>
<span id="cb84-4"><a href="preliminaries.html#cb84-4" aria-hidden="true" tabindex="-1"></a>wordcloud_dfm_comp <span class="ot">&lt;-</span> <span class="fu">dfm_trim</span>(wordcloud_dfm_comp, <span class="at">min_termfreq =</span> <span class="dv">20</span>,</span>
<span id="cb84-5"><a href="preliminaries.html#cb84-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">max_words =</span> <span class="dv">40</span>)</span>
<span id="cb84-6"><a href="preliminaries.html#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="fu">textplot_wordcloud</span>(wordcloud_dfm_comp, <span class="at">comparison =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
</div>
<div id="text-statistics" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Text Statistics</h2>
<p>Finally, <em>quanteda</em> also allows us to calculate quite a number of textual statistics. These are all collected in the <em>quanteda.textstats</em> helper package. Here, we will look at several of them, starting with a simple overview of our corpus in the terms of a summary that tells us the number of characters, sentences, tokens, etc. for each of the texts:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="preliminaries.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textstats)</span>
<span id="cb85-2"><a href="preliminaries.html#cb85-2" aria-hidden="true" tabindex="-1"></a>corpus_summary <span class="ot">&lt;-</span> <span class="fu">textstat_summary</span>(corpus_inaugural)</span></code></pre></div>
<p>If we want, we can then use this data to make some simple graphs telling us various things about the texts in our corpus. As an example, let’s look at the number of sentences the various presidents put in their speeches:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="preliminaries.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>corpus_summary, <span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>sents, <span class="at">group=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb86-2"><a href="preliminaries.html#cb86-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb86-3"><a href="preliminaries.html#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb86-4"><a href="preliminaries.html#cb86-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Number of Characters&quot;</span>)<span class="sc">+</span></span>
<span id="cb86-5"><a href="preliminaries.html#cb86-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;President/Year&quot;</span>)<span class="sc">+</span></span>
<span id="cb86-6"><a href="preliminaries.html#cb86-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()<span class="sc">+</span></span>
<span id="cb86-7"><a href="preliminaries.html#cb86-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>))</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>Another thing we can look at are the readability and lexical diversity of the texts. The former one of these refers to how readable a text is (i.e. how easy or difficult it is to read), while the latter tells us how many different types of words are used in the texts and thus how “diverse” the text is in word choice and use. Given that there are many ways to calculate both metrics, please have a look at the help file to see which one works best for you. Here, we will use the most popular:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="preliminaries.html#cb87-1" aria-hidden="true" tabindex="-1"></a>corpus_readability <span class="ot">&lt;-</span> <span class="fu">textstat_readability</span>(corpus_inaugural, <span class="at">measure =</span> <span class="fu">c</span>(<span class="st">&quot;Flesch.Kincaid&quot;</span>, <span class="st">&quot;Dale.Chall.old&quot;</span>))</span>
<span id="cb87-2"><a href="preliminaries.html#cb87-2" aria-hidden="true" tabindex="-1"></a>corpus_lexdiv <span class="ot">&lt;-</span> <span class="fu">textstat_lexdiv</span>(data_inaugural_tokens, <span class="fu">c</span>(<span class="st">&quot;CTTR&quot;</span>, <span class="st">&quot;TTR&quot;</span>, <span class="st">&quot;MATTR&quot;</span>), <span class="at">MATTR_window =</span> <span class="dv">100</span>)</span></code></pre></div>
<p>As before, we can easily plot this data in a graph to see how lexical diversity developed over time:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="preliminaries.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>corpus_lexdiv, <span class="fu">aes</span>(<span class="at">x=</span>document, <span class="at">y=</span>CTTR, <span class="at">group=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb88-2"><a href="preliminaries.html#cb88-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb88-3"><a href="preliminaries.html#cb88-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb88-4"><a href="preliminaries.html#cb88-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Lexical Diversity (CTTR)&quot;</span>)<span class="sc">+</span></span>
<span id="cb88-5"><a href="preliminaries.html#cb88-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;President/Year&quot;</span>)<span class="sc">+</span></span>
<span id="cb88-6"><a href="preliminaries.html#cb88-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()<span class="sc">+</span></span>
<span id="cb88-7"><a href="preliminaries.html#cb88-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>))</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>Another thing we can do is look at the similarities and distances between documents. With this, we can answer questions such as: how “different” are these documents from each other? And if different (or similar), how different (or similar)? The idea is that the larger the similarity is, the smaller the distance is as well. A good way to understand the idea of similarity is to consider how many operations you need to perform to change one text into the other. The more “replace” options you have to carry out, the more different the text. As for the distances, it is best to consider the texts as having positions on a Cartesian plane (with positions based on their word counts). The distance between these two points (either Euclidean, Manhattan or other) is then the distance between the texts.</p>
<p>Let’s start with a look at these similarities (note again that there are many different methods to calculate this):</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="preliminaries.html#cb89-1" aria-hidden="true" tabindex="-1"></a>corpus_similarties <span class="ot">&lt;-</span> <span class="fu">textstat_simil</span>(data_inaugural_dfm, <span class="at">method =</span> <span class="st">&quot;correlation&quot;</span>, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>)</span>
<span id="cb89-2"><a href="preliminaries.html#cb89-2" aria-hidden="true" tabindex="-1"></a>corpus_similarties <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_similarties)</span></code></pre></div>
<p>As brief look at these results tells us that the 1981 and 1985 Reagan speeches show the highest degree of similarity, while the<br />
1945 Roosevelt and 2017 Trump speeches are the most different. Note that while we look here at the documents, we could also look at individual words (set <code>margin="features</code>). For now, let us look at the distances between the documents, choosing the Euclidean distance between the documents as our metric:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="preliminaries.html#cb90-1" aria-hidden="true" tabindex="-1"></a>corpus_distances <span class="ot">&lt;-</span> <span class="fu">textstat_dist</span>(data_inaugural_dfm, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb90-2"><a href="preliminaries.html#cb90-2" aria-hidden="true" tabindex="-1"></a>corpus_distances_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_distances)</span></code></pre></div>
<p>Here, we find the 1905 and 1945 Roosevelt speeches (the two different Roosevelts) to be the closest, and the 1909 Taft and 1997 Clinton speeches to be furthest apart. If we want to, we can even convert this data into a dendrogram. We do this by taking the information on the distances out of the <code>corpus_distances</code> object, make them into a triangular matrix, and plot them:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="preliminaries.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">hclust</span>(<span class="fu">as.dist</span>(corpus_distances)), <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="Intro-to-Quantitative-Text-Analysis_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p>Here, we can easily see that - amongst others - the 1909 Taft document is the “farthest” away from all the others, that the 1981 and 1985 Reagan speeches were very close, and that the 1997 Clinton speech was closer to Nixon’s speeches, than his 1993 speech (which was close to the 2009 and 2013 Obama speeches).</p>
<p>Finally, let us look at the entropy of our texts. The entropy of a document measures the “amount” of information the is produced by each letter of the text. To get an idea of what this means, consider the “e” is a frequently occurring letter in an English text, while “z” is not. Thus, a word with a “z” in it, it more unique and thus likely to carry unique and interesting information. The “higher” the entropy of a text, the less “information” is in it:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="preliminaries.html#cb92-1" aria-hidden="true" tabindex="-1"></a>corpus_entropy_docs <span class="ot">&lt;-</span> <span class="fu">textstat_entropy</span>(data_inaugural_dfm, <span class="st">&quot;documents&quot;</span>)</span>
<span id="cb92-2"><a href="preliminaries.html#cb92-2" aria-hidden="true" tabindex="-1"></a>corpus_entropy_docs <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_entropy_docs)</span></code></pre></div>
<p>As we can see, the Roosevelt speeches had the lowest entropies, while the 1909 Taft and 1925 Coolidge speeches were the highest (in relative terms). While not as common as the other distances metrics, entropy is sometimes used to measure the similarity between texts and can be useful if we want to know the importance of certain words. This because if a certain word is not “important,” we might consider it to become a stop word:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="preliminaries.html#cb93-1" aria-hidden="true" tabindex="-1"></a>corpus_entropy_feats <span class="ot">&lt;-</span> <span class="fu">textstat_entropy</span>(data_inaugural_dfm, <span class="st">&quot;features&quot;</span>)</span>
<span id="cb93-2"><a href="preliminaries.html#cb93-2" aria-hidden="true" tabindex="-1"></a>corpus_entropy_feats <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(corpus_entropy_feats)</span>
<span id="cb93-3"><a href="preliminaries.html#cb93-3" aria-hidden="true" tabindex="-1"></a>corpus_entropy_feats <span class="ot">&lt;-</span> corpus_entropy_feats[<span class="fu">order</span>(<span class="sc">-</span>corpus_entropy_feats<span class="sc">$</span>entropy),]</span>
<span id="cb93-4"><a href="preliminaries.html#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(corpus_entropy_feats, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##     feature  entropy
## 164  people 4.766391
## 488    life 4.747627
## 385  nation 4.737440
## 5     great 4.654392
## 114     can 4.651396
## 317  future 4.639222
## 197   world 4.616910
## 212    time 4.616614
## 402    must 4.610073
## 231     god 4.601430</code></pre>
<p>Looking at the data, we find that “people,” “life” and “nation” have pretty high entropies, indicating that the words added little in terms of information to the documents, and it would be a candidate to remove from our corpus.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="reliability-and-validity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dictionary-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Intro to Quantitative Text Analysis.pdf", "Intro to Quantitative Text Analysis.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
