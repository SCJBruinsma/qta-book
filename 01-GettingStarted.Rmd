# Getting Started with Quantitative Text Analysis{#getting-started-with-qta}

Quantitative text analysis, like many other techniques, is at its core a method. This means that while it provides you with the tools to answer a particular question, it does not provide you with a theoretical framework. Nor is there any reality to be discovered: the only thing we can do with QTA methods is to provide (hopefully) accurate summaries of our texts.

With this in mind, there are five questions that we can hope to answer with QTA [@Grimmer2022a]:

1. What do our texts look like?
2. What are our texts about?
3. What do our texts measure?
4. What can our texts predict?
5. What can our texts prove?

In the case of the first question, we might simply be interested in how many words we have in different documents, which authors wrote together, or whether certain texts have a distinctive type of wording. Questions like these help us to get a better idea of the type of data we are dealing with, to work out what might be interesting to look at and to identify potential problems early on. We can then ask ourselves what the texts are actually about. Here we could run topic models to look at (a representation of) the underlying structure of our texts. We could look at the sentiment of the texts using different dictionaries, or calculate different readability statistics. In each case, we get a better understanding of what our texts represent and what they might be about. For example, we might discover that some texts cluster together unexpectedly, or have more themes in common than we expected. This might then lead us to focus on them exclusively, to collect more texts on the same topic, or to focus on different documents altogether.

Now that we have our texts and know what we want from them, we can start to use them to measure a concept we are interested in. For example, we could use the codes from the [Manifesto Project](https://manifesto-project.wzb.eu/) to measure the political left-right position of our texts. Or we could measure the occurrence of different issues in these documents to find out the agendas of different parties. Once this is done, we could then use these measurements to predict what kind of agenda a new party might have, or a step further, we could use them to estimate what effect a particular text will have on, say, voters or legislators. 

Note that we can stop this process and any point and do interesting work. That is, if we go about rigorously collecting texts from different sources, structuring them, cleaning them, and describing what they are like, this can be interesting in itself. Similarly, measuring the positions of texts on a variety of scales can be enough to make for interesting research. Ultimately, how far you go depends on the questions you want to ask and the problems you want to solve.


## QTA in steps

So what does a QTA look like in practice? Let's say you already have an idea of what you want to do and what questions you want to ask. Then you need to go through the following steps:

1. **Choose and select**. If you want to look at political manifestos, do you want to see all of them or just those of the major parties? And do you want only the most recent ones or all of them?

2. **Find and collect**. Find all the texts you need and save them somewhere. Make sure that everything you want is included, that you have the right version of the document and that the documents are in a readable format (pdf, txt or .doc format).

3. **Check**. If your documents are in .txt format, are there any conversion errors? For example, is a letter like "Ü" visible in the document, or do you see something like "™" instead? Note that most researchers work in English, and non-English and non-Latin alphabets can cause problems. The best option is to ensure that all your documents are in UTF-8 (more on this here).

4. **Create a corpus**. Load all the texts you want to analyse and associate them with any metadata you want to include. Then transform the texts into a data frequency matrix (DFM). This matrix has your individual texts in the rows and all possible words in the columns - this turns your corpus of textual data into a matrix of numbers.

5. **Preprocess**. Remove words you do not need, such as stop words, remove punctuation, and apply stemming or lemmatisation algorithms.

6. **Describe**. Check your data. Are there any words that occur a lot (and which you might want to remove?) Are there any strange patterns? Is the data in the right form?

7. **Run your model**. Select your model, run it and check that all the hyperparameter settings are correct. Check that all the steps are correct and repeat the process at least once to ensure future reproducibility.

Visualise and interpret. Look at your results using tables and graphs and try to see if you can answer your research question.

Of all these steps, you will often find that the last two are the most commonly covered. However, it is equally important to ensure that your data is correct and of sufficient quality to be of real use. Often problems later in the analysis are caused by problems in the data early on.

## How this book works

From here on, this book will work as follows. In \@ref(getting-started-with-r) we will look at R and how to get it working on our system. Our choice of R here is based on the fact that it is both open and free, as well as being the current choice of software for most social scientists (and therefore the one you are most likely to be working with). R also has a wide range of packages that we can use for text analysis. Then in \@ref(#importing-data), we will focus on the actual texts we are going to use and how to get them into R. This will cover converting PDF files to TXT, reading CSV files, and downloading files from an on-line database. Then in \@ref(reliability-validity), we will cover the outstanding issues of reliability and validity, and how to ensure that the codes you get from (a more classical) text analysis are reliable enough to use later. Then, in \@ref(#preliminaries), we look at what is in our data and how we can best describe the texts we are dealing with. Finally, the last four chapters cover the four main types of techniques we can use to find out more about our text or measure things about it: \@ref(dictionary-analysis), \@ref(scaling), \@ref(supervised-methods) and \@ref(unsupervised-methods). We conclude, of course, with a list of references that we have used and that you can use if you are interested in learning more.

## Further Literature

There has been no shortage recently of good introductions to content analysis. Which book would be best for you depends therefore mostly on your focus. For a traditional (more qualitative) introduction, “Content Analysis - an Introduction to Its Methodology” by Klaus Krippendorff (currently in its 4th edition) is a good place to start. For a more quantitative approach, “Text as Data: A New Framework for Machine Learning and the Social Sciences” by Grimmer, Roberts and Stewart from 2022, is the latest in combining machine learning with text analysis. Finally, for another qualitative approach, “The Content Analysis Guidebook” by Kimberly Neuendorf (currently in its 2nd edition) delves deeper into the underlying theory (which we cursorily discuss here).
